from Model import Model
import pandas as pd
from DomainLevel import DomainLevel 
import numpy as np
from keras.preprocessing.sequence import pad_sequences

m = Model.load(DomainLevel.LOWERT)
m.load_model()
m.nn.summary()
emb = m.nn.get_layer('embedding')

weights = emb.get_weights()[0]
vocab = ['_', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
vocab = {letter: idx+1 for idx, letter in enumerate(vocab)}

out_v = open('/tmp/vectors.tsv', 'w', encoding='utf-8')
out_m = open('/tmp/metadata.tsv', 'w', encoding='utf-8')

for index, word in enumerate(vocab):
  if  index == 0: continue # skip 0, it's padding.
  vec = weights[index] 
  out_v.write('\t'.join([str(x) for x in vec]) + "\n")
  out_m.write(word + "\n")
out_v.close()
out_m.close()


bestemmie = ['01oooaaaooo', '02' + 'a'*60]
for b in bestemmie:
    X_num = np.asarray([m.domain2input(b[2:])])
    X = pad_sequences(X_num, maxlen=m.MAXLEN, truncating='post')
    temp_ = emb(X)
    pd.DataFrame(temp_.numpy()[0]).to_csv('/tmp/%s.csv' % b[:2])


# lstm = m.nn.get_layer('lstm')
# weights = layer.get_weights()
# for idx, weight in enumerate(weights):
#     print(weight.shape)
#     pd.DataFrame(weight).to_csv('/tmp/weights_%d.csv' % idx)