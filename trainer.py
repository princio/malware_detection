"""Train and test LSTM classifier"""
import json
from collections import OrderedDict
import numpy as np
import os
import random
import csv
import collections
import math
import time
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score
import joblib
from sklearn.metrics import confusion_matrix
from datetime import datetime
from keras.models import model_from_json
from keras.losses import binary_crossentropy, sparse_categorical_crossentropy
import keras.backend as K
import json
from Model import Model
from Dataset import Dataset
import psycopg2
import hashlib
import pickle
from enum import Enum

class DomainLevel(Enum):
    """
    TOP means: a3.a2.a1.a0 -> a1a0
    SIDE means: a3.a2.a1.a0 -> a3a0
    """
    TOP = 1
    SIDE = 2


connection = psycopg2.connect(user = "postgres",
                                  password = "porcodio",
                                  host = "127.0.0.1",
                                  port = "5432",
                                  database = "malware")

# This is a global variable used in semantic loss calculation
valid_class = []

train_binary = True
train_multi = False
max_epoch = 20
nfolds = 3
date = datetime.now()
output_folder = "models/%s__epochs_%d_folds_%d" % (date.strftime("%Y%m%d_%H%M%S"), max_epoch, nfolds)

def evaluate(y_test, y_pred, f_report, start, end, fold, valid_class):    
    # take the dga's names
    # labels_names = list(dga_dict.keys())
    target_names = list(valid_class.keys())
    labels = list(valid_class.values())

    y_test = np.array(y_test)
    y_test = y_test.ravel()

    print("y_pred len: %d" % len(y_pred))
    print("y_test len: %d" % len(y_test))
    print(valid_class.keys())
    print(valid_class.values())

    class_report = classification_report(y_test, y_pred, digits=4, labels=labels, target_names=target_names)

    # Macro and micro f1score, precision and recall all dataset
    score_macro = f1_score(y_test, y_pred, average="macro")
    precision_macro = precision_score(y_test, y_pred, average="macro")
    recall_macro = recall_score(y_test, y_pred, average="macro")

    score_micro = f1_score(y_test, y_pred, average="micro")
    precision_micro = precision_score(y_test, y_pred, average="micro")
    recall_micro = recall_score(y_test, y_pred, average="micro")

    matrix=[]

    matrix.append([score_macro, score_micro])
    matrix.append([precision_macro, precision_micro])
    matrix.append([recall_macro, recall_micro])

    matrix = np.array(matrix)

    colonne=["macro", "micro"]

    indici=["f1_score", "precision", "recall"]
    df = pd.DataFrame(matrix, index=indici, columns=colonne)
    print("F1_score Precision Recall dataset \n")
    print(df)

    f_report.write("\n\nTesting (minutes)" + str((end-start)/float(60)) + '\n')

    accuracy = accuracy_score(y_test, y_pred)
    f_report.write("\nClass report: \n" + class_report)
    f_report.write("\nF1_score Precision Recall dataset \n" + str(df))
    f_report.write("\n\nOverall accuracy = " + str(accuracy) + '\n\n')
    f_report.write("\nAccuracy for each class:\n")

    # Accuracy for each class
    c_matrix = confusion_matrix(y_test, y_pred)
    cm_acc = c_matrix.astype('float') / c_matrix.sum(axis=1)[:, np.newaxis]
    accuracy_each_class = cm_acc.diagonal()

    # The following cicle is to format and print(the accuracy)
    count_name = 0
    print("\nAccuracy for each class:\n")
    for acc_cls in accuracy_each_class:
        print('{:>14} {:1}'.format(str(target_names[count_name]), str(round(acc_cls, 3)))+'\n')
        f_report.write('{:>14} {:1}'.format(str(target_names[count_name]), str(round(acc_cls, 3)))+'\n')
        count_name +=1

    f_report.write("\n \n")

    print(class_report)

    TP = np.diag(c_matrix)
    TP = np.sum(TP)
    TotalElement = np.asmatrix(c_matrix)
    TotalElement = np.sum(TotalElement)
    f_report.write("\nTrue positive: " + str(TP) + "\n")
    print("\nTrue positive: " + str(TP) + "\n")
    f_report.write("\n \n")


def get_data(datasetname): 
	data= []
	with open(datasetname, "r") as f:
		reader = csv.reader(f)
		for row in reader:
			data.append(row)
	return data

def build_binary_model(max_features, maxlen):
    """Build LSTM model for two-class classification"""
    model = Sequential()
    model.add(Embedding(max_features, 128, input_length = maxlen))
    model.add(LSTM(128))
    model.add(Dropout(0.5))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))

    model.compile(loss='binary_crossentropy', optimizer='rmsprop')

    return model

def build_multiclass_model(max_features, maxlen):
    """Build multiclass LSTM model for multiclass classification"""
    model = Sequential()
    model.add(Embedding(max_features, 128, input_length = maxlen))
    model.add(LSTM(128))
    model.add(Dropout(0.5))
    model.add(Dense(38))
    model.add(Activation('softmax'))

    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')

    return model


def save_model(model, name):
    with open("%s/%s.json" % (output_folder, name), "w") as json_file:
        json_file.write(model.to_json())
    model.save_weights("%s/%s.h5" % (output_folder, name))


def create_class_weight(labels_dict, mu):
    """Create weight based on the number of domain name in the dataset"""
    total = np.sum(list(labels_dict.values()))
    keys = labels_dict.keys()
    class_weight = dict()

    for key in keys:
        score = math.pow(total / float(labels_dict[key]), mu)
        class_weight[key] = score	

    return class_weight

def model_hash(path):
    sha256_hash = hashlib.sha256()
    with open(f"{path}.json", "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    with open(f"{path}.h5", "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

def run(date, max_epoch=max_epoch, nfolds=nfolds, batch_size=128):
	with open('datasets/setAtrain_h_t_domins.2.csv', "r") as f:
		reader = csv.reader(f)
		indata = [row for row in reader]

    X_full = [x[DomainLevel.TOP] for x in indata]
    labels = [x[1] for x in indata]
    binary_labels = [x[0] for x in indata]

    alphabet = ['_', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
    alphabet_converter = {letter: idx+1 for idx, letter in enumerate(alphabet)}

    max_features = len(letter2number) + 1 # 0-number doesn't correspond to any converted letter
    maxlen = np.max([len(x) for x in X_full])

    X = [[alphabet_converter[y] for y in x] for x in X_full]
    X = sequence.pad_sequences(X, maxlen=maxlen, truncating='post')
    y_binary = np.array([0 if x == 'legit' else 1 for x in binary_labels])

    valid_class = {i:indx for indx, i in enumerate(set(labels))}
    y = np.array([valid_class[x] for x in labels])
    white=valid_class['alexa']
    
    start = time.time()

    # Begin two-class classification stage
    # Divide the dataset into training + holdout (80%) and testing (10%) dataset
    test_size_folds = 0.2
    test_size_epochs = 0.05
    sss_fold = StratifiedShuffleSplit(n_splits=nfolds, test_size=test_size_folds, random_state=0)
    fold = 0
    for train_fold, test_fold in sss_fold.split(X, y):
        print("fold %u/%u" % (fold + 1, nfolds))
        fold = fold + 1
        
        X_train_fold, y_train_fold = X[train_fold], y_binary[train_fold]
        
        model = build_binary_model(max_features, maxlen)
        
        sss_epoch = StratifiedShuffleSplit(n_splits=1, test_size=test_size_epochs, random_state=0)
        for train_epochs, test_epochs in sss_epoch.split(X_train_fold, y_train_fold):
            X_train_epochs, y_train_epochs =  X_train_fold[train_epochs], y_train_fold[train_epochs]
            X_test_epochs, y_test_epochs =  X_train_fold[test_epochs], y_train_fold[test_epochs]
        
        # Create weight for two-class classification stage
        labels_dict = collections.Counter(y_train_epochs)
        class_weight = create_class_weight(labels_dict, 0.1)
        best_auc = 0.0
        for n_epoch in range(max_epoch):
            model.fit(X_train_epochs, y_train_epochs, batch_size=batch_size, epochs=1, class_weight=class_weight)

            t_probs = model.predict(X_test_epochs)
            t_result = [0 if (x <= 0.5) else 1 for x in t_probs]
            t_acc = accuracy_score(y_test_epochs, t_result)

            if t_acc > best_auc:
                best_model = model
                best_auc = t_acc

        with open("model_binary.json", "w") as json_file:
            model_json = best_model.to_json()
            json_file.write(model_json)
            best_model.save_weights("model_binary.h5")

        print("Saved two-class model to disk")
        
if __name__ == "__main__":
    run(date)
