import os, json
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
from datetime import datetime
from PyInquirer import prompt
import csv
import hashlib
import re
import psycopg2
import numpy as np
from Model import Model
from Report import Report
from Dataset import Dataset
from Repository import DatasetRepository, ModelRepository, MetricsRepository
from Metrics import Metrics
import random
import pickle

pickle_path = 'output/pickles/shuffle_problem'

random.seed(datetime.now().timestamp())
connection = psycopg2.connect(user = "postgres",
                                  password = "porcodio",
                                  host = "127.0.0.1",
                                  port = "5432",
                                  database = "malware")

if __name__ == "__main__":
    cursor = connection.cursor()
    sf = "%5s " + ("| %1.6f %1.6f %1.6f %5d " * 4) + "| %f"

    models = ModelRepository.fetchall(connection)
    dataset = DatasetRepository.fetch(connection, '00254ec9-d5fe-4f77-8f6a-5c9131d10480')
    np_dataset = np.empty((0,17))
    np_trainings = np.empty((0,17))


    count_missing  = dataset.supports[1] - dataset.supports[0]
    data_or = dataset.data()
    dataset.supports[0] = dataset.supports[1]
    
    datasets = None
    try:
        datasets = pickle.load(open(f'{pickle_path}/datasets.p', 'rb'))
        metrics = pickle.load(open(f'{pickle_path}/metrics.p', 'rb'))
        pass
    except:
        datasets = None
        pass

    overwrite = True
    TEST_N = 6

    if overwrite or datasets is None:
        datasets = []
        metrics = []
        for model in list(models.values())[0:2]:
            print(f'Testing balanced dataset {dataset.name} with {model.name}...')
            if model.training_confusion_matrix is not None:
                training_metrics = Metrics.create(model.classes, model.training_confusion_matrix)
                for i in range(TEST_N):
                    random.shuffle(dataset._data)
                    dataset_metrics = Metrics.create(model.classes, model.test(dataset)[0])
                    datasets.append(dataset)
                    metrics.append(dataset_metrics.tolist())
                    print(sf % tuple([f'#{i}'] + dataset_metrics.tolist()))
                metrics.append(training_metrics.tolist())
                print(sf % tuple(['train'] + training_metrics.tolist()))
        pickle.dump(datasets, open(f'{pickle_path}/datasets.p', 'wb'))
        pickle.dump(metrics, open(f'{pickle_path}/metrics.p', 'wb'))
    else:
        for i in range(len(metrics)-1):
            a = np.asfarray(metrics[i])
            b = np.asfarray(metrics[-1])
            print(sf % tuple([f'#{i}'] + (a).tolist()))
            # print(sf % tuple([f'Î”'] + (b-a).tolist()))
        print(sf % tuple(['train'] + metrics[-1]))


    connection.close()
