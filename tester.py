import os, fnmatch
import json
from collections import OrderedDict
import numpy as np
import csv
import time
import pandas as pd
from keras.preprocessing import sequence
from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score
import joblib
from sklearn.metrics import confusion_matrix
from datetime import datetime

from keras.models import model_from_json

dataset_path = "datasets/malware_h_t/25_2014-02-10_capture-win3.pcap_table.csv__h_t"
train_dataset_path = "datasets/setAtrain_h_t_domins"

date = "20200917_084748"
nepoch = 20
nfold = 3

trained_model_path = "output/%s__epochs_%d_folds_%d/" % (date, nepoch, nfold)

report_dict = {}

def evaluate(y_test, y_pred, f_report, start, end, valid_class, flag_bin, model_name):
    # Take the dga's names
    # labels_names = list(dga_dict.keys())
    target_names = list(valid_class.keys())

    y_test = np.array(y_test)
    y_test = y_test.ravel()

    class_report_text = classification_report(y_test, y_pred, digits=4, target_names=target_names)
    class_report_dict = classification_report(y_test, y_pred, digits=4, target_names=target_names, output_dict=True)

    # Macro and micro f1score, precision and recall all dataset
    score_macro = f1_score(y_test, y_pred, average="macro")
    precision_macro = precision_score(y_test, y_pred, average="macro")
    recall_macro = recall_score(y_test, y_pred, average="macro")
    score_micro = f1_score(y_test, y_pred, average="micro")
    precision_micro = precision_score(y_test, y_pred, average="micro")
    recall_micro = recall_score(y_test, y_pred, average="micro")

    matrix = []
    matrix.append([score_macro, score_micro])
    matrix.append([precision_macro, precision_micro])
    matrix.append([recall_macro, recall_micro])
    matrix = np.array(matrix)

    colonne = ["macro", "micro"]
    indici = ["f1_score", "precision", "recall"]

    df = pd.DataFrame(matrix, index=indici, columns=colonne)
    print("F1_score Precision Recall dataset \n")
    print(df)

    f_report.write("\n\nTesting (minutes)" + str((end - start) / float(60)) + '\n')

    accuracy = accuracy_score(y_test, y_pred)
    f_report.write("\nClass report: \n" + class_report_text)
    f_report.write("\nF1_score Precision Recall dataset \n" + str(df))
    f_report.write("\n\nOverall accuracy = " + str(accuracy) + '\n\n')
    f_report.write("\nAccuracy for each class:\n")

    class_report_dict["macro"] = {
        "f1_score": score_macro,
        "precision": precision_macro,
        "recall": recall_macro
    }
    class_report_dict["micro"] = {
        "f1_score": score_micro,
        "precision": precision_micro,
        "recall": recall_micro
    }

    print(class_report_dict)

    # Accuracy for each class
    c_matrix = confusion_matrix(y_test, y_pred)
    cm_acc = c_matrix.astype('float') / c_matrix.sum(axis=1)[:, np.newaxis]
    accuracy_each_class = cm_acc.diagonal()


    class_report_dict["accuracy"] = {
        "1": class_report_dict["accuracy"]
    }

    # The following cicle is to format and print the accuracy
    count_name = 0
    print("\nAccuracy for each class:\n")
    for acc_cls in accuracy_each_class:
        print('{:>14} {:1}\n'.format(target_names[count_name], round(acc_cls, 3)))
        f_report.write('{:>14} {:1}\n'.format(target_names[count_name], round(acc_cls, 3)))
        class_report_dict["accuracy"][target_names[count_name]] = acc_cls
        count_name += 1

    f_report.write("\n \n")

    TP = np.diag(c_matrix)
    TP = np.sum(TP)
    TotalElement = np.asmatrix(c_matrix)
    TotalElement = np.sum(TotalElement)
    f_report.write("\nTrue positive: " + str(TP) + "\n")
    print("\nTrue positive: " + str(TP) + "\n")

    class_report_dict["true_positive"] = TP

    f_report.write("\n \n")

    report_dict[model_name] = class_report_dict
    report_dict[model_name]["dataset"] = dataset_path
    report_dict[model_name]["dataset_test"] = train_dataset_path


    # f_total_report = open("reports/total_report.csv", 'a+')
    # if flag_bin == True:
    #     report_row = "binary" + ";" + dataset_path + ".csv" + ";" + '%.5f' % (precision_micro) + ";" + '%.5f' % (
    #         precision_macro) + ";" + '%.5f' % (recall_micro) + ";" + '%.5f' % (recall_macro) + ";" + '%.5f' % (
    #                      score_micro) + ";" + '%.5f' % (score_macro) + ";" + '%.5f' % (accuracy) + ";" + str(
    #         TP) + ";" + str(TotalElement) + "\n"
    # else:
    #     report_row = "multi" + ";" + dataset_path + ".csv" + ";" + '%.5f' % (precision_micro) + ";" + '%.5f' % (
    #         precision_macro) + ";" + '%.5f' % (recall_micro) + ";" + '%.5f' % (recall_macro) + ";" + '%.5f' % (
    #                      score_micro) + ";" + '%.5f' % (score_macro) + ";" + '%.5f' % (accuracy) + ";" + str(
    #         TP) + ";" + str(TotalElement) + "\n"
    # f_total_report.write(report_row)
    # print("\nRisultati scritti correttamente nel report.\n")
    # f_total_report.close()


def get_data(dataset_path):
    """Read data from file (Traning, testing and validation) to process"""
    data = []
    with open(dataset_path + ".csv", "r") as f:
        reader = csv.reader(f)
        for row in reader:
            data.append(row)
    return data

def find(pattern, path):
    for root, dirs, files in os.walk(path):
        for name in files:
            if fnmatch.fnmatch(name, pattern):
                return os.path.splitext(os.path.join(root, name))[0]
    raise NotImplementedError

class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return super(NpEncoder, self).default(obj)

def run(date):
    """Run train/test on logistic regression model"""
    # Begin preprocessing stage

    # Read data to process
    indata = get_data(dataset_path)
    indata_train = get_data(train_dataset_path)

    # Extract data and labels
    binary_labels = [x[0] for x in indata]  # legit o dns
    X = [x[2] for x in indata]  # DNS
    X_train = [x[2] for x in indata_train]  # DNS_train
    labels = [x[1] for x in indata]  # famiglia dga

    # Generate a dictionary of valid characters
    valid_chars = {x: idx + 1 for idx, x in enumerate(OrderedDict.fromkeys(''.join(X)))}
    maxlen = np.max([len(x) for x in X])

    # Convert characters to int and pad
    X_test = [[valid_chars[y] for y in x] for x in X]
    X_test = sequence.pad_sequences(X_test, maxlen=maxlen)

    # Convert labels to 0-1 for binary class
    y_binary = np.array([0 if x == 'legit' else 1 for x in binary_labels])

    start = time.time()

    for f in range(nfold):
        fold = f + 1
        model_path = find("best_binary_model_*", "%s/fold_%d/" % (trained_model_path, fold))
        with  open("%s.json" % model_path, 'r') as json_file:
            loaded_model_json = json_file.read()
        binary_model = model_from_json(loaded_model_json)
        binary_model.load_weights("%s.h5" % model_path)
        
        y_pred = binary_model.predict(X_test)
        y_bin_result = [0 if (x <= 0.5) else 1 for x in y_pred]

        # Calculate final result binary
        with open("%s_report.csv" % model_path, 'a') as f_report_binary:
            class_binary = {'white': 0, 'black': 1}
            evaluate(y_binary, y_bin_result, f_report_binary, start, time.time(), class_binary, True, model_path)

        # Saving probabilities
        np.savetxt("%s/probabilities_fold_%d.csv" % (trained_model_path, fold), y_pred, delimiter=',', fmt='%f')
    
    with open("%s/report.json" % trained_model_path, 'w') as fp:
        json.dump(report_dict, fp, cls=NpEncoder, indent="  ")

    # End binary report

date = datetime.now()
if __name__ == "__main__":
    run(date)