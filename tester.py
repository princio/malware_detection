import sqlite3
import json
import time
from datetime import datetime
from PyInquirer import prompt, Separator
import os, fnmatch
import itertools
from collections import OrderedDict
import numpy as np
import csv
import pandas as pd
from keras.models import model_from_json
from keras.preprocessing import sequence
from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score
import joblib
from sklearn.metrics import confusion_matrix
from pymongo import MongoClient
import hashlib

train_dataset_path = "datasets/setAtrain_h_t_domins.csv"

report_dict = {}

connection = sqlite3.connect('./models/database.sqlite3')

def inquire():
    cursor = connection.cursor()
    cursor.execute("select * from models")
    models = cursor.fetchall()
    models = [{"value": Model(model), "name": model[1]} for model in models]
    questions = [
        {
            'type': 'checkbox',
            'name': 'models',
            'message': 'Which dataset do you want to use as input?',
            'choices': models
        }
    ]
    models = prompt(questions)["models"]

    questions = [
        {
            'type': 'list',
            'name': 'input_category',
            'message': 'Which dataset do you want to use as input?',
            'choices': [
                'benign',
                'malware',
            ]
        }
    ]
    input_category = prompt(questions)["input_category"]

    files = os.listdir("datasets/%s_h_t" % input_category)
    files = [{"name": f} for f in files]
    questions = [
        {
            'type': 'checkbox',
            'name': 'input_files',
            'message': 'Select one or more files',
            'choices': files,
            'pageSize': 5
        }
    ]
    inputs = prompt(questions)["input_files"]

    return (models, ["datasets/%s_h_t/%s" % (input_category, _input) for _input in inputs])

def save_metrics(report_id, metrics, name):
    cursor = connection.cursor()
    if "support" not in metrics:
        metrics["support"] = 0
    cursor.execute(f"insert into metrics values (?, ?, ?, ?, ?, ?, ?)", (None, report_id, name, metrics["precision"], metrics["recall"], metrics["f1-score"], int(metrics["support"])))
    cursor.close()

def save_class_accuracy(report_id, name, value):
    cursor = connection.cursor()
    cursor.execute(f"insert into class_accuracy values (?, ?, ?, ?)", (None, report_id, name, value))
    cursor.close()


def evaluate(y_test, y_pred, f_report, time_elapsed, valid_class, flag_bin, model_id, dataset_path):

    # Take the dga's names
    # labels_names = list(dga_dict.keys())
    target_names = list(valid_class.keys())

    y_test = np.array(y_test)
    y_test = y_test.ravel()

    class_report_text = classification_report(y_test, y_pred, digits=12, target_names=target_names)
    class_report_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)

    print(class_report_dict)

    # Macro and micro f1score, precision and recall all dataset
    macro = {
        "precision": precision_score(y_test, y_pred, average="macro"),
        "f1-score": f1_score(y_test, y_pred, average="macro"),
        "recall": recall_score(y_test, y_pred, average="macro")
    }
    micro = {
        "precision": precision_score(y_test, y_pred, average="micro"),
        "f1-score": f1_score(y_test, y_pred, average="micro"),
        "recall": recall_score(y_test, y_pred, average="micro")
    }

    matrix = []
    matrix.append([macro["f1-score"], micro["f1-score"]])
    matrix.append([macro["precision"], micro["precision"]])
    matrix.append([macro["recall"], micro["f1-score"]])
    matrix = np.array(matrix)

    colonne = ["macro", "micro"]
    indici = ["f1_score", "precision", "recall"]

    df = pd.DataFrame(matrix, index=indici, columns=colonne)
    print("F1_score Precision Recall dataset \n")
    print(df)

    f_report.write("\n\nTesting (minutes)" + str((time_elapsed) / float(60)) + '\n')

    accuracy = accuracy_score(y_test, y_pred)
    f_report.write("\nClass report: \n" + class_report_text)
    f_report.write("\nF1_score Precision Recall dataset \n" + str(df))
    f_report.write("\n\nOverall accuracy = " + str(accuracy) + '\n\n')
    f_report.write("\nAccuracy for each class:\n")

    class_report_dict["macro"] = macro
    class_report_dict["micro"] = micro

    print(class_report_dict)

    # Accuracy for each class
    c_matrix = confusion_matrix(y_test, y_pred)
    cm_acc = c_matrix.astype('float') / c_matrix.sum(axis=1)[:, np.newaxis]
    accuracy_each_class = cm_acc.diagonal()


    class_report_dict["accuracy"] = {
        "1": class_report_dict["accuracy"]
    }

    f_report.write("\n \n")

    TP = np.diag(c_matrix)
    TP = np.sum(TP)
    TotalElement = np.asmatrix(c_matrix)
    TotalElement = np.sum(TotalElement)
    f_report.write("\nTrue positive: " + str(TP) + "\n")
    print("\nTrue positive: " + str(TP) + "\n")

    class_report_dict["true_positive"] = TP

    cursor = connection.cursor()
    
    cursor.execute("""INSERT INTO 
    "main"."reports"("id", "datetime", "model_id", "input_dataset_path", "test_dataset_path", "accuracy", "true_positive", "time_elapsed")
    VALUES (?,?,?,?,?,?,?,?)""", (None, int(datetime.now().timestamp()), model_id, dataset_path, train_dataset_path, accuracy, TP, time_elapsed))

    report_id = cursor.lastrowid

    cursor.close()

    save_metrics(report_id, class_report_dict["white"], "white")
    save_metrics(report_id, class_report_dict["black"], "black")
    save_metrics(report_id, class_report_dict["macro avg"], "macro avg")
    save_metrics(report_id, class_report_dict["weighted avg"], "weighted avg")
    save_metrics(report_id, macro, "macro")
    save_metrics(report_id, micro, "micro")
    
    # The following cicle is to format and print the accuracy
    count_name = 0
    print("\nAccuracy for each class:\n")
    for acc_cls in accuracy_each_class:
        print('{:>14} {:1}\n'.format(target_names[count_name], round(acc_cls, 12)))
        f_report.write('{:>14} {:1}\n'.format(target_names[count_name], round(acc_cls, 12)))
        class_report_dict["accuracy"][target_names[count_name]] = acc_cls
        save_class_accuracy(report_id, target_names[count_name], acc_cls)
        count_name += 1

    f_report.write("\n \n")

    return class_report_dict


    # f_total_report = open("reports/total_report.csv", 'a+')
    # if flag_bin == True:
    #     report_row = "binary" + ";" + dataset_path + ".csv" + ";" + '%.5f' % (precision_micro) + ";" + '%.5f' % (
    #         precision_macro) + ";" + '%.5f' % (recall_micro) + ";" + '%.5f' % (recall_macro) + ";" + '%.5f' % (
    #                      score_micro) + ";" + '%.5f' % (score_macro) + ";" + '%.5f' % (accuracy) + ";" + str(
    #         TP) + ";" + str(TotalElement) + "\n"
    # else:
    #     report_row = "multi" + ";" + dataset_path + ".csv" + ";" + '%.5f' % (precision_micro) + ";" + '%.5f' % (
    #         precision_macro) + ";" + '%.5f' % (recall_micro) + ";" + '%.5f' % (recall_macro) + ";" + '%.5f' % (
    #                      score_micro) + ";" + '%.5f' % (score_macro) + ";" + '%.5f' % (accuracy) + ";" + str(
    #         TP) + ";" + str(TotalElement) + "\n"
    # f_total_report.write(report_row)
    # print("\nRisultati scritti correttamente nel report.\n")
    # f_total_report.close()


def get_data(dataset_path):
    """Read data from file (Traning, testing and validation) to process"""
    data = []
    with open(dataset_path, "r") as f:
        reader = csv.reader(f)
        for row in reader:
            data.append(row)
    return data

def find(pattern, path):
    print(path)
    for root, dirs, files in os.walk(path):
        for name in files:
            if fnmatch.fnmatch(name, pattern):
                return os.path.splitext(os.path.join(root, name))[0]
    raise Exception("Pattern <%s> not found in: %s" % (pattern, path))


class Model:
    def __init__(self, model_fetched):
        self.id = model_fetched[0]
        self.path = model_fetched[1]
        self.hash = model_fetched[2]
        self.date = model_fetched[3]
        self.epochs = model_fetched[4]
        self.folds = model_fetched[5]
        self.epoch = model_fetched[6]
        self.fold = model_fetched[7]
        self.instance = None

class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        print(f"current obj is {obj}")
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            print(f"{obj} is np.floating")
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return super(NpEncoder, self).default(obj)

def model_hash(path):
    sha256_hash = hashlib.sha256()
    with open(f"{path}.json", "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            print(len(byte_block))
            sha256_hash.update(byte_block)
    with open(f"{path}.h5", "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            print(len(byte_block))
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

def load_model(model_id):
    cursor = connection.cursor()
    cursor.execute("select * from models where id=:id", {"id": model_id})
    if cursor.rowcount == 0:
        raise Exception("No model found")

    model = Model(cursor.fetchone())
    with  open("%s.json" % model.path, 'r') as json_file:
        loaded_model_json = json_file.read()

    model.instance = model_from_json(loaded_model_json)
    model.instance.load_weights("%s.h5" % model.path)

    return model



def run(date, models, inputs):
    """Run train/test on logistic regression model"""
    # Begin preprocessing stage

    # Read data to process
    indata_train = get_data(train_dataset_path)
    X_train = [x[2] for x in indata_train]  # DNS_train

    for model in models:

        model = load_model(model.id)

        for input_ in inputs:
            # Extract data and labels
            indata = get_data(input_)
            binary_labels = [x[0] for x in indata]  # legit o dns
            X = [x[2] for x in indata]  # DNS
            labels = [x[1] for x in indata]  # famiglia dga

            # Generate a dictionary of valid characters
            valid_chars = {x: idx + 1 for idx, x in enumerate(OrderedDict.fromkeys(''.join(X)))}
            maxlen = np.max([len(x) for x in X])

            # Convert characters to int and pad
            X_test = [[valid_chars[y] for y in x] for x in X]
            X_test = sequence.pad_sequences(X_test, maxlen=maxlen)

            # Convert labels to 0-1 for binary class
            y_binary = np.array([0 if x == 'legit' else 1 for x in binary_labels])

            time_elapsed = time.time()
            y_pred = model.instance.predict(X_test)
            time_elapsed = time.time() - time_elapsed

            y_bin_result = [0 if (x <= 0.5) else 1 for x in y_pred]

            with open("%s.txt" % model.path, 'w') as f_report:
                evaluate(y_binary, y_bin_result, f_report, time_elapsed, {'white': 0, 'black': 1}, True, model.id, input_)
            
    connection.commit()
    connection.close()

        # folded_trained_model_reports_path = "%s/tests/%s" % (trained_model_path, dataset_name)
        # os.makedirs(folded_trained_model_reports_path, exist_ok=True)
        # with open("%s/fold_%d_reports.json" % (folded_trained_model_reports_path, fold), 'w') as f_report_binary:
        #     report_dict["fold_%d" % fold] = 
        # np.savetxt("%s/fold_%d_probabilities.csv" % (folded_trained_model_reports_path, fold), y_pred, delimiter=',', fmt='%f')

    # End binary report

date = datetime.now()
if __name__ == "__main__":
    a = inquire()
    print(a)
    run(date, a[0], a[1])