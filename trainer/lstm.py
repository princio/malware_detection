"""Train and test LSTM classifier"""
import numpy as np
import os
import random
import csv
import collections
import math
import time
import pandas as pd
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import precision_score, recall_score, classification_report,accuracy_score, f1_score
import joblib
from sklearn.metrics import confusion_matrix
from datetime import datetime
from keras.models import model_from_json
from keras.losses import binary_crossentropy, sparse_categorical_crossentropy
import keras.backend as K
import json

#This is a global variable used in semantic loss calculation
valid_class = []

#cntk.try_set_default_device(cntk.device.cpu())

path_report='report/'
path_model='model/'
datasetname='setAtrain_h_t_domins'

train_binary = True
max_epoch = 20
nfolds = 3

def evaluate(y_test, y_pred, f_report, start, end,fold ,valid_class):    
    # Take the dga's names
    #labels_names = list(dga_dict.keys())
    target_names=valid_class.keys()

    y_test = np.array(y_test)
    y_test = y_test.ravel()

    class_report = classification_report(y_test, y_pred,digits=4, labels = valid_class.values(), target_names = valid_class.keys())
    
    #Macro and micro f1score,precision and recall all dataset
    score_macro = f1_score(y_test, y_pred,average="macro")
    precision_macro = precision_score(y_test, y_pred,average="macro")
    recall_macro = recall_score(y_test, y_pred,average="macro")
    score_micro = f1_score(y_test, y_pred,average="micro")
    precision_micro = precision_score(y_test, y_pred,average="micro")
    recall_micro = recall_score(y_test, y_pred,average="micro")
    matrix=[]
    matrix.append([score_macro, score_micro])
    matrix.append([precision_macro,precision_micro])
    matrix.append([recall_macro, recall_micro])
    matrix=np.array(matrix)
    colonne=["macro","micro"]
    indici=["f1_score", "precision", "recall"]
    df = pd.DataFrame(matrix,index=indici,columns=colonne)
    print("F1_score Precision Recall dataset \n")
    print(df)
    
    f_report.write("\n\nTesting (minutes)" + str((end-start)/float(60)) + '\n')
    
    accuracy = accuracy_score(y_test,y_pred)
    f_report.write("\nClass report: \n" + class_report)
    f_report.write("\nF1_score Precision Recall dataset \n" + str(df))
    f_report.write("\n\nOverall accuracy = " + str(accuracy) + '\n\n')
    f_report.write("\nAccuracy for each class:\n")


    # Accuracy for each class
    c_matrix = confusion_matrix(y_test,y_pred)
    cm_acc = c_matrix.astype('float') / c_matrix.sum(axis=1)[:, np.newaxis]
    accuracy_each_class = cm_acc.diagonal()
    
    # The following cicle is to format and print(the accuracy)
    count_name = 0
    print("\nAccuracy for each class:\n")
    for acc_cls in accuracy_each_class:
        print('{:>14} {:1}'.format(str(target_names[count_name]), str(round(acc_cls,3)))+'\n')
        f_report.write('{:>14} {:1}'.format(str(target_names[count_name]), str(round(acc_cls,3)))+'\n')
        count_name +=1
    
    f_report.write("\n \n")
	
    print(class_report)

    TP = np.diag(c_matrix)
    TP = np.sum(TP)
    TotalElement = np.asmatrix(c_matrix)
    TotalElement = np.sum(TotalElement)
    f_report.write("\nTrue positive: "+str(TP)+"\n")
    print("\nTrue positive: "+str(TP)+"\n")
    f_report.write("\n \n")


def get_data(datasetname): 
	"""Read data from file (Traning, testing and validation) to process"""
	data= []
	with open(datasetname+".csv", "r") as f:
		reader = csv.reader(f)
		for row in reader:
			data.append(row)
	return data

def build_binary_model(max_features, maxlen):
    """Build LSTM model for two-class classification"""
    model = Sequential()
    model.add(Embedding(max_features, 128, input_length=maxlen))
    model.add(LSTM(128))
    model.add(Dropout(0.5))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))

    model.compile(loss='binary_crossentropy',optimizer='rmsprop')

    return model

def my_multi_loss(y_true, y_pred):
	w = 0.05
	l = 0
	for i in range(0,y_pred.shape[1]):
		p = y_pred[0][i]
		prod = 1
		for j in range(0,y_pred.shape[1]):
			if (j!=i):
				prod *= (1-y_pred[0][j])
		l += p*prod
	l = -K.log(l)
	return sparse_categorical_crossentropy(y_true, y_pred) + w*l

def my_multi_loss_2(y_true, y_pred):
    
	target_names=valid_class.keys()
	w = 0.05
	l = 0
	for i in range(0,y_pred.shape[1]):
		p = y_pred[0][i]
		prod = 1
		for j in range(0,y_pred.shape[1]):
			if (j!=i):
				prod *= (1-y_pred[0][j])
		l += p*prod
	l = -K.log(l)
	return sparse_categorical_crossentropy(y_true, y_pred) + w*l

def build_multiclass_model(max_features, maxlen):
    """Build multiclass LSTM model for multiclass classification"""
    model = Sequential()
    model.add(Embedding(max_features, 128, input_length=maxlen))
    model.add(LSTM(128))
    model.add(Dropout(0.5))
    model.add(Dense(38))
    model.add(Activation('softmax'))

    model.compile(loss='sparse_categorical_crossentropy',optimizer='rmsprop')

    return model


def create_class_weight(labels_dict,mu):
    """Create weight based on the number of domain name in the dataset"""
    total = np.sum(list(labels_dict.values()))
    keys = labels_dict.keys()
    class_weight = dict()

    for key in keys:
        score = math.pow(total/float(labels_dict[key]),mu)
        class_weight[key] = score	

    return class_weight


def run(date,max_epoch=max_epoch, nfolds=nfolds, batch_size=128):
    """Run train/test on logistic regression model"""
    #Begin preprocessing stage
    #Read data to process
    indata = get_data(datasetname)
    
    # Extract data and labels
    binary_labels = [x[0] for x in indata] #legit o dns
    X = [x[2] for x in indata] #DNS
    labels = [x[1] for x in indata] #famiglia dga
    # Generate a dictionary of valid characters
    valid_chars = {x:idx+1 for idx, x in enumerate(set(''.join(X)))}
    max_features = len(valid_chars) + 1
    maxlen = np.max([len(x) for x in X])

    # Convert characters to int and pad
    X = [[valid_chars[y] for y in x] for x in X]
    X = sequence.pad_sequences(X, maxlen=maxlen)
   
	# Convert labels to 0-1 for binary class
    y_binary = np.array([0 if x == 'legit' else 1 for x in binary_labels])
    # Convert labels to 0-37 for multi class
    valid_class = {i:indx for indx, i in enumerate(set(labels))}
    print(valid_class)
    y = [valid_class[x] for x in labels]
    y = np.array(y)
    white=valid_class['alexa']
    #End preprocessing stage
    
    #dati_test=get_data(datasetname_test)
    #dati_test_binary_labels=[x[0] for x in dati_test]
    #y_file_test_binary= np.array([0 if x == 'legit' else 1 for x in dati_test_binary_labels])
    #labels_filetest = [x[1] for x in indata]
    #y_dga_test_binary=
    
    start = time.time()


    #Begin two-class classification stage
    #Divide the dataset into training + holdout (80%) and testing (10%) dataset
    sss = StratifiedShuffleSplit(n_splits=nfolds, test_size=0.1, random_state=0)
    fold =0
    for train, test in sss.split(X,y):
        print("fold %u/%u" % (fold+1, nfolds))
        
        X_train, X_test, y_train, y_test, y_dga_train, y_dga_test = X[train], X[test], y_binary[train], y_binary[test], y[train], y[test]
        y_dga =[]
        X_dga =[]
        for i in range(len(y_dga_train)):
        	if y_dga_train[i]!= white:
        		y_dga.append(y_dga_train[i])
        		X_dga.append(X_train[i])
        X_dga =np.array(X_dga)
        y_dga =np.array(y_dga)      

        #Build the model for two-class classification stage
        model = build_binary_model(max_features, maxlen)
        
        
        sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)
        for train, test in sss1.split(X_train, y_train):
            X_train, X_holdout, y_train, y_holdout = X_train[train], X_train[test], y_train[train], y_train[test]
        
        #Create weight for two-class classification stage
        labels_dict=collections.Counter(y_train)
        class_weight = create_class_weight(labels_dict,0.1)
        best_auc = 0.0
        
        #name_file = path_model+'binary_'+datasetname+"_"+str(date.year)+"_"+str(date.strftime("%m"))+"_"+str(date.strftime("%d"))+"-"+str(date.strftime("%H"))+"-"+str(date.strftime("%M"))+'.json'
        #name_file2 = path_model+'binary_'+datasetname+"_"+str(date.year)+"_"+str(date.strftime("%m"))+"_"+str(date.strftime("%d"))+"-"+str(date.strftime("%H"))+"-"+str(date.strftime("%M"))+'.h5'        
        name_file = path_model + 'binary_' + datasetname + '_epochs_' + str(max_epoch) + '_folds_' + str(nfolds) + '.json'
        name_file2 = path_model + 'binary_' + datasetname + '_epochs_' + str(max_epoch) + '_folds_' + str(nfolds) + '.h5'        
        #20
        if train_binary:
            print("Training the model for two-class classification stage...")
            for ep in range(max_epoch):
                 model.fit(X_train, y_train, batch_size=batch_size, epochs=1, class_weight=class_weight)
                 t_probs = model.predict_proba(X_holdout)
                 t_result = [0 if(x<=0.5) else 1 for x in t_probs]
                 t_acc = accuracy_score(y_holdout, t_result)
                 #Get the model with highest accuracy
                 if t_acc > best_auc:
                    best_model = model
                    best_auc = t_acc
		    #Save the model for two-class classification     
            #Serialize model to JSON
            model_json = best_model.to_json()
            with open(name_file, "w") as json_file:
                json_file.write(model_json)
            #Serialize weights to HDF5
                best_model.save_weights(name_file2)
        
            #Salvataggio del classificatore    
            #joblib.dump(best_model, path_model+'model_binary'+datasetname+"_"+str(date.year)+"_"+str(date.strftime("%m"))+"_"+str(date.strftime("%d"))+"-"+str(date.strftime("%H"))+"-"+str(date.strftime("%M"))+'.pkl')
            print("Saved two-class model to disk")
            #End of two-class classification stage
        
        json_file = open(name_file, 'r')
        loaded_model_json = json_file.read()
        json_file.close()
        best_model = model_from_json(loaded_model_json)
        best_model.load_weights(name_file2)        
        
        
        #Salvataggio variabili per report binary
        y_pred = best_model.predict_proba(X_test)
        y_bin_result = [0 if(x<=0.5) else 1 for x in y_pred]
        y_binary_true=y_test
        
        
        #Begin multiclass classification stage
        #Build the model for multiclass classification stage
        model_dga = build_multiclass_model(max_features, maxlen)
        print("Training the model in multiclass classification...")
        
        sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)
        for train, test in sss2.split(X_dga, y_dga):
            X_train, X_holdout, y_train, y_holdout = X_dga[train], X_dga[test], y_dga[train], y_dga[test]
        
        # Caculate class weight 
        labels_dict=collections.Counter(y_train)
        class_weight = create_class_weight(labels_dict,0.3)
        print('Class weight for multiclass classification:')
        print(class_weight)
        best_acc = 0.0
        #20
        for ep in range(max_epoch):
            model_dga.fit(X_train, y_train, batch_size=batch_size, epochs=1, class_weight=class_weight)
            y_pred= model_dga.predict_proba(X_holdout)
            y_result = [np.argmax(x) for x in y_pred]  
            t_acc= accuracy_score(y_holdout, y_result)
            if t_acc > best_acc:
                best_model_dga = model_dga
                best_acc = t_acc
        #Save the model for multiclass classification stage
        model_json = best_model_dga.to_json()
        #name_file = path_model+'multi_'+datasetname+"_"+str(date.year)+"_"+str(date.strftime("%m"))+"_"+str(date.strftime("%d"))+"-"+str(date.strftime("%H"))+"-"+str(date.strftime("%M"))+'.json'
        #name_file2 = path_model+'multi_'+datasetname+"_"+str(date.year)+"_"+str(date.strftime("%m"))+"_"+str(date.strftime("%d"))+"-"+str(date.strftime("%H"))+"-"+str(date.strftime("%M"))+'.h5'
        name_file2 = path_model+'multi_'+datasetname+ '_epochs_' + str(max_epoch) + '_folds_' + str(nfolds) + '.h5'
        name_file = path_model+'multi_'+datasetname+ '_epochs_' + str(max_epoch) + '_folds_' + str(nfolds) + '.json'
        with open(name_file, "w") as json_file:
            json_file.write(model_json)
        # serialize weights to HDF5
            best_model_dga.save_weights(name_file2)
        
        #joblib.dump(best_model_dga, path_model+'model_multi'+datasetname+"_"+str(date.year)+"_"+str(date.strftime("%m"))+"_"+str(date.strftime("%d"))+"-"+str(date.strftime("%H"))+"-"+str(date.strftime("%M"))+'.pkl')
        print("Saved multiclass model to disk")

        y_pred = best_model.predict_proba(X_test)
        y_result = [white if(x<=0.5) else 0 for x in y_pred]
        
        
        end=time.time()
        
        X_dga_test =[]
        y_dga_test_labels =[]
        for i in range(len(y_result)):
            if y_result[i] == 0:
                X_dga_test.append(X_test[i]) 
                y_dga_test_labels.append(y_dga_test[i]) #Valori veri dei dga
        X_dga_test = np.array(X_dga_test)
        y_dga_test_labels = np.array(y_dga_test_labels)
        y_pred_dga = best_model_dga.predict_proba(X_dga_test)
        y_result_dga = [np.argmax(x) for x in y_pred_dga]
        #End of multiclass classification stage

        j = 0
        for i in range(len(y_result)):
            if y_result[i] != white:
                y_result[i] = y_result_dga[j]
                j = j+1

        #Calculate the final result multi class
        print("\n\nMulticlass report: \n")
        f_report_multi=open(path_report+"report_multiclass_"+datasetname+'_epochs_' + str(max_epoch) + '_folds_' + str(nfolds) + ".txt", 'a')
        evaluate(y_dga_test,y_result,f_report_multi,start,end,fold,valid_class)
        f_report_multi.close()
        #end multiclass report

        #Calculate final result binary
        print("\n\nBinary report: \n")
        f_report_binary=open(path_report+"report_binary_"+datasetname+ '_epochs_' + str(max_epoch) + '_folds_' + str(nfolds) + ".txt", 'a')
        class_binary={'white':0 , 'black':1}
        evaluate(y_binary_true,y_bin_result,f_report_binary,start,end,fold,class_binary)
        f_report_binary.close()
        #End binary report
        
        fold = fold+1

date=datetime.now()
if __name__ == "__main__":
    run(date)
