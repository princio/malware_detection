import os
from DomainLevel import DomainLevel, extractor
import csv
import numpy as np
import Capture as cp
import pandas as pd
import Model
from IOview import InputView, OutputView
from keras.preprocessing.sequence import pad_sequences

def count_dn_level():

    path = 'datasets/setAtrain_h_t_domins.csv'

    dga = []
    legit = []

    dns_num = {
        'legit': np.zeros(10),
        'dga': np.zeros(6)
    }
    dns_level_num = {
        'legit': {0: {}, 1: {}, 2: {}, 3: {}, 4: {}, 5: {}},
        'dga': {0: {}, 1: {}, 2: {}, 3: {}, 4: {}, 5: {}}
    }
    dns_by_level_num = {
        'legit': {1: [], 2: [], 3: [], 4: [], 5: []},
        'dga': {1: [], 2: [], 3: [], 4: [], 5: []}
    }
    with open(path, "r") as f:
        reader = csv.reader(f)
        for row in reader:
            dn_levels = row[3].split('.')
            row.append(dn_levels)
            t = row[0]
            if t == 'legit':
                legit.append(row)
            else:
                dga.append(row)

            l_d_l = len(dn_levels)
            dns_by_level_num[t][l_d_l].append(row)

            dns_num[t][l_d_l] += 1

            dn_levels.reverse()
            for idx, l in enumerate(dn_levels):
                if l not in dns_level_num[t][idx]:
                    dns_level_num[t][idx][l] = 0
                dns_level_num[t][idx][l] += 1

    
    for t  in ['legit', 'dga']:
        # for l_n  in dns_level_num[t]:
        #     with open(f'{path}_dns_{l_n}_{t}.csv', "w") as f:
        #         writer = csv.writer(f)
        #         for dn_name in dns_level_num[t][l_n]:
        #             writer.writerow([dn_name, dns_level_num[t][l_n][dn_name]])
        for level_num in dns_by_level_num[t]:
            with open(f'{path}_rows_w_{level_num}_level_{t}.csv', "w") as f:
                writer = csv.writer(f)
                for r in dns_by_level_num[t][level_num]:
                    writer.writerow(r)
    pass


def inspect_alexa_dn_with_least_2_level():
    path = 'datasets/setAtrain_h_t_domins.csv'

    legit = []

    third_level = []
    with open(f'{path}_third_level_legit.csv', "w") as f:
        writer = csv.writer(f)
        with open(path, "r") as f:
            reader = csv.reader(f)
            for row in reader:
                dn_levels = row[3].split('.')
                row.append(dn_levels)
                t = row[0]
                if t == 'legit' and len(dn_levels) > 3:
                    third_level.append(row + dn_levels)
                    writer.writerow(row + dn_levels)

if __name__ == "__main__":

    # translate in captures
    # df = pd.read_csv('datasets/training.csv')
    # df2 = pd.DataFrame()
    # df2['dn'] = df['dns']
    # df2['label'] = df['class']
    # df2['class'] = df['dga']
    # df2.index = df.index
    # df2.sort_index().to_csv('/tmp/training.csv')

    # exit(0)



    # df = pd.DataFrame()
    # df.insert(0, 'dn', training.dn)
    # df.insert(1, 'label', training.label)
    # df.insert(2, 'level', df['dn'].apply(lambda x: x.count('.'))
    # df.set_index('dn')
    # df.sort_index()
    # df.to_csv('/tmp/training.csv')

    # training = cp.training
    # df = pd.DataFrame()
    # df.insert(0, 'dn', training.dn)
    # df.insert(1, 'label', training.label)
    # df.insert(2, 'level', training.dn.apply(lambda x: 0 if extractor(x).subdn == '' else extractor(x).subdn.count('.') + 1))
    # df = df.sort_values('level', ascending=False)
    # df.groupby(['label', 'level']).describe().to_csv('/tmp/training.side.csv')

    # df = pd.DataFrame()
    # df.insert(0, 'dn', training.dn)
    # df.insert(1, 'label', training.label)
    # df.insert(2, 'level', training.dn.apply(lambda x: 0 if extractor(x).subdn == '' else extractor(x).subdn.count('.') + 1))
    # df = df.sort_values('level', ascending=False)
    # df.to_csv('/tmp/training.bo.csv')
    # df.groupby(['label', 'level']).describe().to_csv('/tmp/training.describe.csv')

    # import os
    # if not os.path.exists('/tmp/all.csv'):
    #     dfs = []
    #     for idx, c in enumerate(cp.all_):
    #         df = pd.DataFrame(columns=['dn', 'level', 'type'])
    #         df['dn'] = c.dn
    #         df['level'] = c.dn.apply(lambda x: 0 if extractor(x).subdn == '' else extractor(x).subdn.count('.') + 1)
    #         df['type'] = [c.type] * df.shape[0]
    #         dfs.append(df)
    #         print(f'Made {idx}/{len(cp.all_)}')
    #     df = pd.concat(dfs, axis=0)
    #     df.to_csv('/tmp/all.csv')
    # else:
    #     df = pd.read_csv('/tmp/all.csv')

    # df.groupby(['level', 'type']).describe(include=['O']).to_csv(f'/tmp/all.describe.level_type.csv')
    # df.groupby(['type', 'level']).describe(include=['O']).to_csv(f'/tmp/all.describe.type_level.csv')
    # df.drop(columns='type').groupby(['level']).describe(include=['O']).to_csv(f'/tmp/all.describe.level.csv')



    # training describe for getting level dn distrubtion
    # c = cp.training
    # df = pd.DataFrame(columns=['dn', 'level', 'type'])
    # df['dn'] = c.dn
    # df['level'] = c.dn.apply(lambda x: 0 if extractor(x).subdn == '' else extractor(x).subdn.count('.') + 1)
    # df['type'] = c.label
    # df.groupby(['level', 'type']).describe(include=['O']).to_csv(f'/tmp/training.describe.level_type.csv')
    # df.groupby(['type', 'level']).describe(include=['O']).to_csv(f'/tmp/training.describe.type_level.csv')
    # df.drop(columns='type').groupby(['level']).describe(include=['O']).to_csv(f'/tmp/training.describe.level.csv')
    # df = df.loc[df['type'] == 'legit']
    # df.groupby(['level', 'type']).describe(include=['O']).to_csv(f'/tmp/training.LEGIT.describe.level_type.csv')
    # df.groupby(['type', 'level']).describe(include=['O']).to_csv(f'/tmp/training.LEGIT.describe.type_level.csv')
    # df.drop(columns='type').groupby(['level']).describe(include=['O']).to_csv(f'/tmp/training.LEGIT.describe.level.csv')

    # tld distribution
    # c = cp.training
    # df = pd.DataFrame(columns=['tld', 'level', 'type'])
    # df['dn'] = c.dn
    # df['tld'] = c.dn.apply(lambda x: extractor(x).suffix)
    # df['level'] = c.dn.apply(lambda x: 0 if extractor(x).subdn == '' else extractor(x).subdn.count('.') + 1)
    # df['type'] = c.label
    # df = df.groupby(['tld', 'type', 'level']).describe(include=['O'])
    # # df.to_csv(f'/tmp/training.tld.describe.level_type.csv')
    # df = df.reset_index(level='type')
    # legit = df.loc[df['type'] == 'legit']
    # dga = df.loc[df['type'] == 'dga']
    # legit.join(dga, lsuffix='_legit', rsuffix='_dga').to_csv('/tmp/training.tld.joined.describe.level_type.csv')

    # tld distribution without level
    # c = cp.training
    # df = pd.DataFrame(columns=['domian', 'tld', 'type'])
    # df['dn'] = c.dn
    # df['tld'] = c.dn.apply(lambda x: extractor(x).suffix)
    # df['type'] = c.label
    # df = df.groupby(['tld', 'type']).describe(include=['O'])
    # # df.to_csv(f'/tmp/training.tld.describe.level_type.csv')
    # df = df.reset_index(level='type')
    # legit = df.loc[df['type'] == 'legit']
    # dga = df.loc[df['type'] == 'dga']
    # legit.join(dga, lsuffix='_legit', rsuffix='_dga').to_csv('/tmp/training.tld.joined.describe.wo_level.csv')
    
    # df.groupby(['type', 'level']).describe(include=['O']).to_csv(f'/tmp/training.tld.describe.type_level.csv')
    # df.drop(columns='type').groupby(['level']).describe(include=['O']).to_csv(f'/tmp/training.tld.describe.level.csv')



    # normal-captures: dn-levels distribution
    # dfs = []
    # for idx, c in enumerate(list(cp.normal.values())):
    #     df = pd.DataFrame(columns=['dn', 'level', 'type'])
    #     df['dn'] = c.dn
    #     df['level'] = c.dn.apply(lambda x: 0 if extractor(x).subdn == '' else extractor(x).subdn.count('.') + 1)
    #     df['type'] = [c.type] * df.shape[0]
    #     dfs.append(df)
    # df = pd.concat(dfs, axis=0)
    # # df.groupby(['level', 'type']).describe(include=['O']).to_csv(f'/tmp/normal.describe.level_type.csv')
    # # df.groupby(['type', 'level']).describe(include=['O']).to_csv(f'/tmp/normal.describe.type_level.csv')
    # # df.drop(columns='type').groupby(['level']).describe(include=['O']).to_csv(f'/tmp/normal.describe.level.csv')

    # # df.groupby(['level', 'type']).describe(include='dn').to_csv(f'/tmp/all.describe.level_type.csv')
    # # df.groupby(['type', 'level']).describe(include='dn').to_csv(f'/tmp/all.describe.type_level.csv')
    # # df['dn'].apply(lambda x: extractor(x).dn).value_counts(normalize=True).to_csv(f'/tmp/all.unique.dn.csv')
    # # df['dn'].apply(lambda x: extractor(x).subdn).value_counts(normalize=True).to_csv(f'/tmp/all.unique.subdn.csv')
    # # df['dn'].value_counts(normalize=True).to_csv(f'/tmp/all.value_counts.csv')

    ## normal-captures: tld distribution
    # dfs = []
    # for idx, c in enumerate(list(cp.normal.values())):
    #     df = pd.DataFrame(columns=['dn', 'level', 'type'])
    #     df['dn'] = c.dn
    #     df['tld'] = c.dn.apply(lambda x: extractor(x).suffix.lower())
    #     df['type'] = [c.type] * df.shape[0]
    #     dfs.append(df)
    # df = pd.concat(dfs, axis=0)
    # # df = df.groupby(['tld', 'type']).describe(include=['O']).to_csv('/tmp/normals.tld.joined.describe.wo_level.csv')
    # df = df.drop_duplicates().groupby(['tld', 'type']).describe(include=['O']).to_csv('/tmp/normals.tld.no_dup.describe.wo_level.csv')

    ## botnet-captures: tld distribution
    # dfs = []
    # for idx, c in enumerate(list(cp.botnet.values())):
    #     df = pd.DataFrame(columns=['dn', 'level', 'type'])
    #     df['dn'] = c.dn
    #     df['tld'] = c.dn.apply(lambda x: extractor(x).suffix.lower())
    #     df['type'] = [c.type] * df.shape[0]
    #     dfs.append(df)
    # df = pd.concat(dfs, axis=0)
    # df = df.groupby(['tld', 'type']).describe(include=['O']).to_csv('/tmp/botnets.tld.joined.describe.wo_level.csv')
    # df = df.drop_duplicates().groupby(['tld', 'type']).describe(include=['O']).to_csv('/tmp/botnets.tld.no_dup.describe.wo_level.csv')

    # normal-outputs: tld distribution
    # dfs = []
    # model = Model.Model.load(DomainLevel.SIDE)
    # for idx, c in enumerate(list(cp.botnet.values())):
    #     input = Input
    #     df = pd.DataFrame(columns=['dn', 'level', 'type'])
    #     df['dn'] = c.dn
    #     df['tld'] = c.dn.apply(lambda x: extractor(x).suffix.lower())
    #     df['type'] = [c.type] * df.shape[0]
    #     dfs.append(df)
    # df = pd.concat(dfs, axis=0)
    # df = df.groupby(['tld', 'type']).describe(include=['O']).to_csv('/tmp/botnets.tld.joined.describe.wo_level.csv')
    # df = df.drop_duplicates().groupby(['tld', 'type']).describe(include=['O']).to_csv('/tmp/botnets.tld.no_dup.describe.wo_level.csv')

    # normal-outputs: tld distribution
    # model_side = Model.Model.load(DomainLevel.SIDE)
    # model_lower = Model.Model.load(DomainLevel.LOWER)
    # models = [Model.Model.load(DomainLevel.SIDE), Model.Model.load(DomainLevel.LOWER)]
    # dfs = []
    # for idx, c in enumerate(list(cp.normal.values())):
    #     input_view_s = InputView(c, model_side.dl)
    #     input_view_l = InputView(c, model_lower.dl)
    #     output_view_ss = OutputView(input_view_s, model_side)
    #     output_view_ll = OutputView(input_view_l, model_lower)
    #     df = pd.DataFrame(columns=['dn', 'tld'])
    #     df['dn'] = c.dn
    #     df['tld'] = c.dn.apply(lambda x: extractor(x).suffix.lower())
    #     df['legit_ss'] = output_view_ss.legit
    #     df['legit_ll'] = output_view_ll.legit
    #     df['dga_ss'] = output_view_ss.dga
    #     df['dga_ll'] = output_view_ll.dga
    #     dfs.append(df)
    # df = pd.concat(dfs, axis=0, ignore_index=True)
    # df = df.drop_duplicates()
    # df = df.groupby(['tld']).agg(['sum'])
    # df.drop(columns='dn', inplace=True)
    # df.columns = df.columns.droplevel(level=1)
    # df['tot'] = df.loc[:, ['legit_ss', 'dga_ss']].sum(axis=1)
    # df['legit_ss_p'] = df['legit_ss'].divide(df['tot'])
    # df['dga_ss_p'] = df['dga_ss'].divide(df['tot'])
    # df['legit_ll_p'] = df['legit_ll'].divide(df['tot'])
    # df['dga_ll_p'] = df['dga_ll'].divide(df['tot'])
    # df.reindex(['tot', 'legit_ss', 'dga_ss', 'legit_ll', 'dga_ll', 'legit_ss_p', 'dga_ss_p', 'legit_ll_p', 'dga_ll_p'], axis=1).to_csv('/tmp/predicted.normal.2.tld.csv')


    # normal-outputs: tld distribution
    # model_side = Model.Model.load(DomainLevel.SIDE)
    # model_lower = Model.Model.load(DomainLevel.LOWER)
    # models = [Model.Model.load(DomainLevel.SIDE), Model.Model.load(DomainLevel.LOWER)]
    # dfs = []
    # for idx, c in enumerate(list(cp.normal.values())):
    #     input_view_s = InputView(c, model_side.dl)
    #     input_view_l = InputView(c, DomainLevel.DOMAIN)
    #     output_view_ss = OutputView(input_view_s, model_side)
    #     output_view_ll = OutputView(input_view_l, model_lower)
    #     df = pd.DataFrame(columns=['dn', 'tld'])
    #     df['dn'] = c.dn
    #     df['tld'] = c.dn.apply(lambda x: extractor(x).suffix.lower())
    #     df['input_ss'] = c.dn.apply(DomainLevel.SIDE.translate)
    #     df['len_ss'] = df['input_ss'].apply(len)
    #     df['dga_ss'] = output_view_ss.dga
    #     df['input_ll'] = c.dn.apply(DomainLevel.DOMAIN.translate)
    #     df['len_ll'] = df['input_ll'].apply(len)
    #     df['dga_ll'] = output_view_ll.dga
    #     dfs.append(df)

    # tld distribution without level
    def boo(x):
        dom = x['dn']
        p = dom.rfind('.')
        doms = {sfx: ('%s.%s' % (dom[:p], sfx)) for sfx in sfxs}
        return {**x, **doms}

    # sfxs = ['net','org','ru','info','eu','de', 'in']

    # if not os.path.exists('/tmp/training_sampled.only_com.csv'):
    #     c = cp.training
    #     df = pd.DataFrame()
    #     df = c.readable
    #     df['tld'] = c.tld
    #     df = df.loc[df['tld'] == 'com'].sample(n=10000, axis=0)
    #     df = df.apply(boo, axis=1)
    #     df = pd.DataFrame.from_records(df.values)
    #     df.to_csv('/tmp/training_sampled.only_com.csv')
    # else:
    #     df = pd.read_csv('/tmp/training_sampled.only_com.csv')

    # model = Model.Model.load(DomainLevel.LOWER)
    # df = df.drop('class', axis=1)
    # for dl in [DomainLevel.LOWERT, DomainLevel.DOMAINT, DomainLevel.WHOLE]:
    #     _df = df.copy()
    #     for l in ['dn'] + sfxs:
    #         X_str = df[l].apply(dl.translate)
    #         X_num = X_str.apply(model.dn2input).to_numpy()
    #         X = pad_sequences(X_num, maxlen=model.MAXLEN, truncating='post')
    #         _df[l], _ = model.predict(X)
    #     # _df.groupby(['label', 'class']).agg(['count', 'mean']).to_csv('/tmp/training_sampled.%s_%s.tld.csv' % (dl.name, model.dl.name))
    #     _df.groupby(['label']).agg(['count', 'mean']).to_csv('/tmp/training_sampled.%s_%s.tld.csv' % (dl.name, model.dl.name))


    # 

    suffixes = ['biz', 'cc', 'cn', 'co.uk', 'com', 'de', 'eu', 'in', 'info', 'me', 'net', 'org', 'pw', 'ru', 'su', 'top', 'tv', 'tw']

    bo = pd.DataFrame()
    if not os.path.exists('/tmp/training_sampled.csv'):
        c = cp.training
        df = pd.DataFrame()
        df = c.readable
        df['tld'] = c.tld
        for suffix in suffixes:
            df2 = df.loc[df['tld'] == suffix]
            df2 = df2.sample(n=(df2.shape[0] if df2.shape[0] < 5000 else 5000), axis=0)
            for suffix2 in suffixes:
                df2[suffix2] = df2['dn'].apply(lambda x, s: '%s.%s' % (x[:x.rfind('.')], s), args=(suffix2,))
                print(df2)
            bo = bo.append(df2)

        bo.to_csv('/tmp/training_sampled.csv')
    else:
        bo = pd.read_csv('/tmp/training_sampled.csv')

    models = [ Model.Model.load(DomainLevel.LOWERT), Model.Model.load(DomainLevel.LOWER) ]
    for model in models:
        for dl in [DomainLevel.LOWERT, DomainLevel.DOMAINT, DomainLevel.WHOLE]:
            df = bo.copy()
            if not os.path.exists(f'/tmp/training_sampled.{dl.name}_{model.dl.name}.tld.csv'):
                for l in suffixes:
                    X_str = df[l].apply(dl.translate)
                    X_num = X_str.apply(model.dn2input).to_numpy()
                    X = pad_sequences(X_num, maxlen=model.MAXLEN, truncating='post')
                    df[l], _ = model.predict(X)
                df.to_csv(f'/tmp/training_sampled.{dl.name}_{model.dl.name}.tld.csv')
            else:
                df = pd.read_csv(f'/tmp/training_sampled.{dl.name}_{model.dl.name}.tld.csv')

            if True:
                com = df.loc[df['tld'] == 'com'].to_csv(f'/tmp/training_sampled.{dl.name}_{model.dl.name}.tld.COM.csv')
            else:
                aggs = {s: 'mean' for s in suffixes}
                aggs['dn'] = 'count'
                dff = df.groupby(['label', 'tld']).agg(aggs)
                dff.to_csv(f'/tmp/training_sampled.{dl.name}_{model.dl.name}.tld.grouped.wo_sub.csv')
                counts = dff['dn'].copy()
                for l in ['dga', 'legit']:
                    for s_left in suffixes:
                        pivot = dff.loc[(l, s_left)][s_left]
                        dff.loc[(l, s_left)] = dff.loc[(l, s_left)].sub(pivot)
                dff['dn'] = counts
                dff.to_csv(f'/tmp/training_sampled.{dl.name}_{model.dl.name}.tld.grouped.csv')

                # dff = df.groupby(['tld']).agg(aggs)
                # counts = dff['dn'].copy()
                # for s_left in suffixes:
                #     pivot = dff.loc[s_left][s_left]
                #     dff.loc[s_left] = dff.loc[s_left].rsub(pivot)
                # dff['dn'] = counts
                # dff.to_csv(f'/tmp/training_sampled.{dl.name}_{model.dl.name}.tld.no_split.grouped.csv')
    pass