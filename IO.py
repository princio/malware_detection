import csv, os, numpy as np
from enum import Enum
from math import floor
from DomainLevel import Transformer, extractor
import json
import re
import pandas as pd
from keras.preprocessing.sequence import pad_sequences

def is_right(x):
    distance = abs((0 if x['label'] == 'legit' else 1) - x['probability'])
    rightness = 1 if (x['label'] == 'legit' and x['probability'] < 0.5) or (x['label'] == 'dga' and x['probability'] > 0.5) else 0
    return rightness, distance

def normalcapture_wrongness(x):
    rightness = 1 if x > 0.5 else 0
    distance = 0 if x <= 0.5 else x-0.5
    return rightness, distance


class Input:
    """
    Represent the class that take a Capture (pcap) and transform it into
    a list of dn appropriately modified.
    This is useful because if you have to use the same Input more than once
    you don't have to transform the whole Capture each time.
    """
    def __init__(self, capture, transformation):
        self.capture = capture
        self.transformation = transformation
        self.name = self.capture.name + '.' + self.transformation.name
        self.input = self.capture.dn.apply(self.transformation.translator()).rename('input')
        dir = f'predictions/{self.capture.name}/transformation_{self.transformation.name}/'
        path = f'{dir}/{self.name}.csv'
        if not os.path.exists(path):
            os.makedirs(dir, exist_ok=True)
            pd.concat([self.capture.readable, self.input], axis=1).to_csv(path)
        pass

    @property
    def input_np(self):
        return self.input.to_numpy()

    @property
    def labeled(self):
        return self.capture.labeled

    @property
    def dn(self):
        return self.capture.dn

    @property
    def tlds(self):
        return self.capture.transformations

    @property
    def df(self):
        return pd.concat([self.dn, pd.Series(self.input, name='input')], axis=1)

class DF:
    def __init__(self, dn, input, prediction):
        self.dn = dn
        self._df = pd.DataFrame(data={'X': input, 'Y': prediction})
        self._unique = self._df.drop_duplicates()
        self.DF = self._df
        self.threshold = 0.5
        pass

    def __getitem__(self, key):
        return self.DF[key]

    def is_unique(self):
        return self.DF == self._unique

    def unique(self, unique=True):
        self.DF = self._unique if unique else self._df
        return self

    def with_dn(self):
        index = pd.MultiIndex.from_arrays([pd.RangeIndex(stop=self.dn.shape[0]), self.dn.to_list()])
        self.DF = self._df.set_index(index)
        return self

    def reset(self):
        self.DF = self._df
        return self

    def legit(self):
        self.DF = self.DF.loc[self.DF['Y'] < self.threshold]
        return self

    def dga(self):
        self.DF = self.DF.loc[self.DF['Y'] >= self.threshold]
        return self

    @property
    def dga_frac(self):
        return self.dga().DF.shape[0] / self.DF.shape[0]

    @property
    def legit_frac(self):
        return self.legit().DF.shape[0] / self.DF.shape[0]

    @property
    def level(self):
        def count(x):
            c = 1 + Transformer.nosfx(x).count('.')
            return str(c) if c <= 2 else '>3'
        DN_level = self.DF['X'].apply(count)
        return pd.DataFrame({'X': self.DF['X'], 'level': DN_level})

    @property
    def level_dn(self):
        def count(x):
            c = 1 + Transformer.nosfx(x).count('.')
            return str(c) if c <= 2 else '>3'
        dn = self.dn.iloc[self.DF['X'].index]
        DN_level = dn.apply(count)
        return pd.DataFrame({'dn': dn, 'level': DN_level, 'Y': self.DF['Y']})

    @property
    def level_count(self):
        dff = self.level.value_counts(subset='level', normalize=True, sort=False)
        return dff

    def level_count_dn(self, normalize=True):
        dff = self.level_dn.value_counts(subset='level', normalize=normalize, sort=False)
        return dff

    @staticmethod
    def compare(outputs):
        output = outputs.pop(0)
        df = DF(output.input.capture.dn, output.input.input, output.prediction)
        dff = df.reset().unique().with_dn().dga().DF[['Y']]
        for output in outputs:
            df = DF(output.input.capture.dn, output.input.input, output.prediction)
            dff = dff.join(df.unique().with_dn().dga().DF['Y'], rsuffix=(output.input.capture.name, output.input.transformation.name, output.model.name, 'X'))
            # d[(output.input.capture.name, output.input.transformation.name, output.model.name, 'X')] = df['X']
            # d[(output.input.capture.name, output.input.transformation.name, output.model.name, 'Y')] = df['Y']
        dff.to_csv('/tmp/ciao.csv')

class Output:
    def __init__(self, input, model, prediction, time_elapsed = None):
        self.input = input
        self.model = model
        self.prediction = prediction
        self.time_elapsed = time_elapsed
        self.name = self.input.name + '.' + self.model.name
        self.df = pd.DataFrame({'X': self.input.input, 'Y': self.prediction })
        self.DF = DF(input.capture.dn, input.input, prediction)
        self.threshold = 0.5
        pass

    # @property
    # def unique(self):
    #     return self.df.drop_duplicates()

    # def dga(self, unique = False):
    #     df = self.df.drop_duplicates() if unique else self.df
    #     return df.loc[df['Y'] >= self.threshold]

    # def legit(self, unique = False):
    #     df = self.df.drop_duplicates() if unique else self.df
    #     return df.loc[df['Y'] < self.threshold]

    # def fractions(self, unique=True):
    #     dgas = self.dga.shape[0]
    #     legits = self.legit.shape[0]
    #     dgas_u = self.dga.drop_duplicates().shape[0]
    #     legits_u = self.legit.drop_duplicates().shape[0]
    #     total = self.df.shape[0]
    #     total_u = self.unique.shape[0]
    #     return {'no-unique': {'dga': dgas/total, 'legit': legits/total},
    #             'unique': {'dga': dgas_u/total_u, 'legit': legits_u/total_u}}
    @staticmethod
    def predict(input, model, redo = False):
        dir = f'predictions/{input.capture.name}/transformation_{input.transformation.name}/'
        path = f'{dir}{input.name}.{model.name}.csv'

        if not redo and os.path.exists(path):
            prediction = pd.read_csv(path)['prediction']
            with open(f'{dir}/{input.name}.{model.name}.time', 'r') as f:
                time_elapsed = json.load(f)
        else:
            os.makedirs(dir, exist_ok=True)
            predictions, time_elapsed = model.predict(input.input_np)
            prediction = pd.Series(predictions, name='prediction')
            prediction.to_csv(path)
            with open(f'{dir}/{input.name}.{model.name}.time', 'w') as f:
                json.dump(time_elapsed, f)

        return Output(input, model, prediction, time_elapsed)

