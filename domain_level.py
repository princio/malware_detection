import numpy as np
import Paths
import csv, os
from plot_num import plot_num
from DataHandler import Prediction, Predictor, Capture
import tldextract
from Model import Model
from DomainLevel import DomainLevel
import pandas as pd

dls = [DomainLevel.LOWERT, DomainLevel.LOWER, DomainLevel.BOTH]
models = [ Model.load(DomainLevel.LOWERT), Model.load(DomainLevel.LOWER)]

def is_right(x):
    a = []
    for dl in dls:
        for model in models:
            id = f"{model.dl.name}_{dl.name}"
            rightness = (x['class'] == 'legit' and x[id] < 0.5) or (x['class'] == 'dga' and x[id] > 0.5)
            a.append(1 if rightness else 0)

    return a

def test_tld_importance():
    redo = True
    if redo or Prediction.load:
        capture = Capture.load('training')
        dataset = pd.read_csv('datasets/training.csv')
        for dl in dls:
            predictor = Predictor(dl, capture)
            for model in models:
                print(f'Predicting with model {model.dl.name} and dl {dl.name}')
                prediction = predictor.predict(model)
                dataset[f"{model.dl.name}_{dl.name}"] = prediction.probabilities
        dataset.to_csv('output/training.tested.csv')
    else:
        dataset = pd.read_csv('datasets/training.tested.csv')

    redo = False
    if redo or not os.path.exists('datasets/training.tested.2.csv'):
        bo = list(zip(*dataset.apply(is_right, axis=1)))
        c = 0
        for dl in dls:
            for model in models:
                dataset.insert(8 + c*2,  column=f'{model.dl.name}_{dl.name}_right', value=bo[c])
                c += 1
        dataset.to_csv('datasets/training.tested.2.csv')
    else:
        dataset = pd.read_csv('datasets/training.tested.2.csv')

    redo = False
    if redo or not os.path.exists('datasets/training.tested.2.grp.csv'):
        dataset_grp = dataset.groupby(["tld"]).sum()
        dataset_grp['tot'] = dataset["tld"].value_counts(sort=False)
        dataset_grp.to_csv('datasets/training.tested.2.grp.csv')

    redo = True
    if redo or not os.path.exists('datasets/training.tested.2.grp.mean.csv'):
        dataset_grp = dataset.groupby(["tld"]).mean()
        dataset_grp.insert(3, column='tot', value=dataset["tld"].value_counts(sort=False))
        dataset_grp.to_csv('datasets/training.tested.2.grp.mean.csv')


    # datset_grp = dataset.groupby(["tld"]).mean()

    # dataset['right_lowert'], dataset['right_lower'] = zip(*dataset.apply(is_right, axis=1))
    
    # datset_grp = dataset.groupby(["tld"]).mean()

    # datset_grp.to_csv('datasets/dataset_new.test2.csv')

    # print('done')

    pass






if __name__ == "__main__":


    # capture = Capture.from_path('datasets/training_capture.csv')
    # dataset = pd.read_csv('output/dataset_new.lower.test.csv')
    # model = Model.from_name('lowert')
    # dl = DomainLevel.LOWERT
    # prediction = Prediction(model, dl, capture)
    # dataset[f"{model.dl.name}_{dl.name}"] = prediction.predict()
    # dataset.to_csv('output/dataset_new.lower.test3.csv')

    test_tld_importance()

    # for dl in DomainLevel.all():
    #     print(dl.name, dl.translate('google.com'))

    # extractor = tldextract.TLDExtract(extra_suffixes=['bit'])

    # bt = pd.read_csv('datasets/setAtrain_h_t_domins.csv', names=['class', 'dga', 'bo', 'dns'])
    # bt.drop('bo', axis=1, inplace=True)
    # bt = pd.concat([bt, pd.DataFrame(columns=['subdomain', 'domain', 'tld'])])
    # bt['subdomain'], bt['domain'], bt['tld'] = zip(*bt['dns'].apply(extractor))
    # bt.sort_values(['tld', 'domain', 'subdomain'], inplace=True)
    # bt.to_csv('datasets/dataset_new_2.csv')

    # bt = pd.read_csv('datasets/dataset_new.csv')
    # bt = pd.concat([pd.DataFrame(columns=['num', 'time']), bt['dns']])
    # bt.to_csv('datasets/training_capture.csv')
    # print('done')


    # bt = pd.read_csv('datasets/setAtrain_h_t_domins.csv', names=['class', 'dga', 'bo', 'domain'])
    # bt.drop('bo', axis=1)
    # bt['subdomain'] = bt['domain'].apply(lambda domain: len(tldextract.extract(domain).subdomain) > 0)
    # bt.to_csv('datasets/subdomains.csv')
    # bt = bt.groupby(["class", "subdomain"], sort=True).count()
    # bt.to_csv('datasets/subdomains.grouped.csv')


    # bt = pd.read_csv('datasets/setAtrain_h_t_domins.csv', names=['class', 'dga', 'bo', 'domain'])
    # bt.drop('bo', axis=1)
    # bt['domain_len'] = bt['domain'].apply(lambda domain: len(tldextract.extract(domain).domain))
    # bt['subdomain_len'] = bt['domain'].apply(lambda domain: len(tldextract.extract(domain).subdomain))
    # bt['len'] = bt['domain'].apply(lambda domain: len(domain))
    # bt['training_len'] = bt['domain'].apply(lambda domain: len(DomainLevel.LOWERT.translate(domain)))
    # bt.to_csv('datasets/len.csv')
    # bt = bt.groupby(["class", 'dga'], sort=True).mean()
    # bt.to_csv('datasets/len.grouped.csv')

    # bt = pd.read_csv('datasets/domains.csv', names=['class', 'dga', 'domain', 'domain0'])
    # bt = bt.groupby("domain0", sort=True).count()
    # bt.to_csv('datasets/domains.grouped.csv')

    # bt = pd.read_csv('datasets/tlds.csv', names=[ 'class', 'dga', 'bo', 'domain', 'tld' ])
    # bt.drop(['domain', 'bo'], axis=1, inplace=True)

    # dgas = bt.loc[bt['class'] == 'dga']
    # legits = bt.loc[bt['class'] == 'legit']

    # dgas = dgas.groupby("tld", sort=True, squeeze=True).count()
    # legits = legits.groupby("tld", sort=True, squeeze=True).count()
    
    # dgas.drop('class', axis=1, inplace=True)
    # legits.drop('class', axis=1, inplace=True)

    # dgas.columns = ['dga']
    # legits.columns = ['legit']

    # a = pd.concat([dgas, legits], axis=1)
    # # bo = bt.groupby(["class", "tld"]).count()

    # # bo.to_csv('datasets/tlds2.csv')

    # print(dgas)
    # print(legits)
    # print(a.to_markdown())
    # a.to_csv('datasets/tlds3.csv')
    # print('pd')