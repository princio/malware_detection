import os, json
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
from datetime import datetime
from PyInquirer import prompt
import csv
import hashlib
import re
import psycopg2
import numpy as np
from Model import Model
from Report import Report
from Dataset import Dataset
from Repository import DatasetRepository, ModelRepository, MetricsRepository
from Metrics import Metrics
import random
import pickle

random.seed(datetime.now().timestamp())
connection = psycopg2.connect(user = "postgres",
                                  password = "porcodio",
                                  host = "127.0.0.1",
                                  port = "5432",
                                  database = "malware")

def generate_balanced_data(benign_rows, dataset):
    while (dataset.supports[1] - dataset.supports[0]) > 10:
        b_r = random.randrange(len(benign_rows))
        dataset.supports[0 if benign_rows[b_r][0] == 'legit' else 1] += 1
        b = random.randrange(len(dataset._data))
        dataset._data.insert(b, benign_rows[b_r])

if __name__ == "__main__":
    sf = "%5s " + ("| %1.6f %1.6f %1.6f %5d " * 4) + "| %f"
    scm = "%5s | %6d %6d |\n      | %6d %6d |"

    models = ModelRepository.fetchall(connection)
    datasets = DatasetRepository.fetchmany(connection, ('a9ef66ec-afad-4455-9e4d-1da2cc261b33', '00254ec9-d5fe-4f77-8f6a-5c9131d10480', '3b7ad471-d1c6-442e-b7df-f39088eaf8ae', '3d63b3fe-52cb-4a48-a497-89428229134b', 'f2e70ef0-891b-480a-ac61-b13541180bff'))
    training_dataset = DatasetRepository.fetchone(connection, '2039b45b-14aa-46a2-8ca0-720737761d09')

    overwrite = True
    if overwrite is False:
        datasets_pickle = None
        try:
            datasets_pickle = pickle.load(open(f'output/pickles/balanced_problem/datasets.p', 'rb'))
            metrics = pickle.load(open(f'output/pickles/balanced_problem/metrics.p', 'rb'))
        except:
            datasets_pickle = None
        
    for dataset in datasets:    
        data_or = dataset.data().copy()
        supports_or = dataset.supports.copy()
        benign_rows = []
        with open('datasets/benign_h_t/4_2015-03-24_capture1-only-dns.pcap_table.csv__h_t.csv') as csvfile:
            spamreader = csv.reader(csvfile, delimiter=',')
            for row in spamreader:
                benign_rows.append(row)
        

        TEST_N = 3

        if overwrite or datasets_pickle is None:
            datasets = []
            metrics = []
            for model in list(models.values())[0:2]:
                if model.training_confusion_matrix is not None:
                    print(f'Testing balanced dataset {dataset.name} with {model.name()}...')
                    training_metrics = Metrics.create(model.classes, training_dataset.dga, model.training_confusion_matrix)
                    dataset._data = data_or.copy()
                    dataset.supports = supports_or.copy()
                    dataset_metrics = Metrics.create(model.classes, dataset.dga, model.test(dataset)[0])
                    print(dataset_metrics.str2(f'or'))
                    for i in range(TEST_N):
                        generate_balanced_data(benign_rows, dataset)
                        dataset_metrics = Metrics.create(model.classes, dataset.dga, model.test(dataset)[0])
                        print(dataset_metrics.str2(f'bl_{i}'))

                        datasets.append(dataset)
                        metrics.append(dataset_metrics.tolist())
                        
                        print('')
                    metrics.append(training_metrics.tolist())
                    print(training_metrics.str2(f'train', print_dga=False))
                    # print(training_metrics.str_confusionmatrix(f't_{model.name()}'))
                    # print(sf % tuple([f't_{model.name()}'] + training_metrics.tolist()))
                    print('')
            pickle.dump(datasets, open(f'output/pickles/balanced_problem/datasets.p', 'wb'))
            pickle.dump(metrics, open(f'output/pickles/balanced_problem/metrics.p', 'wb'))
        else:
            for i in range(len(metrics)-1):
                a = np.asfarray(metrics[i])
                b = np.asfarray(metrics[-1])
                print(sf % tuple([f'#{i}'] + (b-a).tolist()))
            print(sf % tuple(['train'] + metrics[-1]))

    connection.close()
