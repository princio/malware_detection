import json
import time
from datetime import datetime
from PyInquirer import prompt, Separator
import os, fnmatch
import itertools
from collections import OrderedDict
import numpy as np
import csv
import pandas as pd
from keras.models import model_from_json
from keras.preprocessing import sequence
from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score
import joblib
from sklearn.metrics import confusion_matrix
import uuid

with open('databases/current/datasets.json') as fp_datasets:
    datasets = json.load(fp_datasets)

with open('databases/current/models.json') as fp_models:
    models = json.load(fp_models)

reports = {}
reports_by_model = {}
reports_by_version = {}
reports_by_dataset = {}

def evaluate(y_test, y_pred, f_report, time_elapsed, valid_class, flag_bin, model, version, dataset):
    target_names = list(valid_class.keys())

    y_test = np.array(y_test)
    y_test = y_test.ravel()

    class_report_dict = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)

    class_report_dict["macro"] = {
        "precision": precision_score(y_test, y_pred, average="macro"),
        "f1-score": f1_score(y_test, y_pred, average="macro"),
        "recall": recall_score(y_test, y_pred, average="macro")
    }
    class_report_dict["micro"] = {
        "precision": precision_score(y_test, y_pred, average="micro"),
        "f1-score": f1_score(y_test, y_pred, average="micro"),
        "recall": recall_score(y_test, y_pred, average="micro")
    }

    c_matrix = np.ravel(confusion_matrix(y_test, y_pred).astype('float'))
    class_report_dict["confusion_matrix"] = np.ravel(c_matrix.astype('float'))
    class_report_dict["true_positive"] = np.sum(np.diag(c_matrix))

    if not model['_id'] in reports_by_model:
        reports_by_model[model['_id']] = []
    if not version['_id'] in reports_by_version:
        reports_by_version[version['_id']] = []
    if not dataset['_id'] in reports_by_dataset:
        reports_by_dataset[dataset['_id']] = []

    class_report_dict['model_id'] = model['_id']
    class_report_dict['version_id'] = version['_id']
    class_report_dict['dataset_id'] = dataset['_id']
    class_report_dict['inputs_number'] = dataset['inputs_number']
    class_report_dict['inputs_number2'] = len(y_test)

    report_id = str(uuid.uuid4())
    reports[report_id] = class_report_dict
    reports_by_model[model['_id']].append(report_id)
    reports_by_version[version['_id']].append(report_id)
    reports_by_dataset[dataset['_id']].append(report_id)

    return class_report_dict


def get_data(dataset_path):
    """Read data from file (Traning, testing and validation) to process"""
    data = []
    with open(dataset_path, "r") as f:
        reader = csv.reader(f)
        for row in reader:
            data.append(row)
    return data

def find(pattern, path):
    print(path)
    for root, dirs, files in os.walk(path):
        for name in files:
            if fnmatch.fnmatch(name, pattern):
                return os.path.splitext(os.path.join(root, name))[0]
    raise Exception("Pattern <%s> not found in: %s" % (pattern, path))


class Model:
    def __init__(self, model_fetched):
        self.id = model_fetched[0]
        self.path = model_fetched[1]
        self.hash = model_fetched[2]
        self.date = model_fetched[3]
        self.epochs = model_fetched[4]
        self.folds = model_fetched[5]
        self.epoch = model_fetched[6]
        self.fold = model_fetched[7]
        self.instance = None

class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        print(f"current obj is {obj}")
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            print(f"{obj} is np.floating")
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, uuid.UUID):
            return str(obj)
        else:
            return super(NpEncoder, self).default(obj)

def load_model(model_path):
    with  open("%s.json" % model_path, 'r') as json_file:
        loaded_model_json = json_file.read()

    model = model_from_json(loaded_model_json)
    model.load_weights("%s.h5" % model_path)

    return model


def run(date):
    for model in models:
        for version in model['versions']:
            model_ai = load_model(version['path'])
            for dataset in datasets['malware']:
                d = datetime.fromtimestamp(model['date']).strftime('%Y%m%d-%H%M%S')
                print(f"testing model {d}/{model['epochs']}/{model['folds']}, version {version['fold']}/{version['epoch']} with dataset {dataset['name']} of length {dataset['inputs_number']}")
                
                indata = get_data(f"datasets/malware_h_t/{dataset['name']}")
                binary_labels = [x[0] for x in indata]  # legit o dns
                X = [x[2] for x in indata]  # DNS
                # labels = [x[1] for x in indata]  # famiglia dga

                valid_chars = {x: idx + 1 for idx, x in enumerate(OrderedDict.fromkeys(''.join(X)))}
                maxlen = np.max([len(x) for x in X])

                X_test = [[valid_chars[y] for y in x] for x in X]
                X_test = sequence.pad_sequences(X_test, maxlen=maxlen)

                y_binary = np.array([0 if x == 'legit' else 1 for x in binary_labels])

                time_elapsed = time.time()
                y_pred = model_ai.predict(X_test)
                time_elapsed = time.time() - time_elapsed

                y_bin_result = [0 if (x <= 0.5) else 1 for x in y_pred]

                with open("%s.txt" % version['path'], 'w') as f_report:
                    evaluate(y_binary, y_bin_result, f_report, time_elapsed, {'white': 0, 'black': 1}, True, model, version, dataset)

    with open('databases/current/reports.json', 'w') as fp_reports:
        json.dump(reports, fp_reports, cls=NpEncoder)
    with open('databases/current/reports_by_dataset.json', 'w') as fp_reports:
        json.dump(reports_by_dataset, fp_reports, cls=NpEncoder)
    with open('databases/current/reports_by_model.json', 'w') as fp_reports:
        json.dump(reports_by_model, fp_reports, cls=NpEncoder)
    with open('databases/current/reports_by_version.json', 'w') as fp_reports:
        json.dump(reports_by_version, fp_reports, cls=NpEncoder)

def average():
    with open('databases/current/reports.json', 'r') as fp:
        reports = json.load(fp)
    with open('databases/current/reports_by_dataset.json', 'r') as fp:
        reports_by_dataset = json.load(fp)
    with open('databases/current/reports_by_model.json', 'r') as fp:
        reports_by_model= json.load(fp)
    with open('databases/current/reports_by_version.json', 'r') as fp:
        reports_by_version = json.load(fp)

    models = {}

    for model_id in reports_by_model:
        for report_id in reports_by_model[model_id]:
            models[model_id] = { 'average': {} }
            sum(models[model_id], reports[report_id], 'white')
            sum(models[model_id], reports[report_id], 'black')
            sum(models[model_id], reports[report_id], 'micro')
            sum(models[model_id], reports[report_id], 'macro')
            sum(models[model_id], reports[report_id], 'macro avg')
            sum(models[model_id], reports[report_id], 'weighted avg')
            pass
        n = len(reports_by_model[model_id])
        avg(models[model_id], 'white', n)
        avg(models[model_id], 'black', n)
        avg(models[model_id], 'micro', n)
        avg(models[model_id], 'macro', n)
        avg(models[model_id], 'macro avg', n)
        avg(models[model_id], 'weighted avg', n)

    print(models)

def sum(model, data, name):
    if not name in model['average']:
        model['average'][name] = {}
    
    for measure_name in data[name]:
        if not measure_name in model['average'][name]:
            model['average'][name][measure_name] = 0
        model['average'][name][measure_name] = model['average'][name][measure_name] + data[name][measure_name]

def avg(model, name, n):
    for measure_name in model['average'][name]:
        model['average'][name][measure_name] = model['average'][name][measure_name]/n

date = datetime.now()
if __name__ == "__main__":
    average()
    # run(date)