import pyshark
import os
from enum import Enum
import pprint
import pandas as pd
import time
# from Model import Model
from Extractor import Extractor

df_10m = pd.read_csv('top_10m.dsfx.csv')
df_10m_set = set(df_10m.dsfx.values)

pp = pprint.PrettyPrinter(indent=4)

ROOT_DIR = '/media/ssd512/stratosphere/'
def root_join(path, *paths):
    return os.path.join(ROOT_DIR, path, *paths)

class Capture_Packet_Type(Enum):
    RESP = 0
    REQ  = 1
    BOTH = 2

class Capture_Type(Enum):
    NORMAL = 0
    BOTNET = 1
    MIXED  = 2
    
class Capture_Version(Enum):
    FULL_PCAP = 0
    DNS_PCAP  = 1
    DNS_CSV   = 2
    DN_CSV    = 3
    
class Capture_Prediction(Enum):
    Y         = 0
    Y_UNIQUE  = 1

    
class PcapCapture():
    BUFFER = 10_00

    def __init__(self, path):
        self.path = path
        self.path_csv = path + '.csv'
        self.path_y_csv = path + '.y.csv'
        self.name = os.path.basename(path)
        self.chosen_dns_fields = [ 'count_labels', 'count_queries', 'flags_authenticated', 'flags_response', 'qry_name', 'qry_type', 'qry_class','resp_ttl', 'resp_name', 'response_to', 'time', 'a', 'aaaa' ]

    def generate_csv(self, only_dn=False, packet_type=Capture_Packet_Type.REQ, pkt_num = 0, timeout = 0):
        tshark_cap = pyshark.FileCapture(self.path, keep_packets=False, display_filter="dns && !_ws.malformed && !icmp")
        i = 0
        data = []
        first = True
        time_begin = time.time()
        legit = True
        
        chosen_dns_fields = (['qry_name'] if only_dn else self.chosen_dns_fields.copy())
        columns = ['frame_number', 'timestamp'] + chosen_dns_fields#  + ['legit']
        if legit:
            columns += ['legit']

        def __flush():
            df = pd.DataFrame(data, index=pd.RangeIndex(i -  len(data), i), columns=columns)
            df.to_csv(self.path + '.csv', mode='w' if first else 'a', header=first)
            print('saving...', i)

        def __packet2row(packet):
            if 'DNS' not in packet:
                return None
            qry = packet['DNS'].get_field('qry_name')
            if not Extractor.is_dn(qry):
                return None
            dns_layer = packet['DNS']
            row = [packet.number, packet.sniff_time.timestamp()]
            row += [dns_layer.get_field(cf) for cf in chosen_dns_fields]
            if legit:
                row += [1 if Extractor.domain_sfx(qry) in df_10m_set else 0]
            return row

        while True:
            try:
                packet = tshark_cap.next()
                row = __packet2row(packet)
                if row is None:
                    continue
                data.append(row)
                if len(data) >= PcapCapture.BUFFER:
                    __flush()
                    data = []
                    first = False
                i += 1

            except StopIteration:
                break
            if timeout != 0 and (time.time() - time_begin) > timeout: break
            if pkt_num > 0 and i > pkt_num: break

        if len(data) > 0:
            __flush()
            
        tshark_cap.close()

        return df


    def predict(self):
        models = [ Model.load('nosfx'), Model.load('domain') ]
        df = pd.read_csv(self.path_csv)
        for model in models:
            df[model.name], _ = model.predict_u(df['qry_name'])
        df.to_csv(self.path_y_csv)


    def _check_domains(self, df):
        DNs = df['qry_name']
        shape_0 = DNs.shape[0]
        DNs_correct = DNs.apply(Extractor.is_dn)
        df = df[DNs_correct == 1]
        shape_1 = df.shape[0]
        if shape_0 > shape_1:
            print(f'found {shape_0 - shape_1} wrong domains.')
        return df

    
    def _legit_top10m(self, df):
        t1 = time.time()
        df['dsfx'] = df['qry_name'].apply(Extractor.domain_sfx)
        df = df.set_index('dsfx').join(df_10m)
        df['rank'] = df['rank'].fillna(-1).apply(lambda x: 0 if x<0 else 1)
        df = df.rename(columns={'rank': 'dsfx_legit'}).reset_index().drop(columns='dsfx')
        print('top10m', time.time() - t1)
        return df



path = '/tmp/capture/322-1_2018-01-29_win6.pcap'
# path = '/tmp/capture/264-1_2017-06-24_win3.pcap'


print('sync...')
pcap_capture = PcapCapture(path)
timer_sync = time.time()
pcap_capture.generate_csv()
timer_sync = time.time() - timer_sync
print('Time sync:  %2.5f' % timer_sync)
