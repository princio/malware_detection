import pyshark
import os
from enum import Enum
import pprint
import pandas as pd
import time
from Model import Model
from Extractor import Extractor
import pickle

class Pcap2Csv():
    def __init__(self, fast_mode=True, buffer_size=0, legit_dsfx=True, only_dn=False, pkt_num=0, timeout=0):
        self.models = [ Model.load('nosfx'), Model.load('domain') ]
        self.chosen_dns_fields = [ 'count_labels', 'count_queries', 'flags_authenticated', 'flags_response', 'qry_name', 'qry_type', 'qry_class','resp_ttl', 'resp_name', 'response_to', 'time', 'a', 'aaaa' ]
        with open('top_10m.dsfx.pkl', 'rb') as fp:
            self.df_10m_set = pickle.load(fp)
        self.fast_mode   = fast_mode
        self.buffer_size = buffer_size
        self.legit_dsfx  = legit_dsfx
        self.only_dn     = only_dn
        self.pkt_num     = pkt_num
        self.timeout     = timeout

    def convert(self, path, fast_mode=None, buffer_size=None, legit_dsfx=None, only_dn=None, pkt_num=None, timeout=None):
        fast_mode   = fast_mode   if fast_mode   is not None else self.fast_mode
        buffer_size = buffer_size if buffer_size is not None else self.buffer_size
        legit_dsfx  = legit_dsfx  if legit_dsfx  is not None else self.legit_dsfx
        only_dn     = only_dn     if only_dn     is not None else self.only_dn
        pkt_num     = pkt_num     if pkt_num     is not None else self.pkt_num
        timeout     = timeout     if timeout     is not None else self.timeout

        tshark_cap = pyshark.FileCapture(path, only_summaries=fast_mode, keep_packets=False, display_filter="dns && !_ws.malformed && !icmp")
        i = 0
        data = []
        first = True
        ext = '.fast.csv' if fast_mode else '.csv'

        if fast_mode:
            columns = ['frame_number', 'timestamp', 'qry_name', 'response']
        else:
            chosen_dns_fields = (['qry_name'] if only_dn else self.chosen_dns_fields.copy())
            columns = ['frame_number', 'timestamp'] + chosen_dns_fields
        if legit_dsfx:
            columns += ['legit_dsfx']

        def __flush():
            df = pd.DataFrame(data, index=pd.RangeIndex(i+1-len(data), i+1), columns=columns)
            df.to_csv(path + ext, mode='w' if first else 'a', header=first)
            print('saving...', i)

        def __packetsummary2row(packet):
            # 'No.': '17242', 'Time': '43049.733195', 'Source': '192.168.1.116', 'Destination': '8.8.8.8',
            # 'Protocol': 'DNS', 'Length': '76', 'dns.a': '', 'dns.qry.name': 'dns.msftncsi.com', 
            # 'dns.flag.response': 'Message is a query', 'dns.qry.type': 'AAAA (IPv6 Address)', 'ip': '8.8.8.8'}
            if packet._fields['Protocol'] != 'DNS':
                return None
            qry = packet._fields['dns.qry.name']
            if not Extractor.is_dn(qry):
                return None
            row = [packet._fields['No.'], packet._fields['Time'], qry, 1 if 'Message is a response' == packet._fields['dns.flag.response'] else 0]
            if legit_dsfx:
                row += [1 if Extractor.domain_sfx(qry) in self.df_10m_set else 0]
            return row

        def __packet2row(packet):
            if 'DNS' not in packet:
                return None
            qry = packet['DNS'].get_field('qry_name')
            if not Extractor.is_dn(qry):
                return None
            dns_layer = packet['DNS']
            row = [packet.number, packet.sniff_time.timestamp()]
            row += [dns_layer.get_field(cf) for cf in chosen_dns_fields]
            if legit_dsfx:
                row += [1 if Extractor.domain_sfx(qry) in self.df_10m_set else 0]
            return row

        time_parsing = time.time()
        time_parsing_cycle_tot = 0
        time_parsing_cycle = 0
        while True:
            time_parsing_cycle_tot += time_parsing_cycle
            try:
                time_parsing_cycle = time.time()
                packet = tshark_cap.next()
                time_parsing_cycle = time.time() - time_parsing_cycle
                row = __packetsummary2row(packet) if fast_mode else __packet2row(packet)
                if row is None:
                    continue
                data.append(row)
                if buffer_size > 0 and len(data) >= buffer_size:
                    __flush()
                    data = []
                    first = False
                i += 1
            except StopIteration:
                break
            if timeout != 0 and (time.time() - time_parsing) > timeout: break
            if pkt_num > 0 and i > pkt_num: break

        tshark_cap.close()
        if len(data) > 0:
            __flush()
        time_parsing = time.time() - time_parsing
        
        df = pd.read_csv(path + ext, index_col=0)

        if not fast_mode:
            time_reqres = time.time()
            set_responses = set(df[df['response_to'] >= 0]['response_to'].astype(int).values)
            def check_reqres(row):
                if row['flags_response'] == 1: return row
                else:
                    row['response_to'] = row['frame_number'] in set_responses
                return row
            df = df.apply(check_reqres, axis=1)
            time_reqres = time.time() - time_reqres

        time_predict = time.time()
        for model in self.models:
            df[model.name], _ = model.predict_u(df['qry_name'])
        time_predict = time.time() - time_predict


        print('Parsing time:', time_parsing)
        print('Next time:', time_parsing_cycle_tot/i)
        if not fast_mode:
            print('ReqRes time:',  time_reqres)
        print('Predict time:',  time_predict)

        df.to_csv(path + '.final' + ext)

        return df

path = '/tmp/capture/322-1_2018-01-29_win6.pcap'

pcap_2_csv = Pcap2Csv(path)
timer_sync = time.time()
size_fast = pcap_2_csv.convert(path).shape[0]
size_full = pcap_2_csv.convert(path, fast_mode=False).shape[0]
timer_sync = time.time() - timer_sync
print('Size fast:  %d' % size_fast)
print('Size full:  %d' % size_full)
