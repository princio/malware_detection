import numpy as np
import os
import random
import csv
import collections
import math
import pandas as pd
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import precision_score, recall_score, classification_report,accuracy_score, f1_score
import time
from datetime import datetime
from DomainLevel import Transformer
import json

def build_binary_model(vocabulary_lenght, units, layers):
    """Build LSTM model for two-class classification"""
    model = Sequential()
    model.add(Embedding(vocabulary_lenght, layers, input_length=units))
    model.add(LSTM(layers))
    model.add(Dropout(0.5))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))

    model.compile(loss='binary_crossentropy',optimizer='rmsprop')

    return model

def build_multiclass_model(vocabulary_lenght, units):
    """Build multiclass LSTM model for multiclass classification"""
    model = Sequential()
    model.add(Embedding(vocabulary_lenght, 128, input_length=units))
    model.add(LSTM(64))
    model.add(Dropout(0.5))
    model.add(Dense(38))
    model.add(Activation('softmax'))

    model.compile(loss='sparse_categorical_crossentropy',optimizer='rmsprop')

    return model


def create_class_weight(labels_dict, mu):
    """Create weight based on the number of dn name in the dataset"""
    counter = collections.Counter(labels_dict)
    total = sum(counter.values())
    class_weight = {}

    for key in labels_dict:
        class_weight[key] = math.pow(total/float(labels_dict[key]), mu)	

    print('class_weight', class_weight)
    return class_weight

def classifaction_report_csv(report, precision, recall, f1_score, fold):
    """Generate the report to data processing"""
    with open('classification_report_cost.csv', 'a') as f:
        report_data = []
        lines = report.split('\n')
        row = {}
        row['class'] =  "fold %u" % (fold+1)
        report_data.append(row)
        for line in lines[2:44]:
            row = {}
            line = " ".join(line.split())
            row_data = line.split(' ')
            if(len(row_data)>2):
                if(row_data[0]!='avg'):
                    row['class'] = row_data[0]
                    row['precision'] = float(row_data[1])
                    row['recall'] = float(row_data[2])
                    row['f1_score'] = float(row_data[3])
                    row['support'] = row_data[4]
                    report_data.append(row)
                else:
                    row['class'] = row_data[0]+row_data[1]+row_data[2]
                    row['precision'] = float(row_data[3])
                    row['recall'] = float(row_data[4])
                    row['f1_score'] = float(row_data[5])
                    row['support'] = row_data[6]
                    report_data.append(row)
        row = {}
        row['class'] = 'macro'
        row['precision'] = float(precision)
        row['recall'] = float(recall)
        row['f1_score'] = float(f1_score)
        row['support'] = 0
        report_data.append(row)
        dataframe = pd.DataFrame.from_dict(report_data)
        dataframe.to_csv(f, index = False)

def gen(train, test, X, Y):
    train_X, test_X, train_y, test_y = X[train], X[test], Y[train], Y[test]
    trainv = {
        'X': train_X,
        'Y': train_y,
        'idx': train
    }
    testv = {
        'X': test_X,
        'Y': test_y,
        'idx': test
    }
    return trainv, testv

def run():


    vocabulary = ['-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
    integer_encoding = {letter: idx+1 for idx, letter in enumerate(vocabulary)}
    vocabulary_lenght = len(vocabulary) + 1

    dataset_path = 'training/setAtrain_h_t_domins.csv'
    now = datetime.now()
    layers = 128
    epochs = 2
    folds = 3
    batch_size = 128
    units = None

    transformer = Transformer.NOTLD
    translator = transformer.translator()

    with open(dataset_path, "r") as f:
        reader = csv.reader(f)
        indata = [row for row in reader]
    df = pd.read_csv(dataset_path)
    
    if os.environ['DEBUG'] == 'True':
        indata = indata[0:100]
        df = df.iloc[0:100]

    bin_labels = [x[0] for x in indata]
    dga_labels = [x[1] for x in indata]
    dns =    [x[3] for x in indata]
    X = [translator(d) for d in dns]
    X = [[integer_encoding[y] for y in x] for x in X]
    X = sequence.pad_sequences(X, maxlen=units, truncating='post')
    y_binary = np.array([0 if x == 'legit' else 1 for x in bin_labels])

    if units is None:
        units = np.max([len(x) for x in X])


    count = 0
    while os.path.exists(f"training/model_{layers}_{units}_{transformer.name}_{folds}_{epochs}_binary_{count}/"):
        count += 1
        pass
    dir_path = f"training/model_{layers}_{units}_{transformer.name}_{folds}_{epochs}_binary_{count}/"
    os.mkdir(dir_path)

    print('indata len', len(indata))
    print('indata row', indata[1])
    print('bin_label', bin_labels[1])
    print('dga_label', dga_labels[1])
    print('input_raw', X[1])
    print('input_int_encoded', X[1])
    print('bin_encoded', y_binary[1])



    print(f'Starting training with:')
    print(f'epochs={epochs},folds={folds},batch_size={batch_size}')
    print('vocabulary_lenght=%d\nvocabulary=%s' % (vocabulary_lenght, ','.join(vocabulary)))
    print(f'layers={layers},units={units},dn_level={transformer}')

    start = time.time()
    sss = StratifiedShuffleSplit(n_splits=folds, test_size=0.2, random_state=0)
    fold = 0
    accuracies = [[] for f in range(folds)]
    histories = [[] for f in range(folds)]
    class_weights = []
    for train_idxs, test_idxs in sss.split(X, y_binary):
        print("fold %u/%u" % (fold+1, folds))
        ftrain, ftest = gen(train_idxs, test_idxs, X, y_binary)

        model = build_binary_model(vocabulary_lenght, units, layers)
        
        print("Training the model for two-class classification stage...")

        sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)
        for train_idxs, test_idxs in sss1.split(ftrain['X'], ftrain['Y']):
            etrain, etest = gen(train_idxs, test_idxs, ftrain['X'], ftrain['Y'])
        
        class_weight = create_class_weight(collections.Counter(ftrain['Y']), 0.1)
        class_weights.append(class_weight)

        best_auc = 0.0

        history = model.fit(ftrain['X'], ftrain['Y'], validation_split=0.05, batch_size=batch_size, epochs=20, class_weight=class_weight)

        model.ev
        print(history.history)

        # for ep in range(epochs):
        #     print("epoch %d/%d" % (ep, epochs))
        #     history = model.fit(etrain['X'], etrain['Y'], batch_size=batch_size, epochs=1, class_weight=class_weight)
        #     histories[fold].append(history)
        #     t_probs = model.predict(etest['X'])
        #     t_result = [0 if(x<=0.5) else 1 for x in t_probs]
        #     t_acc = accuracy_score(etest['Y'], t_result)
        #     if t_acc > best_auc:
        #         best_ep = ep
        #         best_model = model
        #         best_auc = t_acc
        #         best_idx_train = etrain['idx']
        #         best_idx_test = etest['idx']
        #         best_t_predictions = t_probs
        #     accuracies[fold].append(str(t_acc))
        #     print('accuracy:', t_acc)

        # model_json = best_model.to_json()
        # name_file = f"{dir_path}/model_f{fold}_e{best_ep}"
        # with open(name_file + ".json", "w") as json_file:
        #     json_file.write(model_json)
        #     best_model.save_weights(name_file + ".h5")
        #     df.iloc[best_idx_train].to_csv(name_file + ".train.csv")
        #     df_test = df.iloc[best_idx_test]
        #     df_test['prediction'] = best_t_predictions
        #     df_test.to_csv(name_file + ".test.csv")
        # with open(name_file + ".accuracies.txt", "a+") as f:
        #     f.write(','.join(accuracies[fold]))
        # print("Saved two-class model to disk")
        # #End of two-class classification stage

        # training_time = time.time() - start
        # threshold = 0.5
        # y_pred = best_model.predict(ftest['X'])
        # y_result = [0 if(x <= threshold) else 1 for x in y_pred]

        # #Calculate the final result
        # score = f1_score(ftest['Y'], y_result, average="macro")
        # precision = precision_score(ftest['Y'], y_result, average="macro")
        # recall = recall_score(ftest['Y'], y_result, average="macro")
        # report = classification_report(ftest['Y'], y_result, digits=4)
        # acc = accuracy_score(ftest['Y'], y_result)
        # print('\n classification report:\n', report)
        # print('F1 score:', score)
        # print('Recall:', recall)
        # print('Precision:', precision)
        # print('Acc:', acc)
        # print('Time:', training_time)
        
        # with open(dir_path + 'infos.json', 'w') as fp:
        #     json.dump({
        #         'vocabulary': vocabulary,
        #         'integer_encoding': integer_encoding,
        #         'vocabulary_lenght': vocabulary_lenght,
        #         'dataset_path': dataset_path,
        #         'datetime': now.strftime("%d/%m/%Y %H:%M:%S"),
        #         'layers': layers,
        #         'epochs': epochs,
        #         'folds': folds,
        #         'batch_size': batch_size,
        #         'units': units.item(),
        #         'count': count,
        #         'transformer': transformer.name,
        #         'class_weights': class_weights.__str__(),
        #         'accuracies': accuracies,
        #         'histories': [h.history.__str__() for hs in histories for h in hs],
        #         'report': report,
        #         'score': score,
        #         'recall': recall,
        #         'precision': precision,
        #         'accuracy': acc,
        #         'training_time': training_time
        #     },
        #     fp)
        fold = fold+1
        
if __name__ == "__main__":
    run()
