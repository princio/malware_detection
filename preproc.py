import tensorflow as tf
import pandas as pd
from DomainLevel import Transformer
# if os.environ.get('DEBUG') == 'True':
df = pd.read_csv('datasets/dataset_training_debug.csv')


bin_labels = df['legit']
dga_labels = df['class']
domains    = df['domain']
X = [Transformer.last(x) for x in domains]
Y = [1 if b == 'dga' else 0 for b in bin_labels]

dataset = tf.data.Dataset.from_tensor_slices((X, Y))

dataset.


# import time
# start=time.time()
# X = [[c for c in d] for d in domains]


# # training_dataset = (
# #     tf.data.Dataset.from_tensor_slices(
# #         (
# #             X,
# #             tf.cast(bin_labels, tf.string)
# #         )
# #     )
# # )

# vocabulary = ['-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

# # s = preprocessing.StringLookup(vocabulary=vocabulary, mask_token=None)
# def split_(i):
#     print(i, tf.strings.bytes_split(i))
#     return tf.strings.bytes_split(i)

# # layer1 = preprocessing.TextVectorization(output_mode='int', split=split_, standardize=tf.strings.lower)
# # layer1.adapt(['diocaneeeeeeeeeeeeeeeeeee'], reset_state=True)
# # a = layer1.get_vocabulary()
# # bo = layer1(['ciao'])


# # layer2 = preprocessing.TextVectorization(output_mode='int', split=split_, standardize=tf.strings.lower, output_sequence_length=65)
# # layer2.set_vocabulary(vocabulary)
# # layer2.build(tf.TensorShape([1,1]))
# # a = layer2.get_vocabulary()
# # bo = layer2(['ciao','porco.dio'])
# # print(a, bo)

# layer_pre = preprocessing.TextVectorization(output_mode='int', split=split_, standardize=tf.strings.lower, output_sequence_length=65)
# layer_pre.set_vocabulary(vocabulary)
# layer_pre.build(tf.TensorShape([1,1]))
# inputs = keras.Input(shape=(1,), dtype="string")
# x = layer_pre(inputs)
# x = layers.Embedding(input_dim=len(layer_pre.get_vocabulary()), output_dim=128, mask_zero=True)(x)
# x = layers.LSTM(128)(x)
# x = layers.Dropout(0.5)(x)
# outputs = layers.Dense(1)(x)
# model = keras.Model(inputs, outputs)
# model.compile(loss='binary_crossentropy',optimizer='rmsprop')
# return model

# print(model.summary())

# keras.utils.plot_model(model, "/tmp/my_first_model.png")

# # train_dataset.shuffle(buffer_size=1024).batch(64)

# # tf.data.Dataset.from_tensor_slices
# # print(data.take(0))

# # # Instantiate TextVectorization with "int" output_mode
# # text_vectorizer = preprocessing.TextVectorization(output_mode="int")
# # # Index the vocabulary via `adapt()`
# # text_vectorizer.adapt(data)

# # # You can retrieve the vocabulary we indexed via get_vocabulary()
# # vocab = text_vectorizer.get_vocabulary()
# # print("Vocabulary:", vocab)

# # # Create an Embedding + LSTM model
# # inputs = keras.Input(shape=(1,), dtype="string")
# # x = text_vectorizer(inputs)
# # x = layers.Embedding(input_dim=len(vocab), output_dim=64)(x)
# # outputs = layers.LSTM(1)(x)
# # model = keras.Model(inputs, outputs)

# # # Call the model on test data (which includes unknown tokens)
# # test_data = tf.constant(["The Brain is deeper than the sea"])
# # test_output = model(test_data)