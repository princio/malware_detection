import os, json
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
from datetime import datetime
from PyInquirer import prompt
import csv
import hashlib
import re
import psycopg2
import numpy as np
from Model import Model
from Report import Report
from Dataset import Dataset
from Repository import DatasetRepository, ModelRepository, MetricsRepository
from Metrics import Metrics

connection = psycopg2.connect(user = "postgres",
                                  password = "porcodio",
                                  host = "127.0.0.1",
                                  port = "5432",
                                  database = "malware")


if __name__ == "__main__":
    cursor = connection.cursor()
    models = ModelRepository.fetchall(connection)
    dtype_ = [("name", 'U')]
    for i in range(17):
        dtype_.append((f"f{i}", np.float))
    dtype_ = np.dtype(dtype_)
    np_average = np.empty((0,17))
    np_trainings = np.empty((0,17))
    diffs = []
    names = []
    for model in models.values():
        names.append(model.name())
        if model.training_confusion_matrix is not None:
            training_metrics = Metrics.create(model.classes, model.training_confusion_matrix)
            # np_model_average = np.insert(np.asfarray(model_average.tolist()), 0, model.name())
            np_model_average = np.asarray(model.average)
            np_training_metrics = np.asarray(training_metrics.tolist())
            # diffs.append([ f"{model.fold}_{model.epoch}", np_training_metrics - np_model_average])

            print(np_model_average)
            np_average = np.vstack((np_average, np_model_average))
            np_trainings = np.vstack((np_trainings, np_training_metrics))
            # print(model.name() + "," + ",".join([str(f) for f in model_average.tolist()]))
            # print(model.name() + "," + ",".join([str(f) for f in training_metrics.tolist()]))
            # print(model.name() + "," + ",".join([str(f) for f in (np_training_metrics - np_model_average).tolist()]))
            # print("\n")

    sorting_by_dataset_metrics_by_legit_precision = np_average[:,0].argsort()
    sorting_by_training_metrics_by_legit_precision = np_trainings[:,0].argsort()

    sf = "%f " * 17
    sf = "%5s | " + sf + "\n      | " + sf
    for idx in sorting_by_dataset_metrics_by_legit_precision:
        print(sf % tuple([names[idx]] + np_average[idx].tolist() + np_trainings[idx].tolist()))

    r1, r2 = '', ''
    for i in range(len(sorting_by_dataset_metrics_by_legit_precision)):
        r1 += "%2d " % (sorting_by_dataset_metrics_by_legit_precision[i])
        r2 += "%2d " % (sorting_by_training_metrics_by_legit_precision[i])
    print(r1)
    print(r2)


    sorting_by_dataset_metrics_by_legit_accuracy = np_average[:,16].argsort()
    sorting_by_training_metrics_by_legit_accuracy = np_trainings[:,16].argsort()
    r1, r2 = '', ''
    for i in range(len(sorting_by_dataset_metrics_by_legit_accuracy)):
        r1 += "%2d " % (sorting_by_dataset_metrics_by_legit_accuracy[i])
        r2 += "%2d " % (sorting_by_training_metrics_by_legit_accuracy[i])
    print(r1)
    print(r2)


    connection.close()
