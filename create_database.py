import sqlite3
import json
import time
from datetime import datetime
from PyInquirer import prompt, Separator
import os, fnmatch
import itertools
from collections import OrderedDict
import csv
from pymongo import MongoClient
import hashlib
import re
import pprint
import uuid

class JSONEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, datetime):
            return int(obj.timestamp())
        if isinstance(obj, uuid.UUID):
            return str(obj)
        else:
            return super(JSONEncoder, self).default(obj)

def pretty(d, indent=0):
   for key, value in d.items():
      print('\t' * indent + str(key))
      if isinstance(value, dict):
         pretty(value, indent+1)
      else:
         print('\t' * (indent+1) + str(value))

def inquire():
    files = { 'datasets': { 'benign': [], 'malware': [] }, 'models': []}

    for type_ in ['benign', 'malware']:
        type_files = os.listdir(f"datasets/{type_}_h_t")
        for dataset in type_files:
            with open(f"datasets/{type_}_h_t/{dataset}", "r") as fp:
                reader = csv.reader(fp)
                row_counter = 0
                for row in reader:
                    row_counter = row_counter + 1

            m = re.search(r'(\d+)_(\d{4}-\d{2}-\d{2})_([\w\-]+)\.([\w\-]+).csv__h_t.csv', dataset)
            if m is None:
                print(f'Strange dir: {dataset}')
            else:
                hash_ = file_hash(f"datasets/{type_}_h_t/{dataset}")

                files['datasets'][type_].append({
                    "_id": uuid.UUID(hex=hash_[0:32]),
                    "hash": hash_,
                    'name': dataset,
                    'type': type_,
                    'inputs_number': row_counter,
                    'index': m.group(1),
                    'date': datetime.strptime(m.group(2), "%Y-%m-%d"),
                    'from': m.group(3),
                    'table': m.group(4)
                })

    dirs = os.listdir("models/")

    for dir_ in dirs:
        m = re.search(r'(\d{8}_\d{6})__epochs_(\d+)_folds_(\d+)', dir_)
        if m is None:
            print(f'Strange dir: {dir_}')
        else:
            models = {
                '_id': uuid.uuid4(),
                'date': datetime.strptime(m.group(1), "%Y%m%d_%H%M%S"),
                'epochs': m.group(2),
                'folds': m.group(3),
                'models': []
            }
            for model_fold_epoch in os.listdir(f"models/{dir_}"):
                m = re.search(r'(\w+)_model_fold_(\d+)_epoch_(\d+)\.json', model_fold_epoch)
                if m is None:
                    print(f'Strange model: {model_fold_epoch}')
                else:
                    hash_ = model_hash(f"models/{dir_}/{model_fold_epoch}"[:-5])
                    
                    models['models'].append({
                        "_id": uuid.UUID(hash_[0:32]),
                        'hash': hash_,
                        'path': f"models/{dir_}/{model_fold_epoch}",
                        'type': m.group(1),
                        'fold': m.group(2),
                        'epoch': m.group(3)
                    })

                    print(uuid.UUID(hash_[0:32]).time)

    files['models'] = models

    date = datetime.now().strftime("%Y%m%d%H%M%S")
    __dir = f"database/{date}"

    os.makedirs(__dir)

    with open(f"{__dir}/datasets.json", "w") as fp_models:
        json.dump(files['datasets'], fp_models, cls=JSONEncoder)

    with open(f"{__dir}/models.json", "w") as fp_datasets:
        json.dump(files['models'], fp_datasets, cls=JSONEncoder)

    pp = pprint.PrettyPrinter(indent=2)
    pp.pprint(files)

    return files


def file_hash(path):
    sha256_hash = hashlib.sha256()
    with open(path, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

def model_hash(path):
    sha256_hash = hashlib.sha256()
    with open(f"{path}.json", "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    with open(f"{path}.h5", "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

if __name__ == "__main__":
    inquire()