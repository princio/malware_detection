{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import Module\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init\n",
    "from utils.topkids_utils import *\n",
    "from utils.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_workers = 0 # number of subprocesses to use for data loading\n",
    "batch_size = 35 # 35\n",
    "valid_size = 0.2\n",
    "n_epochs = 80\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "classes = list(range(0,n_classes))\n",
    "\n",
    "\n",
    "# Dataset object\n",
    "transform = transforms.ToTensor()\n",
    "train_data = Pacman(root=locals_dir, set='train', transform=transform, download=False)\n",
    "\n",
    "\n",
    "# Split training and validation indices\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # 3\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 3\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 6*6 from image dimension # 6\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda() # Model to GPU\n",
    "\n",
    "\n",
    "# Loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 1, 32, 32]) torch.Size([35])\n"
     ]
    }
   ],
   "source": [
    "# NETWORK TEST\n",
    "\n",
    "\n",
    "\n",
    "# keep track of training and validation loss\n",
    "train_loss = 0.0\n",
    "valid_loss = 0.0\n",
    "\n",
    "model.train()\n",
    "\n",
    "for data, target in train_loader:\n",
    "    print(data.shape, target.shape)\n",
    "    \n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    \n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # update training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING\n",
      "Epoch: 1 \tTraining Loss: 1.841031 \tValidation Loss: 0.460084\n",
      "Validation loss decreased (inf --> 0.460084).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.838906 \tValidation Loss: 0.459700\n",
      "Validation loss decreased (0.460084 --> 0.459700).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.837503 \tValidation Loss: 0.459338\n",
      "Validation loss decreased (0.459700 --> 0.459338).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.835671 \tValidation Loss: 0.458738\n",
      "Validation loss decreased (0.459338 --> 0.458738).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.832061 \tValidation Loss: 0.457255\n",
      "Validation loss decreased (0.458738 --> 0.457255).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.821050 \tValidation Loss: 0.451721\n",
      "Validation loss decreased (0.457255 --> 0.451721).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.740224 \tValidation Loss: 0.399374\n",
      "Validation loss decreased (0.451721 --> 0.399374).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.473018 \tValidation Loss: 0.353185\n",
      "Validation loss decreased (0.399374 --> 0.353185).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.404201 \tValidation Loss: 0.351367\n",
      "Validation loss decreased (0.353185 --> 0.351367).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.364783 \tValidation Loss: 0.326593\n",
      "Validation loss decreased (0.351367 --> 0.326593).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.318872 \tValidation Loss: 0.314821\n",
      "Validation loss decreased (0.326593 --> 0.314821).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.246216 \tValidation Loss: 0.304105\n",
      "Validation loss decreased (0.314821 --> 0.304105).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.194737 \tValidation Loss: 0.330829\n",
      "Epoch: 14 \tTraining Loss: 1.114416 \tValidation Loss: 0.292493\n",
      "Validation loss decreased (0.304105 --> 0.292493).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.058380 \tValidation Loss: 0.263634\n",
      "Validation loss decreased (0.292493 --> 0.263634).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.993185 \tValidation Loss: 0.242402\n",
      "Validation loss decreased (0.263634 --> 0.242402).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.940524 \tValidation Loss: 0.232415\n",
      "Validation loss decreased (0.242402 --> 0.232415).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.894510 \tValidation Loss: 0.210348\n",
      "Validation loss decreased (0.232415 --> 0.210348).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.839549 \tValidation Loss: 0.201710\n",
      "Validation loss decreased (0.210348 --> 0.201710).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.794653 \tValidation Loss: 0.183150\n",
      "Validation loss decreased (0.201710 --> 0.183150).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.741824 \tValidation Loss: 0.180925\n",
      "Validation loss decreased (0.183150 --> 0.180925).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.684813 \tValidation Loss: 0.157170\n",
      "Validation loss decreased (0.180925 --> 0.157170).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.605533 \tValidation Loss: 0.145486\n",
      "Validation loss decreased (0.157170 --> 0.145486).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.549013 \tValidation Loss: 0.131484\n",
      "Validation loss decreased (0.145486 --> 0.131484).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.491605 \tValidation Loss: 0.124278\n",
      "Validation loss decreased (0.131484 --> 0.124278).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.435433 \tValidation Loss: 0.103729\n",
      "Validation loss decreased (0.124278 --> 0.103729).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.386308 \tValidation Loss: 0.107518\n",
      "Epoch: 28 \tTraining Loss: 0.334457 \tValidation Loss: 0.093194\n",
      "Validation loss decreased (0.103729 --> 0.093194).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.294243 \tValidation Loss: 0.073306\n",
      "Validation loss decreased (0.093194 --> 0.073306).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.262000 \tValidation Loss: 0.065428\n",
      "Validation loss decreased (0.073306 --> 0.065428).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.226737 \tValidation Loss: 0.059627\n",
      "Validation loss decreased (0.065428 --> 0.059627).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.197528 \tValidation Loss: 0.056988\n",
      "Validation loss decreased (0.059627 --> 0.056988).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.177583 \tValidation Loss: 0.047035\n",
      "Validation loss decreased (0.056988 --> 0.047035).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.155154 \tValidation Loss: 0.066148\n",
      "Epoch: 35 \tTraining Loss: 0.142764 \tValidation Loss: 0.042580\n",
      "Validation loss decreased (0.047035 --> 0.042580).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.125837 \tValidation Loss: 0.036526\n",
      "Validation loss decreased (0.042580 --> 0.036526).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.113579 \tValidation Loss: 0.036693\n",
      "Epoch: 38 \tTraining Loss: 0.104743 \tValidation Loss: 0.033428\n",
      "Validation loss decreased (0.036526 --> 0.033428).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.088586 \tValidation Loss: 0.034888\n",
      "Epoch: 40 \tTraining Loss: 0.078909 \tValidation Loss: 0.029378\n",
      "Validation loss decreased (0.033428 --> 0.029378).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.070138 \tValidation Loss: 0.025609\n",
      "Validation loss decreased (0.029378 --> 0.025609).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.066416 \tValidation Loss: 0.025701\n",
      "Epoch: 43 \tTraining Loss: 0.058286 \tValidation Loss: 0.025707\n",
      "Epoch: 44 \tTraining Loss: 0.053090 \tValidation Loss: 0.030717\n",
      "Epoch: 45 \tTraining Loss: 0.047063 \tValidation Loss: 0.024752\n",
      "Validation loss decreased (0.025609 --> 0.024752).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.044521 \tValidation Loss: 0.019511\n",
      "Validation loss decreased (0.024752 --> 0.019511).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.038355 \tValidation Loss: 0.018235\n",
      "Validation loss decreased (0.019511 --> 0.018235).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.033687 \tValidation Loss: 0.020538\n",
      "Epoch: 49 \tTraining Loss: 0.029577 \tValidation Loss: 0.017076\n",
      "Validation loss decreased (0.018235 --> 0.017076).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.028722 \tValidation Loss: 0.017631\n",
      "Epoch: 51 \tTraining Loss: 0.023139 \tValidation Loss: 0.016094\n",
      "Validation loss decreased (0.017076 --> 0.016094).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.023695 \tValidation Loss: 0.018004\n",
      "Epoch: 53 \tTraining Loss: 0.023240 \tValidation Loss: 0.020856\n",
      "Epoch: 54 \tTraining Loss: 0.018083 \tValidation Loss: 0.012999\n",
      "Validation loss decreased (0.016094 --> 0.012999).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.017725 \tValidation Loss: 0.014475\n",
      "Epoch: 56 \tTraining Loss: 0.014681 \tValidation Loss: 0.013097\n",
      "Epoch: 57 \tTraining Loss: 0.013083 \tValidation Loss: 0.013929\n",
      "Epoch: 58 \tTraining Loss: 0.011529 \tValidation Loss: 0.013646\n",
      "Epoch: 59 \tTraining Loss: 0.010466 \tValidation Loss: 0.013231\n",
      "Epoch: 60 \tTraining Loss: 0.009500 \tValidation Loss: 0.014487\n",
      "Epoch: 61 \tTraining Loss: 0.008943 \tValidation Loss: 0.012570\n",
      "Validation loss decreased (0.012999 --> 0.012570).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 0.008199 \tValidation Loss: 0.012658\n",
      "Epoch: 63 \tTraining Loss: 0.006942 \tValidation Loss: 0.011971\n",
      "Validation loss decreased (0.012570 --> 0.011971).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.007700 \tValidation Loss: 0.012313\n",
      "Epoch: 65 \tTraining Loss: 0.006363 \tValidation Loss: 0.014863\n",
      "Epoch: 66 \tTraining Loss: 0.006139 \tValidation Loss: 0.012032\n",
      "Epoch: 67 \tTraining Loss: 0.005095 \tValidation Loss: 0.012265\n",
      "Epoch: 68 \tTraining Loss: 0.004820 \tValidation Loss: 0.011843\n",
      "Validation loss decreased (0.011971 --> 0.011843).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.004452 \tValidation Loss: 0.011496\n",
      "Validation loss decreased (0.011843 --> 0.011496).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.004710 \tValidation Loss: 0.011378\n",
      "Validation loss decreased (0.011496 --> 0.011378).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 0.003785 \tValidation Loss: 0.011631\n",
      "Epoch: 72 \tTraining Loss: 0.003570 \tValidation Loss: 0.011826\n",
      "Epoch: 73 \tTraining Loss: 0.003307 \tValidation Loss: 0.011976\n",
      "Epoch: 74 \tTraining Loss: 0.003219 \tValidation Loss: 0.012722\n",
      "Epoch: 75 \tTraining Loss: 0.003094 \tValidation Loss: 0.011560\n",
      "Epoch: 76 \tTraining Loss: 0.002937 \tValidation Loss: 0.011607\n",
      "Epoch: 77 \tTraining Loss: 0.002713 \tValidation Loss: 0.011675\n",
      "Epoch: 78 \tTraining Loss: 0.002520 \tValidation Loss: 0.011987\n",
      "Epoch: 79 \tTraining Loss: 0.002397 \tValidation Loss: 0.011658\n",
      "Epoch: 80 \tTraining Loss: 0.002360 \tValidation Loss: 0.011755\n"
     ]
    }
   ],
   "source": [
    "print(\"START TRAINING\")\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), './model/model.pt')\n",
    "        valid_loss_min = valid_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "Test Loss: 2.136558\n",
      "\n",
      "Test Accuracy of     0: 80% (805/1000)\n",
      "Test Accuracy of     1: 91% (638/700)\n",
      "Test Accuracy of     2: 99% (996/1000)\n",
      "Test Accuracy of     3: 62% (503/800)\n",
      "Test Accuracy of     4: 76% (760/1000)\n",
      "Test Accuracy of     5: 74% (740/1000)\n",
      "Test Accuracy of     6: 74% (748/1000)\n",
      "Test Accuracy of     7: 65% (659/1000)\n",
      "Test Accuracy of     8: 77% (618/800)\n",
      "Test Accuracy of     9: 80% (806/1000)\n",
      "\n",
      "Test Accuracy (Overall): 78% (7273/9300)\n"
     ]
    }
   ],
   "source": [
    "test_data = Pacman(root=locals_dir, set='test', transform=transform, download=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('./model/model_5attempt_78.pt'))\n",
    "\n",
    "\n",
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(n_classes))\n",
    "class_total = list(0. for i in range(n_classes))\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    \n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # update test loss\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    \n",
    "    batch_size = target.shape[0]\n",
    "    \n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(target.shape[0]): # I am using target.shape[0] instead of batch_size\n",
    "        #print(target.shape)\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
