import re
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
import requests
import pandas as pd
import os, sys
import subprocess
import re
import json

PCAP_ROOT_DIR = f'/media/princio/ssd512/pcap/'
DNS_ROOT_DIR = f'/media/princio/ssd512/dns/'
CSV_ROOT_DIR = f'/media/princio/ssd512/csv/'
LIST_ROOT_DIR = f'/media/princio/ssd512/list/'

def get_file_to_save_name(name, ext):
    s_num, num = '', 0
    while os.path.exists('%s%s.%s' % (name, s_num, ext)):
        num += 1
        s_num = '_%d' % num
    
    file_name = '%s%s.%s' % (name, s_num, ext)
    print('Saving in <%s>.' % file_name)
    return file_name



def get_last_saved_file(folder_path, name, ext):
    not_found = True
    num = 0
    for walker in os.walk(folder_path):
        for file_name in walker[2]:
            if file_name.find(name) == 0:
                not_found = False
                if file_name.rfind('_') == -1:
                    num = 0
                    s_num = ''
                else:
                    r_idx = 1 + file_name.rfind('_')
                    l_idx = file_name.rfind('.')
                    tmp = int(file_name[r_idx:l_idx])
                    num = num if num > tmp else tmp
                    s_num = '_%d' % num
    if not_found:
        return None
    file_name = '%s%s.%s' % (name, s_num, ext)
    print('Found <%s>.' % file_name)
    return os.path.join(folder_path, file_name)



def retrieve_real_size(file_url):
    out = subprocess.run(['wget', '--spider', file_url], text=True, capture_output=True)
    r_id = out.stderr.find('Length: ') + len('Length: ')
    l_id = out.stderr[r_id:].find(' ')
    if r_id == -1:
        return None
    try:
        size = int(out.stderr[r_id:r_id + l_id])
        print('Retrieved size:', size)
    except:
        print('Broken link for: <%s>' % (file_url))
        return None
    return size



def retrieve_captures_infos(redo = False):
    if not redo:
        file_path = get_last_saved_file('./stratosphere/', 'captures', 'json')
        if file_path:
            with open(file_path, 'r') as fp:
                return json.load(fp)
        else:
            print('Json captures file not exists.')

    print('Fetching from web...')
    
    r = requests.get('https://mcfp.felk.cvut.cz/publicDatasets/', verify=False)

    regex = r">(CTU-Malware-Capture-Botnet-\d+(?:-\d)?)\/<"

    matches = re.finditer(regex, r.text, re.MULTILINE)

    mw_captures = []
    for match in matches:
        for group in match.groups():
            mw_captures.append(group)

    print('Found %d malware captures.' % len(mw_captures))

    captures = {}
    for mw in mw_captures:
        print(f'https://mcfp.felk.cvut.cz/publicDatasets/{mw}/')
        r = requests.get(f'https://mcfp.felk.cvut.cz/publicDatasets/{mw}/', verify=False)
        regex = r"<td.*?><a.*?>\s*([\w\d\-\._]+)\s*<\/a><\/td><td.*?>\s*([\s\d\-\:]+)\s*<\/td><td.*?>\s*([\d\.]+[KMG]?)\s*<\/td>"
        matches = re.finditer(regex, r.text, re.MULTILINE)
        captures[mw] = []
        for match in matches:
            groups = list(match.groups())
            ext = groups[0][1 + groups[0].rfind('.'):]
            size = None
            if ext == 'pcap':
                size = retrieve_real_size(f'https://mcfp.felk.cvut.cz/publicDatasets/{mw}/{groups[0]}')
            captures[mw].append({
                'capture_id': mw[27:],
                'filename': groups[0],
                'name': groups[0][:groups[0].rfind('.')],
                'websize': groups[2],
                'size': size,
                'ext': ext
            })

    captures_filename = get_file_to_save_name('./stratosphere/captures', 'json')

    with open(captures_filename, 'w') as fp:
        json.dump(captures, fp, indent='\t')
    
    return captures


def is_file_downloaded(pcaps_downloaded, pcap):
    downloaded = False
    filename_right = '%s_%s.pcap' % (pcap['capture_id'], pcap['name'])
    filename = pcap['filename']

    if filename_right in pcaps_downloaded:
        if pcap['size'] != pcaps_downloaded[filename_right].st_size:
            print('WARNING: for <%s> size not match:\texpected %d, got %d' % (pcap['id'], pcap['size'], pcaps_downloaded[filename_right].st_size))
            downloaded = False
        else:
            downloaded = True

    if filename in pcaps_downloaded:
        print('For <%s> exists <%s>' % (pcap['id'], filename))
        if pcap['size'] != pcaps_downloaded[filename].st_size:
            print('WARNING: size not match:\texpected %d, got %d' % (pcap['size'], pcaps_downloaded[filename].st_size))
            downloaded = False
        else:
            print('INFO: renamed %s to %s' % (filename, filename_right))
            os.rename(os.path.join(PCAP_ROOT_DIR, filename), os.path.join(PCAP_ROOT_DIR, filename_right))
            downloaded = True

    return downloaded

def cmd_download(pcap, pcap_id):
    return [
        'wget', '--limit-rate', '5m', '%s%s/%s' % (url, pcap['capture_id'], pcap['filename']),
        '-O', os.path.join(PCAP_ROOT_DIR, f'{pcap_id}.pcap')
    ]

def cmd_dns(pcap_id):
    return [
        'tshark', '-r', os.path.join(PCAP_ROOT_DIR, f'{pcap_id}.pcap'),
        '-Y', 'dns && !_ws.malformed && !icmp',
        '-w', os.path.join(DNS_ROOT_DIR, f'{pcap_id}.dns.pcap.progress')
    ]

def cmd_csv(pcap_id):
    return [
        'tshark', '-r', os.path.join(DNS_ROOT_DIR, f'{pcap_id}.dns.pcap'),
        '-T', 'fields', '-e', 'frame.number', '-e', 'frame.time_epoch',
        '-e', 'ip.src', '-e', 'ip.dst', '-e', 'dns.qry.name',
        '-e', 'dns.flags.response', '-e', 'dns.qry.type', '-e', 'dns.a',
        '-E', 'separator=,']
    #     ,
    #     '>', os.path.join(CSV_ROOT_DIR, f'{pcap_id}.dns.csv.progress'),
    # ]

def cmd_list(pcap_id):
    return [
        'awk', '-F,', '{print $5}', os.path.join(CSV_ROOT_DIR, f'{pcap_id}.dns.csv')
    ]


def get_pcaps_infos(dict_captures, redo = False):
    if not redo:
        file_path = get_last_saved_file('./stratosphere/', 'pcaps', 'csv')
        if file_path:
            with open(file_path, 'r') as fp:
                return json.load(fp)
        else:
            print('CSV pcaps file not exists.')

    pcaps = []
    for capture in dict_captures:
        exts = ''
        passivedns = False
        labeled = False
        for dict_file in dict_captures[capture]:
            if dict_file['ext'] == 'pcap':
                pcap = dict_file
            else:
                exts += dict_file['ext'] + ','
                passivedns |= dict_file['filename'].find('passivedns') > 0
                labeled |= dict_file['filename'].find('labeled') >= 0
        pcap['id'] = '%s_%s' % (pcap['capture_id'], pcap['name'])
        pcap['size_mb'] = size2text(pcap['size'])
        pcap['labeled'] = labeled
        pcap['passivedns'] = passivedns
        pcaps.append(pcap)

    df = pd.DataFrame(pcaps).sort_values(by=['size'], ascending=False)
    df.to_csv(get_file_to_save_name('./stratosphere/pcaps', 'csv'))

    dict_pcaps = {
        'by_name': {},
        'by_id': {},
        'by_size': {}
    }
    for pcap in pcaps:
        if pcap['name'] not in dict_pcaps['by_name']:
            dict_pcaps['by_name'][pcap['name']] = []
        dict_pcaps['by_name'][pcap['name']].append(pcap)

        dict_pcaps['by_id']['%s_%s' % (pcap['capture_id'], pcap['name'])] = pcap

        if pcap['size'] not in dict_pcaps['by_size']:
             dict_pcaps['by_size'][pcap['size']] = []
        dict_pcaps['by_size'][pcap['size']].append(pcap)


    pcaps_downloaded = {}
    for walker in os.walk(PCAP_ROOT_DIR):
        for filename in walker[2]:
            if filename[-4:] == 'pcap':
                pcaps_downloaded[filename] = os.stat(os.path.join(PCAP_ROOT_DIR, filename))

    for id_ in dict_pcaps['by_id']:
        dict_pcaps['by_id'][id_]['downloaded'] = is_file_downloaded(pcaps_downloaded, dict_pcaps['by_id'][id_])
        if dict_pcaps['by_id'][id_]['downloaded']:
            dict_pcaps['by_id'][id_]['has_dns'] = os.path.exists(os.path.join(DNS_ROOT_DIR, f'{id_}.dns.pcap'))
            dict_pcaps['by_id'][id_]['has_csv'] = os.path.exists(os.path.join(CSV_ROOT_DIR, f'{id_}.dns.csv'))
            dict_pcaps['by_id'][id_]['has_list'] = os.path.exists(os.path.join(LIST_ROOT_DIR, f'{id_}.dns.list'))
        else:
            dict_pcaps['by_id'][id_]['has_dns'] = False
            dict_pcaps['by_id'][id_]['has_csv'] = False
            dict_pcaps['by_id'][id_]['has_list'] = False

    return dict_pcaps['by_id'], pcaps_downloaded


def text2size(size):
    last_char = size[-1]
    exp = 1 + 'KMG'.find(last_char)
    if exp > 0:
        return int(float(size[:-1]) * (1000 ** exp if exp > -1 else 0))

def size2text(size):
    from math import log10, floor
    n_zeros = floor(log10(size))
    unit = ''
    n_zeros -= 6
    size = size / (10 ** 6)
    unit = 'M'
    return '%d%s' % (size, unit)

def print_pcap_infos(df):

    sizes = [5_000_000_000, 1_000_000_000, 100_000_000, 1_000_000]
    dfs = []
    for i, size in enumerate(sizes):
        if i == 0:
            dfs.append(df[df['size'] > size].copy())
        elif i != len(sizes)-1:
            dfs.append(df[df['size'] < size].copy())
        else:
            dfs.append(df[(df['size'] > sizes[i]) & (df['size'] < sizes[i-1])].copy())

    dict_f = {}
    for i, _ in enumerate(dfs):
        if i == 0:
            k = '[%s,inf]' % size2text(sizes[i])
        elif i == len(sizes)-1:
            k = '[0,%s]' % size2text(sizes[i])
        else:
            k = '[%s,%s]' % (size2text(sizes[i]), size2text(sizes[i-1]))
        pdns = dfs[i]['passivedns'].value_counts()
        lb = dfs[i]['labeled'].value_counts()
        dict_f[k] = {'passivedns': pdns.loc[True], 'labeled': lb.loc[True], 'tot': pdns.sum()}
    print(pd.DataFrame(dict_f).to_string())

    pass

def print_pcap_lt_1gb(df_captures):
    df = df_captures.copy()
    df = df[(df['size'] < 1_000_000_000) & (df['passivedns'] == True)]
    print(df.sort_values(by='size', ascending=False).head())
    print(df.dtypes)
    pass


def run_cmd(ext, root_dir, cmd):
    print('> %s %s' % (ext.upper(), pcap_id))
    path = os.path.join(root_dir, f'{pcap_id}.{ext}')
    process = subprocess.run(cmd, capture_output=True, text=True)
    if process.returncode != 0:
        print('Error: ', process.stderr)
        if os.path.exists(f'{path}.progress'):
            os.remove(f'{path}.progress')
        return False
    if ext in ['dns.csv', 'dns.list']:
        with open(path, 'w') as fp:
            fp.write(process.stdout)
    elif ext != 'pcap':
        print(process.stdout)
        os.rename(f'{path}.progress', path)
    return True

if __name__ == "__main__":
    redo1 = False if len(sys.argv) < 2 else sys.argv[1].lower() == 'true'
    redo2 = False if len(sys.argv) < 3 else sys.argv[2].lower() == 'true'
    dict_captures = retrieve_captures_infos(redo1)
    dict_pcaps, pcaps_downloaded = get_pcaps_infos(dict_captures, redo2)

    url = 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-'

    df = pd.DataFrame([dict_pcaps[pcap_id] for pcap_id in dict_pcaps], index=dict_pcaps.keys()).sort_values(by=['size'], ascending=True)
    print(df.to_markdown())

    dict_pcaps_sorted = df.to_dict(orient='index')
    for pcap_id in dict_pcaps_sorted:
        pcap = dict_pcaps_sorted[pcap_id]
        if not pcap['downloaded']:
            if not run_cmd('pcap', PCAP_ROOT_DIR, cmd_download(pcap, pcap_id)):
                continue
        if not pcap['has_dns']:
            if not run_cmd('dns.pcap', DNS_ROOT_DIR, cmd_dns(pcap_id)):
                continue
        if not pcap['has_csv']:
            if not run_cmd('dns.csv', CSV_ROOT_DIR, cmd_csv(pcap_id)):
                continue
        if not pcap['has_list']:
            if not run_cmd('dns.list', LIST_ROOT_DIR, cmd_list(pcap_id)):
                continue

    pass