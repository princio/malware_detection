import re
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
import requests
import pandas as pd
import os
import subprocess
import re


def retrieve_captures_infos():
    import json

    try:
        with open('./tmp/stratosphere_captures.json', 'r') as fp:
            return json.load(fp)
    except:
        print('No file, fetching from web...')
    
    r = requests.get('https://mcfp.felk.cvut.cz/publicDatasets/', verify=False)

    regex = r">(CTU-Malware-Capture-Botnet-\d+(?:-\d)?)\/<"

    matches = re.finditer(regex, r.text, re.MULTILINE)

    mw_captures = []
    for match in matches:
        for group in match.groups():
            mw_captures.append(group)

    print(mw_captures[0:20])

    captures = {}

    for mw in mw_captures:
        print(f'https://mcfp.felk.cvut.cz/publicDatasets/{mw}/')
        r = requests.get(f'https://mcfp.felk.cvut.cz/publicDatasets/{mw}/', verify=False)
        regex = r"<td.*?><a.*?>\s*([\w\d\-\._]+)\s*<\/a><\/td><td.*?>\s*([\s\d\-\:]+)\s*<\/td><td.*?>\s*([\d\.]+[KMG]?)\s*<\/td>"
        matches = re.finditer(regex, r.text, re.MULTILINE)
        captures[mw] = []
        for match in matches:
            groups = list(match.groups())
            extensions = groups[0][groups[0].rfind('.'):]
            captures[mw].append([extensions] + groups)

    with open('./tmp/captures.json', 'w') as fp:
        json.dump(captures, fp, indent='\t')

    pcaps = []
    for capture in captures:
        exts = ''
        passivedns = False
        labeled = False
        for file in captures[capture]:
            if file[0] == '.pcap':
                pcap = file
            else:
                exts += file[0].replace('.', '') + ','
                passivedns |= file[0].find('passivedns') > 0
                labeled |= file[0].find('labeled') >= 0
        size = text2size(pcap[3])
        # print(f'{capture[27:]:10}{pcap[1]:40}{pcap[2]:20}{size:30}')
        pcaps.append({
            'capture': capture[27:],
            'name': pcap[1],
            'date': pcap[2],
            'size': size,
            'labeled': labeled,
            'passivedns': passivedns
        })
    df = pd.DataFrame(pcaps).sort_values(by=['size'], ascending=False)
    df.to_csv('./tmp/file.csv')

    return df

def text2size(size):
    last_char = size[-1]
    exp = 1 + 'KMG'.find(last_char)
    if exp > 0:
        return int(float(size[:-1]) * (1000 ** exp if exp > -1 else 0))

def size2text(size):
    from math import log10, floor
    n_zeros = floor(log10(size))
    unit = ''
    n_zeros -= 6
    size = size / (10 ** 6)
    unit = 'M'
    return '%d%s' % (size, unit)

def print_pcap_infos(df):

    sizes = [5_000_000_000, 1_000_000_000, 100_000_000, 1_000_000]
    dfs = []
    for i, size in enumerate(sizes):
        if i == 0:
            dfs.append(df[df['size'] > size].copy())
        elif i != len(sizes)-1:
            dfs.append(df[df['size'] < size].copy())
        else:
            dfs.append(df[(df['size'] > sizes[i]) & (df['size'] < sizes[i-1])].copy())

    dict_f = {}
    for i, _ in enumerate(dfs):
        if i == 0:
            k = '[%s,inf]' % size2text(sizes[i])
        elif i == len(sizes)-1:
            k = '[0,%s]' % size2text(sizes[i])
        else:
            k = '[%s,%s]' % (size2text(sizes[i]), size2text(sizes[i-1]))
        pdns = dfs[i]['passivedns'].value_counts()
        lb = dfs[i]['labeled'].value_counts()
        dict_f[k] = {'passivedns': pdns.loc[True], 'labeled': lb.loc[True], 'tot': pdns.sum()}
    print(pd.DataFrame(dict_f).to_string())

    pass

def print_pcap_lt_1gb(df_captures):
    df = df_captures.copy()
    df = df[(df['size'] < 1_000_000_000) & (df['passivedns'] == True)]
    print(df.sort_values(by='size', ascending=False).head())
    print(df.dtypes)
    pass

if __name__ == "__main__":
    try:
        df_captures = pd.read_csv('./tmp/stratosphere_captures.csv')
    except:
        print('No file, fetching from web...')
        df_captures = retrieve_captures_infos()

    dict_captures_by_name = {}
    dict_captures_by_id = {}
    for capture in df_captures.to_dict(orient='records'):
        dict_captures_by_name[capture['name'][:-5]] = capture
        dict_captures_by_id['%s' % (capture['capture'])] = capture

    url = 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-'

    pcap_files = {}
    folder_path = '/media/princio/ssd512/pcap/'
    for walker in os.walk(folder_path):
        for file_dn in walker[2]:
            print('processing <%s>' % file_dn)
            path = os.path.join(folder_path, file_dn)
            if file_dn[file_dn.rfind('.'):] != '.pcap':
                print('------- removing <%s>' % path)
                os.remove(path)
            name = file_dn[:-5]
            if name.find('_') > 8:
                if name in dict_captures_by_name:
                    _id = dict_captures_by_name[name]['capture']
                    new_path = os.path.join(folder_path, '%s_%s.pcap' % (_id, name))
                    os.rename(path, new_path)
                    file_dn = '%s_%s.pcap' % (_id, name)
                    print(f'{path}\n\trenamed to\n\t\t{new_path}')
            else:
                _id = file_dn[0:file_dn.find('_')]
                name = file_dn[1+file_dn.find('_'):]

            path = os.path.join(folder_path, file_dn)

            # out = subprocess.run(['wget', '--spider', '%s%s/%s' % (url, _id, name)], text=True, capture_output=True)
            # r_id = out.stderr.find('Length: ') + len('Length: ')
            # l_id = out.stderr[r_id:].find(' ')
            # try:
            #     web_size = int(out.stderr[r_id:r_id + l_id])
            # except:
            #     print('Broken link for: <%s_%s>' % (_id, name))
            #     continue

            res = os.stat(path)
            pcap_files[_id] = {
                'name': name,
                'size': res.st_size
                # ,'web_size': web_size
            }

            # if web_size != res.st_size:
            #     print('>%6s\t%30s:\t\t%d != %d' % (_id, name, web_size, res.st_size))

    for pcap_id in dict_captures_by_id:
        if pcap_id not in pcap_files:
            print('missing ', pcap_id, dict_captures_by_id[pcap_id]['size'])
    commands = {}
    for pcap_id in dict_captures_by_id:
        if pcap_id not in pcap_files:
            cc = dict_captures_by_id[pcap_id]
            print('> %8s %50s %10s ' % (pcap_id, cc['name'], size2text(cc['size'])))
            commands[pcap_id] = [ 'wget', '--limit-rate', '5m', '%s%s/%s' % (url, pcap_id, cc['name']),
                            '-P', '/media/princio/ssd512/pcap/',
                            '-O', '%s_%s' % (pcap_id, cc['name'])]

    for pcap_id in commands:
        cc = dict_captures_by_id[pcap_id]
        if cc["size"] < 60_000_000_000 and cc["size"] > 100_000_000:
            print(f'{pcap_id}   {size2text(cc["size"])}')
            cmd = subprocess.run(commands[pcap_id])


    dn_s = {}
    for walker in os.walk('./stratosphere/files/pcap'):
        for file_dn in walker[2]:
            if file_dn[-4:] == 'list':
                df = pd.read_csv(os.path.join(walker[0], file_dn))
                file_name = file_dn[:-9]
                if file_name not in dict_captures:
                    print(file_name)
                else:
                    dn_s[file_name] = {
                        'size': size2text(dict_captures[file_name]["size"]),
                        'DNs num': df.shape[0],
                        'unique DNs num': df.drop_duplicates().shape[0]
                    }

    print(pd.DataFrame(dn_s).T.sort_values(by=['unique DNs num'], ascending=False))
    pass