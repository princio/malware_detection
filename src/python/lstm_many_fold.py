import numpy as np
import os
import random
import csv
import collections
import math
import pandas as pd
import tensorflow as tf
import keras
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras import layers, Input, Model
from tensorflow import TensorShape
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import confusion_matrix, precision_score, recall_score, classification_report,accuracy_score, f1_score
import time
from datetime import datetime
from Extractor import Extractor
import json
from Model import Model as MyModel
import time

def report(y_pred, y_result):
    return [
        confusion_matrix(y_pred, y_result),
        precision_score(y_pred, y_result),
        recall_score(y_pred, y_result),
        f1_score(y_pred, y_result),
        accuracy_score(y_pred, y_result)
    ]

def my_standardize(tensor):
    return tf.reverse(tf.strings.bytes_split(tensor), [1])

def build_binary_model(units, lstm_layers):
    vocabulary = ['-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
    layer_pre = preprocessing.TextVectorization(output_mode='int', output_sequence_length=units, split=my_standardize, standardize=tf.strings.lower)
    layer_pre.set_vocabulary(vocabulary)
    layer_pre.build(TensorShape([1,1]))
    inputs = Input(shape=(1,), dtype="string")
    x = layer_pre(inputs)
    x = layers.Embedding(input_dim=len(layer_pre.get_vocabulary()), output_dim=128)(x)
    x = layers.LSTM(lstm_layers)(x)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(1, activation='sigmoid')(x)
    model = Model(inputs, outputs)
    model.compile(
        optimizer=keras.optimizers.RMSprop(),
        loss=keras.losses.BinaryCrossentropy(),
        metrics=tf.keras.metrics.BinaryAccuracy(name='binary_accuracy', threshold=0.5)
    )
    return model

def create_class_weight(labels_dict, mu):
    """Create weight based on the number of dn name in the dataset"""
    counter = collections.Counter(labels_dict)
    total = sum(counter.values())
    class_weight = {}

    for key in labels_dict:
        class_weight[key] = math.pow(total/float(labels_dict[key]), mu)	

    print('class_weight', class_weight)
    return class_weight

def classifaction_report_csv(report, precision, recall, f1_score, fold):
    """Generate the report to data processing"""
    with open('classification_report_cost.csv', 'a') as f:
        report_data = []
        lines = report.split('\n')
        row = {}
        row['class'] =  "fold %u" % (fold+1)
        report_data.append(row)
        for line in lines[2:44]:
            row = {}
            line = " ".join(line.split())
            row_data = line.split(' ')
            if(len(row_data)>2):
                if(row_data[0]!='avg'):
                    row['class'] = row_data[0]
                    row['precision'] = float(row_data[1])
                    row['recall'] = float(row_data[2])
                    row['f1_score'] = float(row_data[3])
                    row['support'] = row_data[4]
                    report_data.append(row)
                else:
                    row['class'] = row_data[0]+row_data[1]+row_data[2]
                    row['precision'] = float(row_data[3])
                    row['recall'] = float(row_data[4])
                    row['f1_score'] = float(row_data[5])
                    row['support'] = row_data[6]
                    report_data.append(row)
        row = {}
        row['class'] = 'macro'
        row['precision'] = float(precision)
        row['recall'] = float(recall)
        row['f1_score'] = float(f1_score)
        row['support'] = 0
        report_data.append(row)
        dataframe = pd.DataFrame.from_dict(report_data)
        dataframe.to_csv(f, index = False)

def get_sss_tensors(sss_train, sss_test, X, Y, DN=None, C=None):
    return {
        'X_train': X[sss_train],
        'Y_train': Y[sss_train],
        'X_test': X[sss_test],
        'Y_test': Y[sss_test],
        'DN_test': None if DN is None else DN[sss_test],
        'C_test':  None if C is None else C[sss_test],
    }

def run():
    for transformer in [ Extractor.DOMAIN,  Extractor.NOSFX ]:
        dataset_path = 'training/dataset_training.csv'
        now = datetime.now()
        lstm_layers = 256
        epochs = 20
        folds = 3
        batch_size = 128
        units = 63 if transformer == Extractor.DOMAIN else 63

        translator = transformer.translator()

        df = pd.read_csv(dataset_path)
        
        DN = df['dn']
        X = df['dn'].apply(translator).values
        Y = df['legit'].apply(lambda x: 1 if x == 'dga' else 0).values
        C = df['class'].values

        OUT_PATH = 'training/'

        count = 0
        dir_path = f"{OUT_PATH}model_{lstm_layers}_{units}_{transformer.name}_{folds}_{epochs}_binary"
        while os.path.exists(f"{dir_path}_{count}/"):
            count += 1
            pass
        dir_path = f"{dir_path}_{count}/"
        os.mkdir(dir_path)



        sss_fold = StratifiedShuffleSplit(n_splits=folds, test_size=0.1, random_state=0)
        fold =0
        for fold_train, fold_test in sss_fold.split(X,Y):
            fold = fold+1
            fold_tensors = get_sss_tensors(fold_train, fold_test, X, Y, DN, C)

            start = time.time()
            model = build_binary_model(units, lstm_layers)


            ### generating epochs tensors ###
            sss_ep = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)
            for ep_train, ep_test in sss_ep.split(fold_tensors['X_train'], fold_tensors['Y_train']):
                ep_tensors = get_sss_tensors(ep_train, ep_test, fold_tensors['X_train'], fold_tensors['Y_train'])
            ### generating epochs tensors ###

            fold_path = os.path.join(dir_path, f'fold_{fold}')
            os.mkdir(fold_path)
            ### epochs ###
            callbacks = [
                keras.callbacks.ModelCheckpoint(
                    filepath=os.path.join(fold_path, f'model') + '_{epoch}',
                    save_best_only=False,
                    monitor="val_binary_accuracy"
                )
                #, tf.keras.callbacks.TensorBoard(log_dir=f'{dir_path}/logs')
            ]
            class_weight = create_class_weight(collections.Counter(ep_tensors['Y_train']), 0.1)

            history = model.fit(
                ep_tensors['X_train'], ep_tensors['Y_train'], 
                validation_data=(ep_tensors['X_test'], ep_tensors['Y_test']),
                batch_size=batch_size,
                epochs=epochs,
                class_weight=class_weight,
                callbacks=callbacks
            )
            ### epochs ###

            best_model_epoch = 1 + np.argmax(history.history['val_binary_accuracy'])
            best_model = MyModel(os.path.join(fold_path, f'model_{best_model_epoch}'), f'model_{best_model_epoch}', transformer, True)
            df_val_Y, _ = best_model.predict(fold_tensors['DN_test'], return_DN=True)
            val_Y = df_val_Y['Y'].apply(lambda x: 0 if x <= 0.5 else 1)
            val_Y_label = pd.Series(fold_tensors['Y_test'], index=df_val_Y.index)
            df_val_Y['label'] = val_Y_label
            df_val_Y['X'] = pd.Series(fold_tensors['X_test'], index=df_val_Y.index)
            df_val_Y['class'] = pd.Series(fold_tensors['C_test'], index=df_val_Y.index)

            df_val_Y[['X', 'Y', 'label', 'class']].to_csv(dir_path + f'fold_{fold}_infos.csv')
            with open(dir_path + f'fold_{fold}_infos.json', 'w') as fp:
                json.dump({
                    'time': start,
                    'transformer': transformer.name,
                    'dataset_path': dataset_path,
                    'datetime': now.strftime("%d/%m/%Y %H:%M:%S"),
                    'lstm_layers': lstm_layers,
                    'epochs': epochs,
                    'nfolds': folds,
                    'batch_size': batch_size,
                    'units': units,
                    'count': count,
                    'class_weights': [class_weight[k] for k in class_weight],
                    'history': history.history,
                    'best_epoch': (1 + np.argmax(history.history['binary_accuracy'])).item(),
                    'score': str(f1_score(val_Y, val_Y_label, average="macro")),
                    'precision': str(precision_score(val_Y, val_Y_label, average="macro")),
                    'recall': str(recall_score(val_Y, val_Y_label, average="macro")),
                    'report': str(classification_report(val_Y,val_Y_label, digits=8)),
                    'acc': str(accuracy_score(val_Y, val_Y_label)),
                    'confusion_matrix': str(confusion_matrix(val_Y, val_Y_label))
                }, fp)
        print('resting...', datetime.now())
        time.sleep(8*60)
        print('done.', datetime.now())
        
if __name__ == "__main__":
    run()
