import re
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
import requests
import pandas as pd
import os, sys
import subprocess
import re
import json
import pickle
from pcap2csv import ProcessPcap
from shutil import copyfile


with open(os.path.join(os.path.dirname(__file__), 'top_10m.dsfx.pkl'), 'rb') as fp:
    df_10m_set = pickle.load(fp)

TYPE='Botnet'
FOLDER = {
    'Normal': 'normal',
    'Botnet': 'malware'
}
REGEX = {
    'Normal': r">(CTU-Normal-\d+(?:-\d)?)\/<",
    'Botnet': r">(CTU-Malware-Capture-Botnet-\d+(?:-\d)?)\/<"
}
URL_NAME = {
    'Normal': 'CTU-Normal',
    'Botnet': 'CTU-Malware-Capture-Botnet'
}
CAPTURE_ID_POS = {
    'Normal': 11,
    'Botnet': 27
}
ROOT_DIR = '/media/princio/ssd512/stratosphere2'
ROOT_DIRS = {
    'Botnet': os.path.join(ROOT_DIR, 'malware'),
    'Normal': os.path.join(ROOT_DIR, 'normal')
}


def _join(*paths):
    return os.path.join(ROOT_DIRS[TYPE], *paths)


def get_outs(out_file):
    prg_file = out_file + '.progress'
    log_file = out_file + '.log'
    err_file = out_file + '.error'
    return prg_file, log_file, err_file

def handle_outs(ret, out_file, message):
    prg_file, log_file, err_file = get_outs(out_file)
    if os.path.exists(log_file) and os.stat(log_file).st_size == 0:
        os.remove(log_file)

    if ret.returncode == 0:
        if os.path.exists(prg_file):
            os.rename(prg_file, out_file)
    else:
        if os.path.exists(prg_file):
            os.rename(prg_file, err_file)
            size_new = os.stat(err_file).st_size
        else:
            open(err_file, 'a').close()
        print('%s [%d].' % (message, ret.returncode))
    pass

def retrieve_real_size(file_url):
    out = subprocess.run(['wget', '--spider', file_url], text=True, capture_output=True)
    r_id = out.stderr.find('Length: ') + len('Length: ')
    l_id = out.stderr[r_id:].find(' ')
    if r_id == -1:
        return None
    try:
        size = int(out.stderr[r_id:r_id + l_id])
        print('Retrieved size:', size)
    except:
        print('Broken link for: <%s>' % (file_url))
        return None
    return size

def retrieve_captures_infos(redo = False):
    if not os.path.exists('stratosphere_malware.csv'):
        print('Fetching from web...')
        
        r = requests.get('https://mcfp.felk.cvut.cz/publicDatasets/', verify=False)

        regex = REGEX[TYPE]

        matches = re.finditer(regex, r.text, re.MULTILINE)

        mw_captures = []
        for match in matches:
            for group in match.groups():
                mw_captures.append(group)

        print('Found %d malware captures.' % len(mw_captures))

        pcaps = {}
        for mw in mw_captures:
            print(f'https://mcfp.felk.cvut.cz/publicDatasets/{mw}/')
            r = requests.get(f'https://mcfp.felk.cvut.cz/publicDatasets/{mw}/', verify=False)
            regex = r"<td.*?><a.*?>\s*([\w\d\-\._]+)\s*<\/a><\/td><td.*?>\s*([\s\d\-\:]+)\s*<\/td><td.*?>\s*([\d\.]+[KMG]?)\s*<\/td>"
            matches = re.finditer(regex, r.text, re.MULTILINE)
            for match in matches:
                groups = list(match.groups())
                ext = groups[0][1 + groups[0].rfind('.'):]
                if ext != 'pcap': continue
                size = retrieve_real_size(f'https://mcfp.felk.cvut.cz/publicDatasets/{mw}/{groups[0]}')
                pcap = {
                    'capture_id': mw[CAPTURE_ID_POS[TYPE]:],
                    'filename': groups[0],
                    'name': groups[0][:groups[0].rfind('.')],
                    'size': size
                }
                pcap['id'] = '%s_%s' % (pcap['capture_id'], pcap['name'])
                pcaps[groups[0]] = pcap
        df = pd.DataFrame(pcaps).T.sort_values(by='size')
        df.to_csv('stratosphere_malware.csv')

    if not os.path.exists(_join('stratosphere_malware.csv')):
        df = pd.read_csv('stratosphere_malware.csv', index_col=0)
        df['download'] = -30
        df['tshark'] = -30
        df['pcapfix'] = -30
        df['tshark2'] = -30
        df['2csv'] = -30
        df['old_csv'] = -30
        for idx, row in df.iterrows():
            p = os.path.join('/media/princio/ssd512/stratosphere/pcap/csv/malware', row['id'] + '.pcap.csv')
            if os.path.exists(p):
                size = -1 + int(subprocess.check_output(['wc', '-l', p]).split()[0])
            else:
                p = os.path.join('/media/princio/ssd512/stratosphere/pcap/csv/malware', row['id'] + '.fixed.pcap.csv')
                if os.path.exists(p):
                    size = -1 + int(subprocess.check_output(['wc', '-l', p]).split()[0])
                size = -1
            df.loc[idx, 'old_csv'] = size
        df.to_csv(_join('stratosphere_malware.csv'))
    else:
        df = pd.read_csv(_join('stratosphere_malware.csv'))

    for file_name, pcap in df.iterrows():

        if pcap['tshark'] > 0 or pcap['2csv'] != -30:
            print('Skipping %s.' % pcap['id'])
            continue
        
        df.to_csv(_join('stratosphere_malware.csv'))

        print('Processing %s.' % pcap['id'])

        if pcap['download'] != 0:
            df.loc[file_name, 'download'] = cmd_download(pcap)
            if df.loc[file_name, 'download'] != 0: continue


        # os.remove(_join(f'{pcap["id"]}.pcap'))


        df.loc[file_name, 'tshark'] = cmd_tshark(pcap, False)
        if df.loc[file_name, 'tshark'] in [0,2]:
            df.loc[file_name, '2csv'] = cmd_2csv(pcap, df.loc[file_name, 'tshark'] == 0)
        else:
            df.loc[file_name, 'pcapfix'] = cmd_pcapfix(pcap)
            if df.loc[file_name, 'pcapfix'] != 0:
                continue
            else:
                df.loc[file_name, 'tshark2'] = cmd_tshark(pcap, True)
                if df.loc[file_name, 'tshark2'] in [0,2]:
                    df.loc[file_name, '2csv'] = cmd_2csv(pcap, df.loc[file_name, 'tshark2'] == 0)
            continue
        
        # os.remove(_join(f'{pcap["id"]}.fixed.pcap'))
        # os.remove(_join(f'{pcap["id"]}.dns.pcap'))
        print('Processing done for %s.' % pcap['id'])

    return captures

def cmd_pcapfix(pcap):
    in_file = _join(f'{pcap["id"]}.pcap')

    out_file = _join(f'{pcap["id"]}.fixed.pcap')

    prg_file, log_file, err_file = get_outs(out_file)

    print('Fixing pcap.')
    with open(_join(out_file + '.pcapfix_log'), 'w') as biglog:
        with open(log_file,"w") as flog:
            ret = subprocess.run([
                'pcapfix', '-s' , '-k', in_file,
                '-o', prg_file],
                stdout=biglog,
                stderr=flog)

    handle_outs(ret, out_file, 'Error during fixing with pcapfix')

    return ret.returncode


def cmd_2csv(pcap, dns):
    in_file = _join(f'{pcap["id"]}%s.pcap' % ('.dns' if dns else ''))
    print('Converting 2 csv.')
    processer = ProcessPcap(in_file, df_10m_set, redo=True)
    df = processer.run()
    return -1 if df is None else df.shape[0]

def cmd_tshark(pcap, fixed):
    in_file = _join(f'{pcap["id"]}%s.pcap' % ('.fixed' if fixed else ''))

    out_file = _join(f'{pcap["id"]}.dns.pcap')

    prg_file, log_file, err_file = get_outs(out_file)

    print('Filtering dns.')
    with open(log_file,"w") as flog:
        ret = subprocess.run([
            'tshark', '-r', in_file,
            '-F', 'pcap',
            '-Y', 'dns',
            '-w', prg_file],
            stderr=flog)

    handle_outs(ret, out_file, 'Error during parsing with tshark')

    return ret.returncode

def cmd_download(pcap):
    print('Downloading %s having size %d bytes...' % (pcap['filename'], pcap['size']))

    out_file = _join(f'{pcap["id"]}.pcap')

    prg_file, log_file, err_file = get_outs(out_file)

    with open(log_file, 'w') as flog:
        ret = subprocess.run([
            'wget',
            '--limit-rate', '5m',
            f'https://mcfp.felk.cvut.cz/publicDatasets/{URL_NAME[TYPE]}-{pcap["capture_id"]}/{pcap["name"]}',
            '-O', prg_file
        ], stderr=flog)

    handle_outs(ret, out_file, 'Error during download with wget')

    return ret.returncode

    
def walk_root_dirs():
    dict_files = {'no-id': {ext: [] for ext in ROOT_DIRS}}
    for ext in ROOT_DIRS:
        root_dir = ROOT_DIRS[ext]
        for walker in os.walk(root_dir):
            for filename in walker[2]:
                capture_id_idx = filename.find('_')
                if capture_id_idx == -1 or capture_id_idx > 5:
                    dict_files['no-id'][ext].append(filename)
                    print(f'No-id for {filename} in {ext}')
                else:
                    filename_id = filename2id(filename, ext)
                    if filename_id not in dict_files:
                        dict_files[filename_id] = {}
                    if ext in dict_files[filename_id]:
                        print(f'Duplicate for {filename} in {ext}')
                    dict_files[filename_id][ext] = filename
    return dict_files

def text2size(size):
    last_char = size[-1]
    exp = 1 + 'KMG'.find(last_char)
    if exp > 0:
        return int(float(size[:-1]) * (1000 ** exp if exp > -1 else 0))

def size2text(size):
    from math import log10, floor
    n_zeros = floor(log10(size))
    unit = ''
    n_zeros -= 6
    size = size / (10 ** 6)
    unit = 'M'
    return '%d%s' % (size, unit)

def yes_or_no(question):
    while "the answer is invalid":
        reply = str(input(question+' (y/n): ')).lower().strip()
        if reply[0] == 'y':
            return True
        if reply[0] == 'n':
            return False

if __name__ == "__main__":
    redo1 = False if len(sys.argv) < 2 else sys.argv[1].lower() == 'true'
    redo2 = False if len(sys.argv) < 3 else sys.argv[2].lower() == 'true'

    dict_captures = retrieve_captures_infos(redo1)

    dict_files = walk_root_dirs()

    df_pcaps, dict_pcaps, not_processed = check_pcaps_state(dict_captures, dict_files)

    for ext in ROOT_DIRS:
        df = df_pcaps[df_pcaps[ext] == False]
        print(f'For ext:{ext} missing {df.shape[0]}/{df_pcaps.shape[0]} files')

    df_to_process = pd.DataFrame(not_processed).sort_values(by='size')

    # dict_to_process = df_to_process.to_dict(orient='index')
    # for index in dict_to_process:
    #     pcap = dict_to_process[index]
    #     cmd = cmd_download(pcap)
    #     print('Downloading %s having size %d bytes...' % (pcap['id'], pcap['size']))
    #     # if pcap['size'] > 500_000_000:
    #     #     if not yes_or_no('Size exceed 500MB, do you want to continue?'):
    #     #         continue
    #     print(subprocess.run(cmd, capture_output=True))

    pass