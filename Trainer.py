from datetime import datetime
import csv, time
from json import dump as json_dump
from collections import OrderedDict, Counter as collection_counter
import numpy as np
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM
from sklearn.model_selection import StratifiedShuffleSplit
import pandas as pd
import os

class Training:
    def __init__(self, layers = 128, epochs=20, folds=3, transformer=None, batch_size=128, units=None, dataset = 'training/setAtrain_h_t_domins.csv'):
        self.layers = layers
        self.epochs = epochs
        self.folds = folds
        self.batch_size = batch_size

        self.dropout = 0.5
        self.dense = 1

        self.vocabulary = ['-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
        self.integer_encoding = {letter: idx+1 for idx, letter in enumerate(self.vocabulary)}
        self.vocabulary_lenght = len(self.vocabulary) + 1
        self.transformer = transformer
        self.units = self.vocabulary_lenght if units is None else units

        self.dataset = dataset

    def build_binary_model(self):
        """Build LSTM model for two-class classification"""
        model = Sequential()
        model.add(Embedding(self.vocabulary_lenght, self.layers, input_length=self.units))
        model.add(LSTM(self.layers))
        model.add(Dropout(self.dropout))
        model.add(Dense(1))
        model.add(Activation('sigmoid'))

        model.compile(loss='binary_crossentropy',optimizer='rmsprop')

        return model

    def build_multiclass_model(vocabulary_lenght, units):
        """Build multiclass LSTM model for multiclass classification"""
        model = Sequential()
        model.add(Embedding(vocabulary_lenght, 128, input_length=units))
        model.add(LSTM(64))
        model.add(Dropout(self.dropout))
        model.add(Dense(38))
        model.add(Activation('softmax'))

        model.compile(loss='sparse_categorical_crossentropy',optimizer='rmsprop')

        return model


    def create_class_weight(labels_dict, mu):
        """Create weight based on the number of domain name in the dataset"""
        counter = collections.Counter(labels_dict)
        total = sum(counter.values())
        class_weight = {}

        for key in labels_dict:
            class_weight[key] = math.pow(total/float(labels_dict[key]), mu)	

        print('class_weight', class_weight)
        return class_weight

    def classifaction_report_csv(report, precision, recall, f1_score, fold):
        """Generate the report to data processing"""
        with open('classification_report_cost.csv', 'a') as f:
            report_data = []
            lines = report.split('\n')
            row = {}
            row['class'] =  "fold %u" % (fold+1)
            report_data.append(row)
            for line in lines[2:44]:
                row = {}
                line = " ".join(line.split())
                row_data = line.split(' ')
                if(len(row_data)>2):
                    if(row_data[0]!='avg'):
                        row['class'] = row_data[0]
                        row['precision'] = float(row_data[1])
                        row['recall'] = float(row_data[2])
                        row['f1_score'] = float(row_data[3])
                        row['support'] = row_data[4]
                        report_data.append(row)
                    else:
                        row['class'] = row_data[0]+row_data[1]+row_data[2]
                        row['precision'] = float(row_data[3])
                        row['recall'] = float(row_data[4])
                        row['f1_score'] = float(row_data[5])
                        row['support'] = row_data[6]
                        report_data.append(row)
            row = {}
            row['class'] = 'macro'
            row['precision'] = float(precision)
            row['recall'] = float(recall)
            row['f1_score'] = float(f1_score)
            row['support'] = 0
            report_data.append(row)
            dataframe = pd.DataFrame.from_dict(report_data)
            dataframe.to_csv(f, index = False)

    def gen(train, test, X, Y):
        train_X, test_X, train_y, test_y = X[train], X[test], Y[train], Y[test]
        trainv = {
            'X': train_X,
            'Y': train_y,
            'idx': train
        }
        testv = {
            'X': test_X,
            'Y': test_y,
            'idx': test
        }
        return trainv, testv

    def train_epochs(self, model, ftrain, ftest):
        sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)
        for train_idxs, test_idxs in sss1.split(ftrain['X'], ftrain['Y']):
            etrain, etest = gen(train_idxs, test_idxs, ftrain['X'], ftrain['Y'])

        class_weight = create_class_weight(collections.Counter(ftrain['Y']), 0.1)
        print('class weight', class_weight)

        best_auc = 0.0
        for ep in range(max_epoch):
            print("epoch %d/%d" % (ep, max_epoch))
            model.fit(etrain['X'], etrain['Y'], batch_size=batch_size, epochs=1, class_weight=class_weight)
            t_probs = model.predict(etest['X'])
            t_result = [0 if(x<=0.5) else 1 for x in t_probs]
            t_acc = accuracy_score(etest['Y'], t_result)
            if t_acc > best_auc:
                best_model = model
                best_auc = t_acc
                best_idx_train = etrain['idx']
                best_idx_test = etest['idx']
                best_t_predictions = t_probs
            accuracies[fold].append(str(t_acc))
            print('accuracy:', t_acc)


    def prepare(self):

        with open('training/setAtrain_h_t_domins.csv', "r") as f:
            reader = csv.reader(f)
            indata = [row for row in reader]
        df = pd.read_csv('training/setAtrain_h_t_domins.csv')
        
        if os.environ['DEBUG'] == 'True':
            indata = indata[0:1000]
            df = df.iloc[0:1000]


        bin_labels = [x[0] for x in indata]
        dga_labels = [x[1] for x in indata]
        domains    = [x[3] for x in indata]
        translator = self.transformer.translator()
        X = [translator(d) for d in domains]
        X = [[self.integer_encoding[y] for y in x] for x in X]
        X = sequence.pad_sequences(X, maxlen=self.units, truncating='post')
        y_binary = np.array([0 if x == 'legit' else 1 for x in bin_labels])

        if units is None:
            units = np.max([len(x) for x in X])

        # print('indata len', len(indata))
        # print('indata row', indata[1])
        # print('bin_label', bin_labels[1])
        # print('dga_label', dga_labels[1])
        # print('input_raw', X[1])
        # print('input_int_encoded', X[1])
        # print('bin_encoded', y_binary[1])
        # print(f'Starting training with:')
        # print(f'epochs={max_epochs},folds={self.folds},batch_size={batch_size}')
        # print('vocabulary_lenght=%d\nvocabulary=%s' % (vocabulary_lenght, ','.join(vocabulary)))
        # print(f'layers={layers},units={units},domain_level={domain_level}')

        start = time.time()
        sss = StratifiedShuffleSplit(n_splits=self.folds, test_size=0.2, random_state=0)
        fold = 0
        accuracies = [[] for f in range(self.folds)]
        for train_idxs, test_idxs in sss.split(X, y_binary):
            print("fold %u/%u" % (fold+1, self.folds))
            fold = fold+1
            ftrain, ftest = gen(train_idxs, test_idxs, X, y_binary)

            model = build_binary_model(self.vocabulary_lenght, units, self.layers)
            
            print("Training the model for two-class classification stage...")

            model = self.train_epochs(model, ftrain, ftest)

            

            model_json = best_model.to_json()
            name_file = f"training/model_{layers}_{units}_{domain_level.name}_{fold}_{ep}_binary"
            with open(name_file + ".json", "w") as json_file:
                json_file.write(model_json)
                best_model.save_weights(name_file + ".h5")
                df.iloc[best_idx_train].to_csv(name_file + ".train.csv")
                df_test = df.iloc[best_idx_test]
                df_test['prediction'] = best_t_predictions
                df_test.to_csv(name_file + ".test.csv")
            with open(name_file + ".accuracies.txt", "a+") as f:
                f.write(','.join(accuracies[fold]))

    def __build_binary_model(self, max_features, maxlen):
        """Build LSTM model for two-class classification"""
        model = Sequential()
        model.add(Embedding(max_features, 128, input_length = maxlen))
        model.add(LSTM(128))
        model.add(Dropout(0.5))
        model.add(Dense(1))
        model.add(Activation('sigmoid'))

        model.compile(loss='binary_crossentropy', optimizer='rmsprop')

        return model

    def __create_class_weight(self, labels_dict, mu):
        """Create weight based on the number of domain name in the dataset"""
        total = np.sum(list(labels_dict.values()))
        keys = labels_dict.keys()
        class_weight = dict()

        for key in keys:
            score = math.pow(total / float(labels_dict[key]), mu)
            class_weight[key] = score	

        return class_weight