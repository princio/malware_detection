"""Train and test LSTM classifier"""
import json
from collections import OrderedDict
import numpy as np
import os
import random
import csv
import collections
import math
import time
import pandas as pd
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score
import joblib
from sklearn.metrics import confusion_matrix
from datetime import datetime
from keras.models import model_from_json
from keras.losses import binary_crossentropy, sparse_categorical_crossentropy
import keras.backend as K
import json

# This is a global variable used in semantic loss calculation
valid_class = []

datasetname='setAtrain_h_t_domins'

train_binary = True
train_multi = False
max_epoch = 20
nfolds = 3

def evaluate(y_test, y_pred, f_report, start, end, fold, valid_class):    
    # Take the dga's names
    #labels_names = list(dga_dict.keys())
    target_names = list(valid_class.keys())
    labels = list(valid_class.values())

    y_test = np.array(y_test)
    y_test = y_test.ravel()

    print("y_pred len: %d" % len(y_pred))
    print("y_test len: %d" % len(y_test))
    print(valid_class.keys())
    print(valid_class.values())

    class_report = classification_report(y_test, y_pred, digits=4, labels=labels, target_names=target_names)
    
    # Macro and micro f1score, precision and recall all dataset
    score_macro = f1_score(y_test, y_pred, average="macro")
    precision_macro = precision_score(y_test, y_pred, average="macro")
    recall_macro = recall_score(y_test, y_pred, average="macro")

    score_micro = f1_score(y_test, y_pred, average="micro")
    precision_micro = precision_score(y_test, y_pred, average="micro")
    recall_micro = recall_score(y_test, y_pred, average="micro")

    matrix=[]

    matrix.append([score_macro, score_micro])
    matrix.append([precision_macro, precision_micro])
    matrix.append([recall_macro, recall_micro])

    matrix = np.array(matrix)

    colonne=["macro","micro"]

    indici=["f1_score", "precision", "recall"]
    df = pd.DataFrame(matrix, index=indici, columns=colonne)
    print("F1_score Precision Recall dataset \n")
    print(df)
    
    f_report.write("\n\nTesting (minutes)" + str((end-start)/float(60)) + '\n')
    
    accuracy = accuracy_score(y_test, y_pred)
    f_report.write("\nClass report: \n" + class_report)
    f_report.write("\nF1_score Precision Recall dataset \n" + str(df))
    f_report.write("\n\nOverall accuracy = " + str(accuracy) + '\n\n')
    f_report.write("\nAccuracy for each class:\n")


    # Accuracy for each class
    c_matrix = confusion_matrix(y_test, y_pred)
    cm_acc = c_matrix.astype('float') / c_matrix.sum(axis=1)[:, np.newaxis]
    accuracy_each_class = cm_acc.diagonal()
    
    # The following cicle is to format and print(the accuracy)
    count_name = 0
    print("\nAccuracy for each class:\n")
    for acc_cls in accuracy_each_class:
        print('{:>14} {:1}'.format(str(target_names[count_name]), str(round(acc_cls, 3)))+'\n')
        f_report.write('{:>14} {:1}'.format(str(target_names[count_name]), str(round(acc_cls, 3)))+'\n')
        count_name +=1
    
    f_report.write("\n \n")
	
    print(class_report)

    TP = np.diag(c_matrix)
    TP = np.sum(TP)
    TotalElement = np.asmatrix(c_matrix)
    TotalElement = np.sum(TotalElement)
    f_report.write("\nTrue positive: " + str(TP) + "\n")
    print("\nTrue positive: " + str(TP) + "\n")
    f_report.write("\n \n")


def get_data(datasetname): 
	"""Read data from file (Traning, testing and validation) to process"""
	data= []
	with open("datasets/%s.csv" % datasetname, "r") as f:
		reader = csv.reader(f)
		for row in reader:
			data.append(row)
	return data

def build_binary_model(max_features, maxlen):
    """Build LSTM model for two-class classification"""
    model = Sequential()
    model.add(Embedding(max_features, 128, input_length = maxlen))
    model.add(LSTM(128))
    model.add(Dropout(0.5))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))

    model.compile(loss='binary_crossentropy', optimizer='rmsprop')

    return model

def build_multiclass_model(max_features, maxlen):
    """Build multiclass LSTM model for multiclass classification"""
    model = Sequential()
    model.add(Embedding(max_features, 128, input_length = maxlen))
    model.add(LSTM(128))
    model.add(Dropout(0.5))
    model.add(Dense(38))
    model.add(Activation('softmax'))

    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')

    return model


def save_model(model, name):
    with open("%s/%s.json" % (output_folder, name), "w") as json_file:
        json_file.write(model.to_json())
    model.save_weights("%s/%s.h5" % (output_folder, name))


def create_class_weight(labels_dict, mu):
    """Create weight based on the number of domain name in the dataset"""
    total = np.sum(list(labels_dict.values()))
    keys = labels_dict.keys()
    class_weight = dict()

    for key in keys:
        score = math.pow(total / float(labels_dict[key]), mu)
        class_weight[key] = score	

    return class_weight


def run(date,max_epoch=max_epoch, nfolds=nfolds, batch_size=128):
    """Run train/test on logistic regression model"""
    # Begin preprocessing stage
    # Read data to process
    indata = get_data(datasetname)

    confusion_matricies = {}
    
    # Extract data and labels
    binary_labels = [x[0] for x in indata] #legit o dns
    X = [x[2] for x in indata] #DNS
    labels = [x[1] for x in indata] #famiglia dga

    # Generate a dictionary of valid characters
    valid_chars = {x:idx+1 for idx, x in enumerate(OrderedDict.fromkeys(''.join(X)))}

    with open("%s/alphabet.json" % output_folder, "w") as json_file:
        json.dump(valid_chars, json_file)

    max_features = len(valid_chars) + 1
    maxlen = np.max([len(x) for x in X])

    # Convert characters to int and pad
    X = [[valid_chars[y] for y in x] for x in X]
    X = sequence.pad_sequences(X, maxlen=maxlen)
   
	# Convert labels to 0-1 for binary class
    y_binary = np.array([0 if x == 'legit' else 1 for x in binary_labels])

    # Convert labels to 0-37 for multi class
    valid_class = {i:indx for indx, i in enumerate(set(labels))}
    y = [valid_class[x] for x in labels]
    y = np.array(y)
    white=valid_class['alexa']
    # End preprocessing stage
    
    start = time.time()

    # Begin two-class classification stage
    # Divide the dataset into training + holdout (80%) and testing (10%) dataset
    sss = StratifiedShuffleSplit(n_splits=nfolds, test_size=0.1, random_state=0)
    fold =0
    for train, test in sss.split(X, y):
        print("fold %u/%u" % (fold + 1, nfolds))
        fold = fold + 1
        
        X_train, X_test, y_train, y_test, y_dga_train, y_dga_test = X[train], X[test], y_binary[train], y_binary[test], y[train], y[test]
        y_dga =[]
        X_dga =[]
        for i in range(len(y_dga_train)):
        	if y_dga_train[i]!= white:
        		y_dga.append(y_dga_train[i])
        		X_dga.append(X_train[i])
        X_dga =np.array(X_dga)
        y_dga =np.array(y_dga)      

        # Build the model for two-class classification stage
        model = build_binary_model(max_features, maxlen)
        
        sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)
        
        # one time for-cycle
        for train, test in sss1.split(X_train, y_train):
            X_train, X_holdout, y_train, y_holdout=  X_train[train], X_train[test], y_train[train], y_train[test]
        
        # Create weight for two-class classification stage
        labels_dict=collections.Counter(y_train)
        class_weight = create_class_weight(labels_dict, 0.1)
        best_auc = 0.0
        best_epoch = -1
        
        print("Training the model for two-class classification stage...")
        for n_epoch in range(max_epoch):
            print("Training epoch %d..." % n_epoch)
            model.fit(X_train, y_train, batch_size=batch_size, epochs=1, class_weight=class_weight)
            t_probs = model.predict(X_holdout)
            t_result = [0 if (x <= 0.5) else 1 for x in t_probs]
            t_acc = accuracy_score(y_holdout, t_result)
            confusion_m = confusion_matrix(y_holdout, t_result)
            confusion_matricies[f"{fold}_{n_epoch}"] = confusion_m.ravel().tolist()
            if t_acc > best_auc:
                best_model = model
                best_auc = t_acc
                best_epoch = n_epoch
            save_model(model, "fold_%d/binary_model_epoch_%d" % (fold, n_epoch))
        
        with open("%s/accuracies.json" % (output_folder), 'w') as fp:
            json.dump(confusion_matricies, fp)
            
        # save_model(best_model, "fold_%d/best_binary_model_epoch_%d" % (fold, best_epoch))

        # y_pred = best_model.predict(X_test)
        # y_bin_result = [0 if(x <= 0.5) else 1 for x in y_pred]
        # y_binary_true = y_test
        # with open("%s/report_binary_fold_%d.txt" % (output_folder, fold), 'a') as f_report_binary:
        #     class_binary={'white':0 , 'black':1}
        #     evaluate(y_binary_true, y_bin_result, f_report_binary, start, time.time(), fold, class_binary)

        # End of two-class classification stage
        

        if train_multi:
            # Begin multiclass classification stage
            # Build the model for multiclass classification stage
            model_dga = build_multiclass_model(max_features, maxlen)
            print("Training the model in multiclass classification...")
            
            sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)
            for train, test in sss2.split(X_dga, y_dga):
                X_train, X_holdout, y_train, y_holdout = X_dga[train], X_dga[test], y_dga[train], y_dga[test]
            
            # Caculate class weight 
            labels_dict = collections.Counter(y_train)
            class_weight = create_class_weight(labels_dict, 0.3)

            print('Class weight for multiclass classification: {}'.format(class_weight))
            best_acc = 0.0

            # 20
            for n_epoch in range(max_epoch):
                model_dga.fit(X_train, y_train, batch_size=batch_size, epochs=1, class_weight=class_weight)
                y_pred= model_dga.predict_proba(X_holdout)
                y_result = [np.argmax(x) for x in y_pred]  
                t_acc= accuracy_score(y_holdout, y_result)
                if t_acc > best_acc:
                    best_model_dga = model_dga
                    best_acc = t_acc

            print("Saving multiclass model to disk...")
            # Save the model for multiclass classification stage
            model_json = best_model_dga.to_json()
            
            with open("%s/multicass_model.json" % output_folder, "w") as json_file:
                json_file.write(model_json)

            # serialize weights to HDF5
            best_model_dga.save_weights("%s/multicass_weights.h5" % output_folder)
            
            print("Saved multiclass model to disk.")

            y_pred = best_model.predict_proba(X_test)
            y_result = [white if(x <= 0.5) else 0 for x in y_pred]
        
            end = time.time()
            
            X_dga_test =[]
            y_dga_test_labels =[]
            for i in range(len(y_result)):
                if y_result[i] == 0:
                    X_dga_test.append(X_test[i]) 
                    y_dga_test_labels.append(y_dga_test[i]) #Valori veri dei dga
            X_dga_test = np.array(X_dga_test)
            y_dga_test_labels = np.array(y_dga_test_labels)
            y_pred_dga = best_model_dga.predict_proba(X_dga_test)
            y_result_dga = [np.argmax(x) for x in y_pred_dga]

            j = 0
            for i in range(len(y_result)):
                if y_result[i] != white:
                    y_result[i] = y_result_dga[j]
                    j = j + 1

            print("\n\nMulticlass report: \n")
            with open("%s/report_multiclass.txt" % output_folder, 'a') as f_report_multi:
                evaluate(y_dga_test, y_result, f_report_multi, start, end, fold, valid_class)


date = datetime.now()
if __name__ == "__main__":

    output_folder = "models/%s__epochs_%d_folds_%d" % (date.strftime("%Y%m%d_%H%M%S"), max_epoch, nfolds)
    if os.path.exists(output_folder) is True:
        raise NotADirectoryError
    os.makedirs(output_folder)
    for f in range(nfolds):
        os.makedirs("%s/fold_%d" % (output_folder, f+1))

    run(date)
