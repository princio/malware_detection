import numpy as np
import os
import random
import csv
import collections
import math
import pandas as pd
import tensorflow as tf
import keras
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras import layers, Input, Model
from tensorflow import TensorShape
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import precision_score, recall_score, classification_report,accuracy_score, f1_score
import time
from datetime import datetime
from DomainLevel import Transformer
import json


def build_binary_model(units, lstm_layers):
    vocabulary = ['-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
    layer_pre = preprocessing.TextVectorization(output_mode='int', output_sequence_length=units, split=tf.strings.bytes_split, standardize=tf.strings.lower)
    layer_pre.set_vocabulary(vocabulary)
    layer_pre.build(TensorShape([1,1]))
    inputs = Input(shape=(1,), dtype="string")
    x = layer_pre(inputs)
    x = layers.Embedding(input_dim=len(layer_pre.get_vocabulary()), output_dim=128)(x)
    x = layers.LSTM(lstm_layers)(x)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(1, activation='sigmoid')(x)
    model = Model(inputs, outputs)
    model.compile(
        optimizer=keras.optimizers.RMSprop(),
        loss=keras.losses.BinaryCrossentropy(),
        metrics=[keras.metrics.BinaryAccuracy()]
    )
    return model

def create_class_weight(labels_dict, mu):
    """Create weight based on the number of dn name in the dataset"""
    counter = collections.Counter(labels_dict)
    total = sum(counter.values())
    class_weight = {}

    for key in labels_dict:
        class_weight[key] = math.pow(total/float(labels_dict[key]), mu)	

    print('class_weight', class_weight)
    return class_weight

def classifaction_report_csv(report, precision, recall, f1_score, fold):
    """Generate the report to data processing"""
    with open('classification_report_cost.csv', 'a') as f:
        report_data = []
        lines = report.split('\n')
        row = {}
        row['class'] =  "fold %u" % (fold+1)
        report_data.append(row)
        for line in lines[2:44]:
            row = {}
            line = " ".join(line.split())
            row_data = line.split(' ')
            if(len(row_data)>2):
                if(row_data[0]!='avg'):
                    row['class'] = row_data[0]
                    row['precision'] = float(row_data[1])
                    row['recall'] = float(row_data[2])
                    row['f1_score'] = float(row_data[3])
                    row['support'] = row_data[4]
                    report_data.append(row)
                else:
                    row['class'] = row_data[0]+row_data[1]+row_data[2]
                    row['precision'] = float(row_data[3])
                    row['recall'] = float(row_data[4])
                    row['f1_score'] = float(row_data[5])
                    row['support'] = row_data[6]
                    report_data.append(row)
        row = {}
        row['class'] = 'macro'
        row['precision'] = float(precision)
        row['recall'] = float(recall)
        row['f1_score'] = float(f1_score)
        row['support'] = 0
        report_data.append(row)
        dataframe = pd.DataFrame.from_dict(report_data)
        dataframe.to_csv(f, index = False)

def gen(train, test, X, Y):
    train_X, test_X, train_y, test_y = X[train], X[test], Y[train], Y[test]
    trainv = {
        'X': train_X,
        'Y': train_y,
        'idx': train
    }
    testv = {
        'X': test_X,
        'Y': test_y,
        'idx': test
    }
    return trainv, testv



def run():
    dataset_path = 'training/dataset_training.csv'
    now = datetime.now()
    lstm_layers = 128
    epochs = 30
    folds = 1
    batch_size = 64
    units = None #max input lenght

    transformer = Transformer.NOTLD
    translator = transformer.translator()

    df = pd.read_csv(dataset_path)#, nrows=1_000)
    
    bin_labels = df['legit']
    dga_labels = df['class']
    dns    = df['dn']
    X = [translator(x) for x in dns]
    Y = [1 if b == 'dga' else 0 for b in bin_labels]
    X = np.array(X)
    Y = np.array(Y)
    if units is None:
        units = np.max([len(x) for x in X]).item()

    # dataset = tf.data.Dataset.from_tensor_slices((X, Y))

    count = 0
    while os.path.exists(f"training/model_{lstm_layers}_{units}_{transformer.name}_{folds}_{epochs}_binary_{count}/"):
        count += 1
        pass
    dir_path = f"training/model_{lstm_layers}_{units}_{transformer.name}_{folds}_{epochs}_binary_{count}/"
    os.mkdir(dir_path)

    print('bin_label', bin_labels[1])
    print('dga_label', dga_labels[1])
    print('input_raw', X[1])
    print('input_int_encoded', X[1])



    print(f'Starting training with:')
    print(f'epochs={epochs},folds={folds},batch_size={batch_size}')
    print(f'lstm_layers={lstm_layers},units={units},dn_level={transformer}')

    start = time.time()
    sss = StratifiedShuffleSplit(n_splits=folds, test_size=0.2, random_state=0)
    fold = 0
    folds_info = []
    for train_idxs, test_idxs in sss.split(X, Y):
        print("fold %u/%u" % (fold+1, folds))
        ftrain, ftest = gen(train_idxs, test_idxs, X, Y)

        model_path = f"{dir_path}model_{fold}"

        callbacks = [
            keras.callbacks.ModelCheckpoint(
                filepath=model_path + "_{epoch}",
                save_best_only=True,
                monitor="binary_accuracy",
            ),
            tf.keras.callbacks.TensorBoard(log_dir=f'{dir_path}/logs')
        ]

        model = build_binary_model(units, lstm_layers)
        
        class_weight = create_class_weight(collections.Counter(ftrain['Y']), 0.1)

        history = model.fit(ftrain['X'], ftrain['Y'], validation_split=0.05, batch_size=batch_size, epochs=epochs, class_weight=class_weight, callbacks=callbacks)
        
        best_epoch = 1 + np.argmax(history.history['binary_accuracy'])
        with tf.keras.utils.custom_object_scope({'string_lower': tf.strings.lower, 'string_bytes_split': tf.strings.bytes_split}):
            best_model = keras.models.load_model(f"{dir_path}model_{fold}_{best_epoch}")
        evaluate_metrics = best_model.evaluate(ftest['X'], ftest['Y'], batch_size=batch_size)
        
        folds_info.append({
            'class_weights': [class_weight[k] for k in class_weight],
            'history': history.history,
            'evaluate_metrics': evaluate_metrics,
            'class_distribution': pd.Series(df['class'].values[train_idxs]).value_counts(normalize=True).to_dict()
        })

        start = time.time() - start
        fold = fold+1
        
    with open(dir_path + f'infos.json', 'w') as fp:
        json.dump({
            'time': start,
            'dataset_path': dataset_path,
            'datetime': now.strftime("%d/%m/%Y %H:%M:%S"),
            'lstm_layers': lstm_layers,
            'epochs': epochs,
            'nfolds': folds,
            'batch_size': batch_size,
            'units': units,
            'count': count,
            'transformer': transformer.name,
            'folds': folds_info
        }, fp)
        
if __name__ == "__main__":
    run()
