import numpy as np
import os
import random
import csv
import collections
import math
import pandas as pd
import tensorflow as tf
import keras
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras import layers, Input, Model
from tensorflow import TensorShape
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import precision_score, recall_score, classification_report,accuracy_score, f1_score
import time
from datetime import datetime
from DomainLevel import Transformer
import json

def build_binary_model(units, lstm_layers, standardize):
    vocabulary = ['-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
    layer_pre = preprocessing.TextVectorization(output_mode='int', output_sequence_length=units, split=tf.strings.bytes_split, standardize=standardize)
    layer_pre.set_vocabulary(vocabulary)
    layer_pre.build(TensorShape([1,1]))
    inputs = Input(shape=(1,), dtype="string")
    x = layer_pre(inputs)
    x = layers.Embedding(input_dim=len(layer_pre.get_vocabulary()), output_dim=128)(x)
    x = layers.LSTM(lstm_layers)(x)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(1, activation='sigmoid')(x)
    model = Model(inputs, outputs)
    model.compile(
        optimizer=keras.optimizers.RMSprop(),
        loss=keras.losses.BinaryCrossentropy(),
        metrics=[keras.metrics.BinaryAccuracy()]
    )
    return model

def create_class_weight(labels_dict, mu):
    """Create weight based on the number of domain name in the dataset"""
    counter = collections.Counter(labels_dict)
    total = sum(counter.values())
    class_weight = {}

    for key in labels_dict:
        class_weight[key] = math.pow(total/float(labels_dict[key]), mu)	

    print('class_weight', class_weight)
    return class_weight

def classifaction_report_csv(report, precision, recall, f1_score, fold):
    """Generate the report to data processing"""
    with open('classification_report_cost.csv', 'a') as f:
        report_data = []
        lines = report.split('\n')
        row = {}
        row['class'] =  "fold %u" % (fold+1)
        report_data.append(row)
        for line in lines[2:44]:
            row = {}
            line = " ".join(line.split())
            row_data = line.split(' ')
            if(len(row_data)>2):
                if(row_data[0]!='avg'):
                    row['class'] = row_data[0]
                    row['precision'] = float(row_data[1])
                    row['recall'] = float(row_data[2])
                    row['f1_score'] = float(row_data[3])
                    row['support'] = row_data[4]
                    report_data.append(row)
                else:
                    row['class'] = row_data[0]+row_data[1]+row_data[2]
                    row['precision'] = float(row_data[3])
                    row['recall'] = float(row_data[4])
                    row['f1_score'] = float(row_data[5])
                    row['support'] = row_data[6]
                    report_data.append(row)
        row = {}
        row['class'] = 'macro'
        row['precision'] = float(precision)
        row['recall'] = float(recall)
        row['f1_score'] = float(f1_score)
        row['support'] = 0
        report_data.append(row)
        dataframe = pd.DataFrame.from_dict(report_data)
        dataframe.to_csv(f, index = False)

def run():
    dataset_path = 'training/dataset_training.csv'
    now = datetime.now()
    standardize = 'lower_and_strip_punctuation' #tf.strings.lower # LOWER_AND_STRIP_PUNCTUATION
    lstm_layers = 128
    epochs = 30
    folds = 1
    batch_size = 64
    units = None #max input lenght

    transformer = Transformer.NOTLD
    translator = transformer.translator()

    df = pd.read_csv(dataset_path)#, nrows=1_000)
    
    bin_labels = df['legit']
    dga_labels = df['class']
    domains    = df['domain']
    X = [translator(x) for x in domains]
    Y = [1 if b == 'dga' else 0 for b in bin_labels]
    X = np.array(X)
    Y = np.array(Y)
    if units is None:
        units = np.max([len(x) for x in X]).item()

    # dataset = tf.data.Dataset.from_tensor_slices((X, Y))

    count = 0
    while os.path.exists(f"training/model_{lstm_layers}_{units}_{transformer.name}_{folds}_{epochs}_binary_{count}/"):
        count += 1
        pass
    dir_path = f"training/model_{lstm_layers}_{units}_{transformer.name}_{folds}_{epochs}_binary_{count}/"
    os.mkdir(dir_path)

    start = time.time()
    model_path = f"{dir_path}model"
    callbacks = [
        keras.callbacks.ModelCheckpoint(
            filepath=model_path + "_{epoch}",
            save_best_only=True,
            monitor="binary_accuracy",
        ),
        tf.keras.callbacks.TensorBoard(log_dir=f'{dir_path}/logs')
    ]
    model = build_binary_model(units, lstm_layers, standardize)
    class_weight = create_class_weight(collections.Counter(Y), 0.1)
    history = model.fit(X, Y, validation_split=0.20, batch_size=batch_size, epochs=epochs, class_weight=class_weight, callbacks=callbacks)
    start = time.time() - start
    
    with open(dir_path + f'infos.json', 'w') as fp:
        json.dump({
            'time': start,
            'transformer': transformer.name,
            'standardize': standardize.__str__(),
            'dataset_path': dataset_path,
            'datetime': now.strftime("%d/%m/%Y %H:%M:%S"),
            'lstm_layers': lstm_layers,
            'epochs': epochs,
            'nfolds': folds,
            'batch_size': batch_size,
            'units': units,
            'count': count,
            'class_weights': [class_weight[k] for k in class_weight],
            'history': history.history,
            'best_epoch': (1 + np.argmax(history.history['binary_accuracy'])).item()
        }, fp)
        
if __name__ == "__main__":
    run()
