import subprocess
import os
import pandas as pd
from extractor import Extractor
import pickle
from model import Model


class ProcessPcap():
    def __init__(self, pcap_path, legit_set, out_dir=None, redo=False):
        self.models = [ Model.load('nosfx'), Model.load('domain') ]
        self.redo = redo
        self.legit_set = legit_set
        if not os.path.exists(pcap_path):
            raise FileNotFoundError('Pcap file does not exists: %s' % pcap_path)
        self.pcap_path = pcap_path
        if out_dir is not None:
            if not os.path.exists(out_dir):
                raise FileNotFoundError('Output directory does not exists: %s' % out_dir)
            self.out_path = os.path.join(out_dir, os.path.basename(pcap_path))
        else:
            self.out_path = pcap_path
        self.csv_path  = f'{self.out_path}.dns.csv'
        self.prg_path  = f'{self.out_path}.dns.csv.progress'
        self.log_path_progress  = f'{self.out_path}.log.progress'
        self.log_path  = f'{self.out_path}.log'
        self.full_path = f'{self.out_path}.csv'
        pass

    def run(self):
        if not self.redo and os.path.exists(self.full_path):
            print('Pcap already processed.')
            return pd.read_csv(self.full_path)

        df = self._parse()
        if df is None:
            return None
        if df.shape[0] == 0:
            return df
        df = self._legitness(df)
        df = self._predict(df)

        #df = df.fillna(-1).astype({'frame.number_req': int, 'frame.number_res': int, 'dns.flags.rcode': int})
        df.to_csv(self.full_path, index=False)
        os.remove(self.csv_path)
        return df

    def _parse(self):
        # print('Parsing pcap to csv....')
        if not self.redo:
            if not os.path.exists(self.csv_path) and os.path.exists(self.log_path):
                print('Pcap already parsed.')
                return df.DataFrame()
            if os.path.exists(self.csv_path):
                print('Pcap already parsed.')
                return pd.read_csv(self.csv_path)
        
        fields = [
            'frame.number',
            'frame.time_epoch',
            'ip.dst',
            'dns.time',
            'dns.response_in',
            'dns.response_to',
            'dns.qry.name',
            'dns.qry.type',
            'dns.flags.response',
            'dns.flags.rcode',
            'dns.resp.ttl',
            'dns.a'
        ]

        cmd_fields = []
        for f in fields:
            cmd_fields.append('-e')
            cmd_fields.append(f)

        with open(self.log_path_progress,"w") as errfile:
            with open(self.prg_path,"w") as outfile:
                ret = subprocess.run(['tshark', '-r', self.pcap_path,
                                '-Y', 'dns', # && !_ws.malformed && !icmp',
                                '-T', 'fields'] + cmd_fields,
                                stdout=outfile, stderr=errfile)

        if ret.returncode != 0:
            print('Error during parsing with tshark [%d].' % ret.returncode)
            return None

        if os.stat(self.log_path_progress).st_size == 0:
            os.remove(self.log_path_progress)
        else:
            os.rename(self.log_path_progress, self.log_path)

        if os.stat(self.prg_path).st_size == 0:
            os.remove(self.prg_path)
            print('File size equal to 0.')
            return pd.DataFrame()

        df = pd.read_csv(self.prg_path, sep='\t', names=fields)
                
        series = df['dns.flags.response'] == 0

        df_query = df[series]
        df_response = df[series == False]

        df_query = df_query.drop(columns=['dns.time', 'dns.flags.response', 'dns.response_to', 'dns.flags.rcode', 'dns.resp.ttl', 'dns.a' ])
        df_response = df_response.drop(columns=['ip.dst', 'dns.qry.name', 'dns.qry.type', 'dns.flags.response', 'dns.response_in' ])

        df = df_query.merge(df_response, how='left', left_on='frame.number', right_on='dns.response_to', suffixes=('_req', '_res'))

        columns = [
            'frame.number_req',
            'frame.number_res',
            'frame.time_epoch_req',
            'frame.time_epoch_res',
            'ip.dst',
            'dns.time',
            'dns.qry.name',
            'dns.qry.type',
            'dns.response_in',
            'dns.response_to',
            'dns.flags.rcode',
            'dns.resp.ttl',
            'dns.a'
        ]

        df = df[columns].rename(columns={'ip.dst': 'dns_server_ip'}).drop(columns=['dns.response_in', 'dns.response_to'])

        df = df[df['dns.qry.name'].isna() == False]
        df = df[df['dns.qry.name'].apply(Extractor.is_dn)]

        if df.shape[0] == 0:
            print('No reqres.')
            return df.DataFrame()

        df.to_csv(self.csv_path, index=False)
        os.remove(self.prg_path)
        return df

    def _legitness(self, df):
        # print('Legit list check....')
        series = df['dns.qry.name'].apply(lambda dn: 1 if Extractor.domain_sfx(dn) in self.legit_set else 0)
        df.insert(len(df.columns)-2, 'legit_list', series)
        return df

    def _predict(self, df):
        # print('Predicting....')
        for model in self.models:
            series, _ = model.predict_u(df['dns.qry.name'])
            df.insert(len(df.columns)-2, model.name, series)
        return df


if __name__ == '__main__':

    pcap = '/tmp/pcap/148-1_2015-10-23_capture-win8.pcap.csv'

    models = [ Model.load('nosfx'), Model.load('domain') ]
    with open(os.path.join(os.path.dirname(__file__), 'top_10m.dsfx.pkl'), 'rb') as fp:
        df_10m_set = pickle.load(fp)

    df = pd.read_csv(pcap)

    for model in self.models:
        series, _ = model.predict_u(df['dns.qry.name'])
        df.insert(len(df.columns)-2, model.name, series)

    print(df)
    
    if False:
        folder =        '/media/princio/ssd512/stratosphere/pcap/dns_pcap/normal'
        out_directory = '/media/princio/ssd512/stratosphere/pcap/csv/normal'

        files = []
        sizes = []
        if not os.path.isdir(folder):
            print(f'Argument <{folder}> is not a folder.')
            exit(1)

        for walker in os.walk(folder):
            for filename in walker[2]:
                if filename[filename.rfind('.'):] != '.pcap': continue
                files.append(os.path.join(folder, filename))
                sizes.append(os.stat(os.path.join(folder, filename)).st_size)
                print(f'Added to list <{filename}> of size {int(sizes[-1]/1_000)} kB.')
            break

        files = {files[i[0]]: i[1] for i in sorted(enumerate(sizes), key=lambda x:x[1])}

        for file_path in files:
            print('Processing:\nfilepath: %s\n    size: %d kB' % (file_path, int(files[file_path]/1_000)))
            processer = ProcessPcap(file_path, df_10m_set, out_dir=out_directory, redo=False)
            processer.run()
            print()

    exit(0)