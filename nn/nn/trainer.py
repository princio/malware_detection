from distutils.command.config import config
import numpy as np
import os
import collections
import math
import pandas as pd
import tensorflow as tf
# from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras import layers, Input, Model
from tensorflow import TensorShape
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import confusion_matrix, precision_score, recall_score, classification_report,accuracy_score, f1_score
import time
from datetime import datetime
# from extractor import Extractor
import json
import time
from pathlib import Path
import sys
from pslregex import PSLdict
import os
import argparse
import inquirer
from dataclasses import dataclass


vocabulary = ['', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']


@dataclass
class TrainingConfiguration:
    dataset_path: str
    output_dir: str
    lstm_units: int
    folds: int
    epochs: int
    psl_remove: str
    model_name: str
    batch_size: int = 128
    input_max_len: int = 60
    vocabulary = vocabulary

    def print(self):
        print('Configuration:')
        print(f'output_dir={self.output_dir}')
        print(f'dataset={self.dataset_path}')
        print(f'units={self.lstm_units}')
        print(f'folds={self.folds}')
        print(f'epochs={self.epochs}')
        print(f'psl_remove={self.psl_remove}')
        print(f'model_name={self.model_name}')
        print(f'batch_size={self.batch_size}')
        print(f'input_max_len={self.input_max_len}')
        pass

    def save(self):
        with open(os.path.join(self.output_dir, 'config.txt'), 'w') as f:
            f.write(f'output_dir={self.output_dir}\n')
            f.write(f'dataset={self.dataset_path}\n')
            f.write(f'units={self.lstm_units}\n')
            f.write(f'folds={self.folds}\n')
            f.write(f'epochs={self.epochs}\n')
            f.write(f'psl_remove={self.psl_remove}\n')
            f.write(f'model_name={self.model_name}\n')
            f.write(f'batch_size={self.batch_size}\n')
            f.write(f'input_max_len={self.input_max_len}\n')
    pass





def report(y_pred, y_result):
    return [
        confusion_matrix(y_pred, y_result),
        precision_score(y_pred, y_result),
        recall_score(y_pred, y_result),
        f1_score(y_pred, y_result),
        accuracy_score(y_pred, y_result)
    ]

def my_standardize(tensor):
    return tf.reverse(tf.strings.bytes_split(tensor), [1])

def build_binary_model(input_max_len, lstm_units):
    """
    The LSTM input shape is defined by the Embedding output shape. The shape of Embedding, which is:
        vocabulary size x space of embedding (output_dim)
    is also the input shape of the LSTM layer. So the input shape of the LSTM layer is indipendent to
    the LSTM units number.
    The embedding layer maps a vector of 60x1 to a vector of 60x128 for example. Each letter is mapped
    from one dimension to 128-dimension.
    """
    # layer_pre = preprocessing.TextVectorization(output_mode='int', output_sequence_length=input_max_len, split=my_standardize, standardize=tf.strings.lower)
    # layer_pre.set_vocabulary(vocabulary)
    # # layer_pre.build(TensorShape([1,1]))
    # inputs = Input(shape=(input_max_len,), dtype="int32")
    #x = layer_pre(inputs)
    
    inputs = Input(shape=(input_max_len,), dtype="int32")
    x = layers.Embedding(
        input_dim=len(vocabulary), 
        output_dim=32,
        mask_zero=False
    )(inputs)
    x = layers.LSTM(lstm_units)(x)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(1, activation='sigmoid')(x)
    model = Model(inputs, outputs)
    model.compile(
        optimizer=tf.keras.optimizers.RMSprop(),
        loss=tf.keras.losses.BinaryCrossentropy(),
        metrics=tf.keras.metrics.BinaryAccuracy(name='binary_accuracy', threshold=0.5)
    )
    
    return model

def create_class_weight(labels_dict, mu):
    """Create weight based on the number of dn name in the dataset"""
    counter = collections.Counter(labels_dict)
    total = sum(counter.values())
    class_weight = {}

    for key in labels_dict:
        class_weight[key] = math.pow(total/float(labels_dict[key]), mu)	

    print('class_weight', class_weight)
    return class_weight

def classifaction_report_csv(report, precision, recall, f1_score, fold):
    """Generate the report to data processing"""
    with open('classification_report_cost.csv', 'a') as f:
        report_data = []
        lines = report.split('\n')
        row = {}
        row['class'] =  "fold %u" % (fold+1)
        report_data.append(row)
        for line in lines[2:44]:
            row = {}
            line = " ".join(line.split())
            row_data = line.split(' ')
            if(len(row_data)>2):
                if(row_data[0]!='avg'):
                    row['class'] = row_data[0]
                    row['precision'] = float(row_data[1])
                    row['recall'] = float(row_data[2])
                    row['f1_score'] = float(row_data[3])
                    row['support'] = row_data[4]
                    report_data.append(row)
                else:
                    row['class'] = row_data[0]+row_data[1]+row_data[2]
                    row['precision'] = float(row_data[3])
                    row['recall'] = float(row_data[4])
                    row['f1_score'] = float(row_data[5])
                    row['support'] = row_data[6]
                    report_data.append(row)
        row = {}
        row['class'] = 'macro'
        row['precision'] = float(precision)
        row['recall'] = float(recall)
        row['f1_score'] = float(f1_score)
        row['support'] = 0
        report_data.append(row)
        dataframe = pd.DataFrame.from_dict(report_data)
        dataframe.to_csv(f, index = False)

def get_sss_tensors(sss_train, sss_test, X, Y, DN=None, C=None):
    return {
        'X_train': X[sss_train],
        'Y_train': Y[sss_train],
        'X_test': X[sss_test],
        'Y_test': Y[sss_test],
        'DN_test': None if DN is None else DN[sss_test],
        'C_test':  None if C is None else C[sss_test],
    }


def run(ds, configuration: TrainingConfiguration):

    count = 0
    folder_name = f"{configuration.model_name}_{configuration.psl_remove}_{configuration.lstm_units}_{configuration.folds}_{configuration.epochs}_binary"
    while os.path.exists(os.path.join(configuration.output_dir, f"{folder_name}_{count}/")):
        count += 1
        pass
    configuration.output_dir = os.path.join(configuration.output_dir, f"{folder_name}_{count}/")

    configuration.print()

    output_dir = configuration.output_dir
    lstm_units = configuration.lstm_units
    folds = configuration.folds
    epochs = configuration.epochs
    batch_size = configuration.batch_size
    input_max_len = configuration.input_max_len

    os.makedirs(output_dir)
    
    configuration.save()

    now = datetime.now()
    
    # pattern = re.compile(r"[\-\.\d_\w]+")
    # DN = DN[DN.apply(lambda x: bool(pattern.match))].str[-input_max_len:]

    DN = ds['dn'].to_numpy()
    X = ds['X'].to_numpy()
    Y = (ds['label'] == 'dga').to_numpy()
    C = ds['class'].to_numpy()
    ds['X'] = ds['X'].apply(lambda x: '.'.join(x.split('.')[::-1]))
    X = ds['X'].apply(lambda x: [ vocabulary.index(c) for c in x ])

    X = tf.keras.preprocessing.sequence.pad_sequences(
        X, maxlen=input_max_len, padding='post',
        truncating='post', value=vocabulary.index('')
    )

    ds_head = ds.head().copy()

    ds_head['_X_'] = [ str(x) for x in ds_head['X'].values ]

    ds_head.to_csv(os.path.join(output_dir, 'ds_head.csv'))

    sss_fold = StratifiedShuffleSplit(n_splits=folds, test_size=0.1, random_state=0)
    fold = 0
    for fold_train, fold_test in sss_fold.split(X,Y):
        fold = fold+1

        fold_tensors = get_sss_tensors(fold_train, fold_test, X, Y, DN, C)

        start = time.time()
        model = build_binary_model(input_max_len, lstm_units)

        ### generating epochs tensors ###
        sss_ep = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)
        for ep_train, ep_test in sss_ep.split(fold_tensors['X_train'], fold_tensors['Y_train']):
            ep_tensors = get_sss_tensors(ep_train, ep_test, fold_tensors['X_train'], fold_tensors['Y_train'])
        ### generating epochs tensors ###

        fold_path = os.path.join(output_dir, f'fold_{fold}')
        os.mkdir(fold_path)
        ### epochs ###
        callbacks = [
            tf.keras.callbacks.ModelCheckpoint(
                filepath=os.path.join(fold_path, f'model') + '_{epoch}',
                save_best_only=False,
                monitor="val_binary_accuracy"
            )
            #, tf.keras.callbacks.TensorBoard(log_dir=f'{dir_path}/logs')
        ]
        class_weight = create_class_weight(collections.Counter(ep_tensors['Y_train']), 0.1)

        history = model.fit(
            ep_tensors['X_train'], ep_tensors['Y_train'], 
            validation_data=(ep_tensors['X_test'], ep_tensors['Y_test']),
            batch_size=batch_size,
            epochs=epochs,
            class_weight=class_weight,
            callbacks=callbacks
        )

        ### epochs ###

        best_model_epoch = 1 + np.argmax(history.history['val_binary_accuracy'])
        best_model_path = os.path.join(fold_path, f'model_{best_model_epoch}')

        best_model = tf.keras.models.load_model(best_model_path)

        X_pred = best_model.predict(fold_tensors['X_test'])

        X_pred = np.where(X_pred <= 0.5, 0, 1)
        
        with open(os.path.join(output_dir, f'fold_{fold}_infos.json'), 'w') as fp:
            json.dump({
                'time': start,
                'datetime': now.strftime("%d/%m/%Y %H:%M:%S"),
                'class_weights': [class_weight[k] for k in class_weight],
                'history': history.history,
                'best_epoch': (1 + np.argmax(history.history['binary_accuracy'])).item(),
                'score': str(f1_score(X_pred, fold_tensors['Y_test'], average="macro")),
                'precision': str(precision_score(X_pred, fold_tensors['Y_test'], average="macro")),
                'recall': str(recall_score(X_pred, fold_tensors['Y_test'], average="macro")),
                'report': str(classification_report(X_pred,fold_tensors['Y_test'], digits=8)),
                'acc': str(accuracy_score(X_pred, fold_tensors['Y_test'])),
                'confusion_matrix': str(confusion_matrix(X_pred, fold_tensors['Y_test']))
            }, fp)
        print('resting...', datetime.now())
        time.sleep(60)
        print('done.', datetime.now())
    pass

def add_X(ds, psl_remove):
    psldict = PSLdict()
    psldict.init(download=False, update=True)

    psldict.match('mortiscontrastatim.s3.cn-north-1.amazonaws.com.cn')

    if psl_remove == 'NONE':
        X = ds.dn.copy()
    else:
        sfx = [ psldict.match(dn) for dn in ds.dn.values ]

        ds['icann'] = [ s['icann']['suffix'] if s['icann'] is not None else None for s in sfx if s is not None ]
        ds['private'] = [ s['private']['suffix'] if s['private'] is not None else None for s in sfx if s is not None ]

        ds['tld_less'] = ds.dn.apply(lambda dn: dn[:dn.rfind('.')])
        ds['icann_less'] = [ s['icann']['dn-suffix'] if s['icann'] is not None and 'dn-suffix' in s['icann'] else None for s in sfx if s is not None ]
        ds['private_less'] = [ s['private']['dn-suffix'] if s['private'] is not None and 'dn-suffix' in s['private'] else None for s in sfx if s is not None ]

        if psl_remove == 'TLD':
            X = ds.tld_less
            pass
        elif psl_remove == 'ICANN':
                X = ds.tld_less.copy().where(ds.icann_less.isna(), ds.icann_less[~ds.icann_less.isna()], axis=0)
        elif psl_remove == 'PRIVATE':
            X = ds.tld_less.copy()
            X = X.where(ds.icann_less.isna(), ds.icann_less[~ds.icann_less.isna()], axis=0)
            X = X.where(ds.private_less.isna(), ds.private_less[~ds.private_less.isna()], axis=0)
            pass


        print(pd.concat([
                    ds.head(3),
                ds[~ds.private.isna()].head(3),
                ds[ds.private.isna()].head(3),
                ds[ds.icann.isna()].head(3)
            ]).to_markdown()
        )
        pass

    ds['X'] = X

    return ds


def perform(configuration):
    ds = pd.read_csv(configuration.dataset_path)

    ds = add_X(ds, configuration.psl_remove)

    ds = ds[['dn', 'X', 'label', 'class']]
        
    run(ds, configuration)
    
    pass
        
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Start LSTM Training')
    parser.add_argument('-f', '--folds', type=int, default=3)
    parser.add_argument('-e', '--epochs', type=int, default=20)
    parser.add_argument('-u', '--lstm-units', type=int, default=256)
    parser.add_argument('-b', '--batch-size', type=int, default=128)
    parser.add_argument('-m', '--max-len', type=int, default=60)
    parser.add_argument('--psl-remove', metavar="PSL", type=str, choices=[ 'NONE', 'ICANN', 'PRIVATE', 'TLD'], help='How to handle suffixes.', default=None)
    parser.add_argument('-o', '--output-dir', metavar="DIR", help='The output dir.', default=os.path.join(os.getcwd(), 'output/'))
    parser.add_argument('dataset_path', help='The training dataset path.')
    parser.add_argument('model_name', help='The model name.')
    args = parser.parse_args()

    
    configuration = TrainingConfiguration(
        args.dataset_path, 
        args.output_dir,
        args.lstm_units,
        args.folds,
        args.epochs,
        args.psl_remove,
        args.model_name,
        args.batch_size,
        args.max_len
    )

    if configuration.psl_remove is None:
        questions = [
            inquirer.List('psl-remove',
                message=f"How do you want to handle suffixes? [ ciao.compute-1.amazonaws.co.cn ]\n",
                choices=[ 
                    ( 'remove at least TLD -> ciao.compute-1.amazonaws.co', 'TLD' ),
                    ( 'remove at least ICANN: ciao.compute-1.amazonaws', 'ICANN' ),
                    ( 'remove at least PRIVATE: ciao', 'PRIVATE' ),
                    ( 'no remove: ciao.compute-1.amazonaws.co.cn', 'NONE' )
                ],
            ),
        ]
        answers = inquirer.prompt(questions)
        psl_remove = answers['psl-remove']
        pass

    if tf.test.is_built_with_cuda():
        with tf.device('GPU:0'):
            perform(configuration)
    else:
        perform(configuration)

    pass
