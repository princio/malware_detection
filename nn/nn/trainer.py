import numpy as np
import os
import collections
import math
import pandas as pd
import tensorflow as tf
# from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras import layers, Input, Model
from tensorflow import TensorShape
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import confusion_matrix, precision_score, recall_score, classification_report,accuracy_score, f1_score
import time
from datetime import datetime
# from extractor import Extractor
import json
import time
from pathlib import Path
import sys
from pslregex import PSLdict
import os

vocabulary = ['', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

def report(y_pred, y_result):
    return [
        confusion_matrix(y_pred, y_result),
        precision_score(y_pred, y_result),
        recall_score(y_pred, y_result),
        f1_score(y_pred, y_result),
        accuracy_score(y_pred, y_result)
    ]

def my_standardize(tensor):
    return tf.reverse(tf.strings.bytes_split(tensor), [1])

def build_binary_model(input_max_len, lstm_units):
    """
    The LSTM input shape is defined by the Embedding output shape. The shape of Embedding, which is:
        vocabulary size x space of embedding (output_dim)
    is also the input shape of the LSTM layer. So the input shape of the LSTM layer is indipendent to
    the LSTM units number.
    The embedding layer maps a vector of 60x1 to a vector of 60x128 for example. Each letter is mapped
    from one dimension to 128-dimension.
    """
    # layer_pre = preprocessing.TextVectorization(output_mode='int', output_sequence_length=input_max_len, split=my_standardize, standardize=tf.strings.lower)
    # layer_pre.set_vocabulary(vocabulary)
    # # layer_pre.build(TensorShape([1,1]))
    # inputs = Input(shape=(input_max_len,), dtype="int32")
    #x = layer_pre(inputs)
    
    inputs = Input(shape=(input_max_len,), dtype="int32")
    x = layers.Embedding(
        input_dim=len(vocabulary), 
        output_dim=32,
        mask_zero=False
    )(inputs)
    x = layers.LSTM(lstm_units)(x)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(1, activation='sigmoid')(x)
    model = Model(inputs, outputs)
    model.compile(
        optimizer=tf.keras.optimizers.RMSprop(),
        loss=tf.keras.losses.BinaryCrossentropy(),
        metrics=tf.keras.metrics.BinaryAccuracy(name='binary_accuracy', threshold=0.5)
    )
    
    return model

def create_class_weight(labels_dict, mu):
    """Create weight based on the number of dn name in the dataset"""
    counter = collections.Counter(labels_dict)
    total = sum(counter.values())
    class_weight = {}

    for key in labels_dict:
        class_weight[key] = math.pow(total/float(labels_dict[key]), mu)	

    print('class_weight', class_weight)
    return class_weight

def classifaction_report_csv(report, precision, recall, f1_score, fold):
    """Generate the report to data processing"""
    with open('classification_report_cost.csv', 'a') as f:
        report_data = []
        lines = report.split('\n')
        row = {}
        row['class'] =  "fold %u" % (fold+1)
        report_data.append(row)
        for line in lines[2:44]:
            row = {}
            line = " ".join(line.split())
            row_data = line.split(' ')
            if(len(row_data)>2):
                if(row_data[0]!='avg'):
                    row['class'] = row_data[0]
                    row['precision'] = float(row_data[1])
                    row['recall'] = float(row_data[2])
                    row['f1_score'] = float(row_data[3])
                    row['support'] = row_data[4]
                    report_data.append(row)
                else:
                    row['class'] = row_data[0]+row_data[1]+row_data[2]
                    row['precision'] = float(row_data[3])
                    row['recall'] = float(row_data[4])
                    row['f1_score'] = float(row_data[5])
                    row['support'] = row_data[6]
                    report_data.append(row)
        row = {}
        row['class'] = 'macro'
        row['precision'] = float(precision)
        row['recall'] = float(recall)
        row['f1_score'] = float(f1_score)
        row['support'] = 0
        report_data.append(row)
        dataframe = pd.DataFrame.from_dict(report_data)
        dataframe.to_csv(f, index = False)

def get_sss_tensors(sss_train, sss_test, X, Y, DN=None, C=None):
    return {
        'X_train': X[sss_train],
        'Y_train': Y[sss_train],
        'X_test': X[sss_test],
        'Y_test': Y[sss_test],
        'DN_test': None if DN is None else DN[sss_test],
        'C_test':  None if C is None else C[sss_test],
    }


def run(ds, models_folder, name):
    now = datetime.now()
    epochs = 20 if os.environ.get('EPOCHS') is None else int(os.environ.get('EPOCHS'))
    folds = 3 if os.environ.get('FOLDS') is None else int(os.environ.get('FOLDS'))
    lstm_units = 256
    batch_size = 128
    input_max_len = 60
    
    # pattern = re.compile(r"[\-\.\d_\w]+")
    # DN = DN[DN.apply(lambda x: bool(pattern.match))].str[-input_max_len:]

    DN = ds['dn'].to_numpy()
    X = ds['X'].to_numpy()
    Y = (ds['label'] == 'dga').to_numpy()
    C = ds['class'].to_numpy()

    X = ds['X'].apply(lambda x:[ vocabulary.index(c) for c in x[-input_max_len:]])

    X = tf.keras.preprocessing.sequence.pad_sequences(
        X, maxlen=input_max_len, padding='post',
        truncating='pre', value=vocabulary.index('')
    )

    count = 0
    dir_path = Path(models_folder).joinpath(f"model_{lstm_units}_{name}_{folds}_{epochs}_binary")
    while os.path.exists(f"{dir_path}_{count}/"):
        count += 1
        pass
    dir_path = f"{dir_path}_{count}/"
    os.mkdir(dir_path)


    sss_fold = StratifiedShuffleSplit(n_splits=folds, test_size=0.1, random_state=0)
    fold =0
    for fold_train, fold_test in sss_fold.split(X,Y):
        fold = fold+1
        fold_tensors = get_sss_tensors(fold_train, fold_test, X, Y, DN, C)

        start = time.time()
        model = build_binary_model(input_max_len, lstm_units)

        ### generating epochs tensors ###
        sss_ep = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)
        for ep_train, ep_test in sss_ep.split(fold_tensors['X_train'], fold_tensors['Y_train']):
            ep_tensors = get_sss_tensors(ep_train, ep_test, fold_tensors['X_train'], fold_tensors['Y_train'])
        ### generating epochs tensors ###

        fold_path = os.path.join(dir_path, f'fold_{fold}')
        os.mkdir(fold_path)
        ### epochs ###
        callbacks = [
            tf.keras.callbacks.ModelCheckpoint(
                filepath=os.path.join(fold_path, f'model') + '_{epoch}',
                save_best_only=False,
                monitor="val_binary_accuracy"
            )
            #, tf.keras.callbacks.TensorBoard(log_dir=f'{dir_path}/logs')
        ]
        class_weight = create_class_weight(collections.Counter(ep_tensors['Y_train']), 0.1)

        history = model.fit(
            ep_tensors['X_train'], ep_tensors['Y_train'], 
            validation_data=(ep_tensors['X_test'], ep_tensors['Y_test']),
            batch_size=batch_size,
            epochs=epochs,
            class_weight=class_weight,
            callbacks=callbacks
        )

        ### epochs ###

        best_model_epoch = 1 + np.argmax(history.history['val_binary_accuracy'])
        best_model_path = os.path.join(fold_path, f'model_{best_model_epoch}')

        best_model = tf.keras.models.load_model(best_model_path)

        X_pred = best_model.predict(fold_tensors['X_test'])

        X_pred = np.where(X_pred <= 0.5, 0, 1)
        
        dfdfdf = pd.concat([
                pd.Series(fold_tensors['DN_test']),
                pd.Series(X_pred.flatten()),
                pd.Series(fold_tensors['Y_test']),
                pd.Series(fold_tensors['C_test'])
            ],
            keys=['dn', 'X', 'label', 'class'],
            axis=1)
        dfdfdf['vects'] = [ ','.join([str(x) for x in v]) for v in fold_tensors['X_test'] ]
        dfdfdf.to_csv(os.path.join(dir_path, f'fold_{fold}_infos.csv'))

        with open(dir_path + f'fold_{fold}_infos.json', 'w') as fp:
            json.dump({
                'time': start,
                'transformer': 'X',
                'dataset_path': dataset_path,
                'datetime': now.strftime("%d/%m/%Y %H:%M:%S"),
                'lstm_units': lstm_units,
                'epochs': epochs,
                'nfolds': folds,
                'batch_size': batch_size,
                'count': count,
                'class_weights': [class_weight[k] for k in class_weight],
                'history': history.history,
                'best_epoch': (1 + np.argmax(history.history['binary_accuracy'])).item(),
                'score': str(f1_score(X_pred, fold_tensors['Y_test'], average="macro")),
                'precision': str(precision_score(X_pred, fold_tensors['Y_test'], average="macro")),
                'recall': str(recall_score(X_pred, fold_tensors['Y_test'], average="macro")),
                'report': str(classification_report(X_pred,fold_tensors['Y_test'], digits=8)),
                'acc': str(accuracy_score(X_pred, fold_tensors['Y_test'])),
                'confusion_matrix': str(confusion_matrix(X_pred, fold_tensors['Y_test']))
            }, fp)
        print('resting...', datetime.now())
        time.sleep(60)
        print('done.', datetime.now())
        
if __name__ == "__main__":

    if len(sys.argv) < 4:
        print('error', sys.argv)
        exit(1)

    
    dataset_path  = sys.argv[1]

    ds = pd.read_csv(dataset_path)

    psldict = PSLdict()
    
    psldict.init(download=False, update=False, nrows=os.environ.get('NROWS'))
    
    sfx = [ psldict.match(dn) for dn in ds.dn.values ]
    
    if os.path.exists('/tmp/ds.csv'):
        ds = pd.read_csv('/tmp/ds.csv', index_col=0, nrows=100)
    else:
        ds['icann'] = [ s['icann']['dn-suffix'] if s['icann'] is not None and 'dn-suffix' in s['icann'] else None for s in sfx if s is not None ]
        ds['private'] = [ s['private']['dn-suffix'] if s['private'] is not None and 'dn-suffix' in s['private'] else None for s in sfx if s is not None ]
        ds['X'] = ds.dn.where(~ds['icann'].isna(), ds['dn'][ds['icann'].isna()], axis=0)

        for col in [ 'X', 'label' ]:
            if col not in ds.columns:
                raise f'Dataset format error: missing {col} column'

        ds.to_csv('/tmp/ds.csv')
        
        pass
    models_folder  = sys.argv[2]

    name  = sys.argv[3]

    if tf.test.is_built_with_cuda():
        with tf.device('GPU:0'):
            run(ds, models_folder, name)
