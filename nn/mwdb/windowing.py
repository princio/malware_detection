from re import DEBUG
from numpy.lib.arraysetops import unique
import psycopg2
from psycopg2.extras import execute_values
import pandas as pd
import numpy as np
import time
import pprint

pp = pprint.PrettyPrinter(indent=4)

# With LLR2 we intend to set-to-0 (zeroing) the NOSFX values between two thresholds.
#
#
#
#
#
#
#
#


db = psycopg2.connect("host=localhost dbname=dns user=postgres password=postgres")


def f_llr(s_nosfx):
    num = s_nosfx.replace(
        [0, 1], [0.000_000_000_000_000_1, 1 - 0.000_000_000_000_000_1]
    )
    den = np.ones(len(num)) - num
    llr = np.log(num / den)
    return llr


df_pcap = pd.read_sql(
    'SELECT id, "name", "malware_id", "infected", "qr", q, r, "unique", days FROM pcap ORDER BY name',
    db,
)

windows = [
    2500,
    500,
    100,
]

max_q = np.lcm.reduce(windows) * (1_000_000 // np.lcm.reduce(windows))

query = """
DROP TABLE IF EXISTS public.window_qr_4;

CREATE TABLE IF NOT EXISTS public.window_qr_4
(
    id integer NOT NULL GENERATED ALWAYS AS IDENTITY ( INCREMENT 1 START 1 MINVALUE 1 MAXVALUE 2147483647 CACHE 1 ),
    pcap_id integer NOT NULL,
    size integer NOT NULL,
    "window" integer NOT NULL,

    "first" integer[] DEFAULT '{0,0,0}'::integer[],
    "last" integer[] DEFAULT '{0,0,0}'::integer[],

    "begin_time" real DEFAULT 0.0,
    "end_time" real DEFAULT 0.0,

    "app" integer[] DEFAULT '{0,0,0}'::integer[],
    "unique" integer[] DEFAULT '{0,0,0}'::integer[],

    "nx" integer DEFAULT 0,

    "gt09" double precision,

    "llr" double precision[] DEFAULT '{0,0,0}'::double precision[],
    "llr_t" double precision[] DEFAULT '{0,0,0}'::double precision[],

    CONSTRAINT window_qr_4_pkey PRIMARY KEY (id)
)

TABLESPACE pg_default;

ALTER TABLE public.window_qr_4
    OWNER to postgres;
"""

cur = db.cursor()

cur.execute(query)

cur.close()

db.commit()

BATCH_SIZE = 10_000_000

pd.options.display.float_format = '{:.2f}'.format

for idx, pcap in df_pcap.iterrows():

    cur = db.cursor()

    query = """SELECT m3.fn,
                    is_response
                FROM message3 as m3
                WHERE pcap_id=%d
                        ORDER BY fn""" % pcap.id
    df_fn = pd.read_sql(query, db)

    df_fn_q = df_fn[~df_fn.is_response].fn.to_frame().copy()
    df_fn_q['n'] = np.arange(df_fn_q.shape[0])
    df_fn_q['batch'] = df_fn_q['n'] // BATCH_SIZE

    print("Processing pcap %d [%d/%d]" % (pcap.id, idx, df_pcap.shape[0]))

    for s in windows[::-1]:
        
        # the windows bounds are defined by the number of queries (qnum = s), because I suppose
        # that is better to trigger the intrusion detector whit a threshold dependent on the number of queries.
        df_fn_q_s = df_fn_q.copy()
        df_fn_q_s['window'] = df_fn_q_s['n'] // s
        df_fn_q_s = df_fn_q_s[['window', 'fn']].groupby(by='window').aggregate(['first']).reset_index()
        df_fn_q_s.columns = ['window', 'first_fn']
        df_fn_q_s['last_fn'] = df_fn_q_s['first_fn'].shift(-1, fill_value=df_fn.fn.max() + 1) - 1
        df_fn_q_s['batch'] = df_fn_q_s['last_fn'] // BATCH_SIZE

        for i in range(df_fn_q_s.batch.max()+1):
            df_fn_q_s_batch = df_fn_q_s[df_fn_q_s.batch == i]

            first_fn_bacth = df_fn_q_s_batch['first_fn'].min()
            last_fn_bacth = df_fn_q_s_batch['last_fn'].max()

            tic = time.perf_counter()
            query = (
                    """SELECT m3.*, dn."logit.0"[1] as logit, dn.top10m
                        FROM message3 as m3 JOIN dn ON m3.dn_id=dn.id
                            WHERE pcap_id=%d AND fn >= %d AND fn <= %d
                                ORDER BY fn"""
                    % (
                        pcap.id,
                        first_fn_bacth,
                        last_fn_bacth
                    )
                )
            df_w_batch = pd.read_sql(query, db)

            df_w_batch['nx'] = (df_w_batch.rcode == 3)
            df_w_batch['nlegit'] = (df_w_batch.top10m >= 1_000_000)

            print("Fetching from %d <= fn <= %d for pcap_%d and size=%d" % (first_fn_bacth, last_fn_bacth, pcap.id, s))
            values = []
            for (_, s_window) in df_fn_q_s_batch.iterrows():

                first_fn = s_window['first_fn']
                last_fn = s_window['last_fn']

                df_qr = df_w_batch.iloc[first_fn:last_fn+1]
                df_q = df_qr[~df_qr.is_response]
                df_r = df_qr[df_qr.is_response]
                if df_r.shape[0] == 0:
                    df_r = df_r.append(pd.Series(0, index=df_r.columns), ignore_index=True)

                dfs = [df_qr, df_q, df_r]

                sum_cols = ["app", "ok", "txt", "nx", "no", "other", "llr"]

                app = [ _df.shape[0] for _df in dfs ]

                first = [ _df.iloc[0].fn for _df in dfs ]
                last = [ _df.iloc[-1].fn for _df in dfs ]

                begin_time = df_qr.iloc[0].time_s
                end_time = df_qr.iloc[-1].time_s

                nunique = [ _df.dn_id.drop_duplicates().shape[0] for _df in dfs ]

                nx = df_r.nx.sum()

                gt09 = (dfs[1].logit > 0.9).sum()

                llr = [ _df.logit.sum() for _df in dfs ]

                llr_t = [ _df.logit[_df.nlegit].sum() for _df in dfs ]

                def cv(v):
                    if type(v).__module__ == np.__name__:
                        return v.item()
                    if type(v).__name__ == "list":
                        return [ vv.item() if type(vv).__module__ == np.__name__ else vv for vv in v ]
                    return v

                values.append([ cv(v) for v in [pcap.id, s, s_window['window'], first, last, begin_time, end_time, app, nunique,  nx, gt09, llr, llr_t ] ])
                pass #end window
            # print("\n".join([v.__str__() for v in values]))
            # print()
            execute_values(
                cur,
                """INSERT INTO public.window_qr_4(
                    pcap_id, size, "window", first, last, begin_time, end_time, app, "unique", nx, gt09, llr, llr_t)
                    VALUES  %s;""",
                values
            )
            pass #end batch
        db.commit()
        pass #end batches
    pass #end pcap
    cur.close()
