import pyshark
import os
from enum import Enum
import pprint
import pandas as pd
import time
from .model import Model
from .extractor import Extractor
import pickle

class Pcap2Csv():
    def __init__(self, domain=True, nosfx=True, outdir=None, fast_mode=True, buffer_size=0, legit_dsfx=True, only_dn=False, pkt_num=0, timeout=0):
        self.chosen_dns_fields = [ 'count_labels', 'count_queries', 'flags_authenticated', 'flags_response', 'qry_name', 'qry_type', 'qry_class','resp_ttl', 'resp_name', 'response_to', 'time', 'a', 'aaaa' ]
        
        with open(os.path.join(os.path.dirname(__file__), 'top_10m.dsfx.pkl'), 'rb') as fp:
            self.df_10m_set = pickle.load(fp)
        self.fast_mode   = fast_mode
        self.buffer_size = buffer_size
        self.legit_dsfx  = legit_dsfx
        self.only_dn     = only_dn
        self.pkt_num     = pkt_num
        self.timeout     = timeout
        self.outdir      = outdir
        self.models = []
        if nosfx:
            self.models.append(Model.load('nosfx'))
        if domain:
            self.models.append(Model.load('domain'))


    def convert(self, path, redo=False, fast_mode=None, buffer_size=None, legit_dsfx=None, pkt_num=None, timeout=None):
        fast_mode   = fast_mode   if fast_mode   is not None else self.fast_mode
        buffer_size = buffer_size if buffer_size is not None else self.buffer_size
        legit_dsfx  = legit_dsfx  if legit_dsfx  is not None else self.legit_dsfx
        pkt_num     = pkt_num     if pkt_num     is not None else self.pkt_num
        timeout     = timeout     if timeout     is not None else self.timeout
        
        i = 0
        data = []
        first = True
        ext = '.fast.csv' if fast_mode else '.csv'
        filedir  = os.path.dirname(path)
        filename = os.path.basename(path)
        path_out_progress = os.path.join(self.outdir or filedir, filename + ext + '.tmp')
        path_out = os.path.join(self.outdir or filedir, filename + ext)

        if not redo and os.path.exists(path_out):
            return pd.read_csv(path_out, index_col=0)

        if os.path.exists(path_out_progress):
            os.remove(path_out_progress)

        if fast_mode:
            columns = ['frame_number', 'timestamp', 'qry_name', 'response']
        else:
            chosen_dns_fields = self.chosen_dns_fields.copy()
            columns = ['frame_number', 'timestamp'] + chosen_dns_fields
        if legit_dsfx:
            columns += ['legit_dsfx']

        def __flush():
            df = pd.DataFrame(data, index=pd.RangeIndex(i-len(data), i), columns=columns)
            df.to_csv(path_out_progress, mode='w' if first else 'a', header=first)

        def __packetsummary2row(packet):
            # 'No.': '17242', 'Time': '43049.733195', 'Source': '192.168.1.116', 'Destination': '8.8.8.8',
            # 'Protocol': 'DNS', 'Length': '76', 'dns.a': '', 'dns.qry.name': 'dns.msftncsi.com', 
            # 'dns.flag.response': 'Message is a query', 'dns.qry.type': 'AAAA (IPv6 Address)', 'ip': '8.8.8.8'}
            if packet._fields['Protocol'] != 'DNS':
                return None
            qry = packet._fields['dns.qry.name']
            if not Extractor.is_dn(qry):
                return None
            row = [packet._fields['No.'], packet._fields['Time'], qry, 1 if 'Message is a response' == packet._fields['dns.flag.response'] else 0]
            if legit_dsfx:
                row += [1 if Extractor.domain_sfx(qry) in self.df_10m_set else 0]
            return row

        def __packet2row(packet):
            if 'DNS' not in packet:
                return None
            qry = packet['DNS'].get_field('qry_name')
            if not Extractor.is_dn(qry):
                return None
            dns_layer = packet['DNS']
            row = [packet.number, packet.sniff_time.timestamp()]
            row += [dns_layer.get_field(cf) for cf in chosen_dns_fields]
            if legit_dsfx:
                row += [1 if Extractor.domain_sfx(qry) in self.df_10m_set else 0]
            return row

        time_begin = time.time()
        tshark_cap = pyshark.FileCapture(path, only_summaries=fast_mode, keep_packets=False, display_filter="dns && !_ws.malformed && !icmp")
        try:
            while True:
                packet = tshark_cap.next()
                row = __packetsummary2row(packet) if fast_mode else __packet2row(packet)
                if row is None:
                    continue
                data.append(row)
                i += 1
                if buffer_size > 0 and len(data) >= buffer_size:
                    __flush()
                    data = []
                    first = False
                if timeout != 0 and (time.time() - time_begin) > timeout: break
                if pkt_num > 0 and i > pkt_num: break
        except (StopIteration, pyshark.capture.capture.TSharkCrashException):
            try:
                tshark_cap.close()
            except pyshark.capture.capture.TSharkCrashException:
                print('Warning: error during closing pcap file with pyshark.')
                tshark_cap._running_processes = False
                pass
            if i == 0:
                print('Error: probably not a PCAP file.')
                return pd.DataFrame()
            pass
        
        if len(data) > 0:
            __flush()
        
        df = pd.read_csv(path_out_progress, index_col=0)

        if not fast_mode:
            set_responses = set(df[df['response_to'] >= 0]['response_to'].astype(int).values)
            def check_reqres(row):
                if row['flags_response'] == 1: return row
                else:
                    row['response_to'] = row['frame_number'] in set_responses
                return row
            df = df.apply(check_reqres, axis=1)

        for model in self.models:
            df[model.name], _ = model.predict_u(df['qry_name'])

        df.to_csv(path_out)

        os.remove(path_out_progress)

        return df


if __name__ == '__main__':
    root_path = '/media/princio/ssd512/stratosphere/pcap/normal/'
    files = []
    sizes = []
    for walker in os.walk('/media/princio/ssd512/stratosphere/pcap/normal/'):
        for filename in walker[2]:
            if filename[filename.rfind('.'):] != '.pcap': continue
            try:
                files.append(filename)
                sizes.append(os.stat(os.path.join(root_path, filename)).st_size)
            except pd.errors.EmptyDataError:
                print(f'File <{filename}> is empty.')

    files = {files[i[0]]: i[1] for i in sorted(enumerate(sizes), key=lambda x:x[1])}

    time_tot = 0
    size_tot = 0
    row_tot  = 0
    pcap_2_csv = Pcap2Csv()
    for file in files:
        print('Parsing %s, having size %d' % (file, files[file]))
        t1 = time.time()
        num_row = pcap_2_csv.convert(os.path.join(root_path, file), fast_mode=False, buffer_size=10_000).shape[0]
        t1 = time.time() - t1
        print('   Time:     %0.4f' % t1)
        print('Row num:     %d'    % num_row)
        time_tot += t1
        row_tot  += num_row
        size_tot += files[file]

    n = len(files.keys())
    print('Pcap num:\t%4d' % n)
    print('Time tot:\t%0.4f\t%0.4f'  % (t1, t1/n))
    print(' Row tot:\t%4d\t%0.4f'    % (num_row, num_row/n))
    print('Size tot:\t%4d\t%0.4f'    % (size_tot, size_tot/n))
