{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/princio/anaconda3/envs/keras/lib/python3.8/site-packages/tldextract/.tld_set\n",
      "/home/princio/anaconda3/envs/keras/lib/python3.8/site-packages/tldextract/.tld_set\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Extractor import Extractor\n",
    "from IPython.core.display import display, HTML\n",
    "from hyphen import dictools, Hyphenator\n",
    "from pyphonetics import Soundex, Metaphone\n",
    "from wordsegment import load, segment\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p\n",
    "g2p = G2p()\n",
    "df = pd.read_csv('training/dataset_training.csv')\n",
    "df['nosfx'] = df.dn.apply(Extractor.nosfx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M AO1 R T IH0 S   K AA1 N T R AE0 S T   AH0   T IH1 M\n",
      "K IH2 V AH0 N AH0 F OW1 S IH0 K S   HH AH1 N D R AH0 D   TH ER2 T AH0 S K AH0 V ER1 S T AH0 N B ER0.D EH1 N S IY1 N Z\n",
      "P L AE1 S T IH0 K   B AE1 G Z.S AA1\n",
      "M Z EH1 L   T R AE1 K\n",
      "M IH1 S   S L IH1 M\n",
      "T EH2 N K AH0 Z M UH1 R T AH0 K AH0 T\n",
      "M AY1   HH OW1 S T IH0 NG   P AE1 K\n",
      "IH2 K S IH0 K S JH IH1 R AH0 V AH0 M.D EH1 N S IY1 N Z\n",
      "R AY2 Y UW2 M OW0 S F EY1 SH AH0 N D L IY0\n",
      "D ER0 K AH0 M T AA1 G AH0 B AH0 L\n",
      "T B AH0 M B IH1 V W AH0 L CH AO2 F S\n",
      "B R AH1 DH ER0   N ER1 V   P L EY1 S   B R IH1 NG   K AH0 N S AH1 L T\n",
      "D AW1 N L OW2 D   B AY1   S AA0 T OW1 SH IY0\n",
      "L R UW2 SH T IY0 B IY0 S IY1 T IY0\n",
      "R AY0 K AO1 K W IY0 AH0\n",
      "IH0 N   DH AH0   M IH1 D AH0 L   N AE1 SH V IH0 L\n",
      "S IY1 D IY1   JH IH1 M   F AO1 JH   S P EH1 M P IY1\n",
      "G AE1 S   AA1 Z   JH AA1 M\n",
      "K W IH1 N   Y UW1 AH0\n",
      "D UW1   K IY1   AH1 K S   OW1 K EY1\n",
      "K AA1 L V AH0   D UW1\n",
      "M EH1 R AH0 L AH0 N D\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "{'K', 'IY0', 'AH1', 'AO1', 'EH1', 'EH2', 'AY0', 'JH', 'AE0', 'OW1', 'S', 'UW2', 'AO2', 'DH', 'AE1', 'L', ' ', 'B', 'Z', 'Y', 'T', 'W', 'HH', 'SH', 'ER0', 'IH1', 'ER2', 'M', 'G', 'EY1', 'UH1', 'AY1', 'AH0', 'ER1', 'IY1', 'D', 'OW2', 'UW1', 'P', 'AA1', 'TH', 'V', 'AW1', 'IH0', 'CH', 'NG', 'R', 'IH2', 'AY2', 'F', 'OW0', 'AA0', 'N'}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "X = []\n",
    "for i, dn in enumerate(df.values):\n",
    "    phrases = [g2p(' '.join(phrase)) for phrase in [segment(p) for p in dn[3].split('.')]]\n",
    "    [vocabulary.add(arpachar) for phrase in phrases for arpachar in phrase ]\n",
    "    X.append('.'.join([ ' '.join(phrase) for phrase in phrases ]))\n",
    "    if i > 20:\n",
    "        break\n",
    "print([print(x) for x in X])\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "X = []\n",
    "for i, dn in enumerate(df.values):\n",
    "    phrases = [g2p(' '.join(phrase)) for phrase in [segment(p) for p in dn[3].split('.')]]\n",
    "    [vocabulary.add(arpachar) for phrase in phrases for arpachar in phrase ]\n",
    "    X.append('.'.join([ ' '.join(phrase) for phrase in phrases ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N',\n",
       " 'AY1',\n",
       " 'N',\n",
       " 'T',\n",
       " 'IY1',\n",
       " 'N',\n",
       " ' ',\n",
       " 'N',\n",
       " 'IY2',\n",
       " 'IH0',\n",
       " 'N',\n",
       " 'T',\n",
       " 'UW1',\n",
       " 'T',\n",
       " 'IY0',\n",
       " 'OW0']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2p('1992')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('training/dataset_arpanet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_s = []\n",
    "for i, dn in enumerate(df.values):\n",
    "    phrases = [segment(p) for p in dn[3].split('.')]\n",
    "    [vocabulary.add(arpachar) for phrase in phrases for arpachar in phrase ]\n",
    "    dn_segmented = '. '.join([' '.join(phrase) for phrase in phrases])\n",
    "    phrases_s.append(dn_segmented)\n",
    "    if i%10000 == 0:\n",
    "        print('%d00\\'000' % (i/100000))\n",
    "phrases_s\n",
    "#df['bo'] = phrases_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "#df.to_csv('training/dataset_arpanet.csv')\n",
    "df2['bo'] = phrases_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns={'bo': 'segmented'}).to_csv('training/dataset_training2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3[(df3['len'] == 2) & (df3['vocal'] == True) & (df3['num'] == False)]\n",
    "\n",
    "df4.to_csv('/tmp/bo_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1479655\n"
     ]
    }
   ],
   "source": [
    "voc = []\n",
    "for i, ph in enumerate(phrases_s):\n",
    "    for w in ph.split(' '):\n",
    "        voc.append(w) \n",
    "print(len(voc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spaces</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rovnix</th>\n",
       "      <td>0.004963</td>\n",
       "      <td>18.004963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qadars</th>\n",
       "      <td>0.008963</td>\n",
       "      <td>12.008963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ramdo</th>\n",
       "      <td>0.011111</td>\n",
       "      <td>16.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotet</th>\n",
       "      <td>0.012741</td>\n",
       "      <td>16.012741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ranbyus</th>\n",
       "      <td>0.014074</td>\n",
       "      <td>16.875630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cryptolocker</th>\n",
       "      <td>0.084148</td>\n",
       "      <td>13.586148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>padcrypt</th>\n",
       "      <td>0.095037</td>\n",
       "      <td>16.095037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tinba</th>\n",
       "      <td>0.136815</td>\n",
       "      <td>12.139926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dircrypt</th>\n",
       "      <td>0.224963</td>\n",
       "      <td>14.269778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ramnit</th>\n",
       "      <td>0.315111</td>\n",
       "      <td>13.826667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>necurs</th>\n",
       "      <td>0.377778</td>\n",
       "      <td>14.351037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fobber</th>\n",
       "      <td>0.479556</td>\n",
       "      <td>10.562519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>murofet</th>\n",
       "      <td>0.666889</td>\n",
       "      <td>27.007333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pykspa</th>\n",
       "      <td>0.883037</td>\n",
       "      <td>11.361926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nymaim</th>\n",
       "      <td>1.001037</td>\n",
       "      <td>14.910519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corebot</th>\n",
       "      <td>1.009185</td>\n",
       "      <td>25.018519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suppobox</th>\n",
       "      <td>1.176296</td>\n",
       "      <td>14.558444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conficker</th>\n",
       "      <td>1.262519</td>\n",
       "      <td>7.788074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alexa</th>\n",
       "      <td>1.424834</td>\n",
       "      <td>12.399282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kraken</th>\n",
       "      <td>1.613333</td>\n",
       "      <td>13.114222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simda</th>\n",
       "      <td>1.621852</td>\n",
       "      <td>8.844593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pushdo</th>\n",
       "      <td>1.956074</td>\n",
       "      <td>9.956074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vawtrak</th>\n",
       "      <td>2.130222</td>\n",
       "      <td>11.140148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmi</th>\n",
       "      <td>2.303704</td>\n",
       "      <td>18.809185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gozi</th>\n",
       "      <td>2.935481</td>\n",
       "      <td>22.192444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matsnu</th>\n",
       "      <td>3.676667</td>\n",
       "      <td>29.772741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                spaces        len\n",
       "class                            \n",
       "rovnix        0.004963  18.004963\n",
       "qadars        0.008963  12.008963\n",
       "ramdo         0.011111  16.011111\n",
       "emotet        0.012741  16.012741\n",
       "ranbyus       0.014074  16.875630\n",
       "cryptolocker  0.084148  13.586148\n",
       "padcrypt      0.095037  16.095037\n",
       "tinba         0.136815  12.139926\n",
       "dircrypt      0.224963  14.269778\n",
       "ramnit        0.315111  13.826667\n",
       "necurs        0.377778  14.351037\n",
       "fobber        0.479556  10.562519\n",
       "murofet       0.666889  27.007333\n",
       "pykspa        0.883037  11.361926\n",
       "nymaim        1.001037  14.910519\n",
       "corebot       1.009185  25.018519\n",
       "suppobox      1.176296  14.558444\n",
       "conficker     1.262519   7.788074\n",
       "alexa         1.424834  12.399282\n",
       "kraken        1.613333  13.114222\n",
       "simda         1.621852   8.844593\n",
       "pushdo        1.956074   9.956074\n",
       "vawtrak       2.130222  11.140148\n",
       "symmi         2.303704  18.809185\n",
       "gozi          2.935481  22.192444\n",
       "matsnu        3.676667  29.772741"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['spaces'] = df2['segmented'].apply(lambda x: x.count(' '))\n",
    "df2['len'] = df2['segmented'].apply(len)\n",
    "df2.groupby('class').mean().sort_values(by='spaces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mortis', 'contrast', 'a', 'tim'] M AO1 R T IH0 S | K AA1 N T R AE0 S T | AH0 | T IH1 M\n",
      "0'000\n",
      "['cvyh1po636avyrsxebwbkn7.', 'ddns'] K IH2 V AH0 N AH0 F OW1 S IH0 K S | HH AH1 N D R AH0 D | TH ER2 T AH0 S K AH0 V ER1 S T AH0 N B ER0 | . | D EH1 N S IY1 N Z\n",
      "['plastic', 'bags.', 'sa'] P L AE1 S T IH0 K | B AE1 G Z | . | S AA1\n",
      "['mzl', 'track'] M Z EH1 L | T R AE1 K\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_s = []\n",
    "voc = set()\n",
    "values = df2.values\n",
    "for i, dn in enumerate(df2.values):\n",
    "    arpa_phrase = g2p(dn[3])\n",
    "    [voc.add(arpa_char) for arpa_char in arpa_phrase ]\n",
    "    #dn_segmented = '. '.join([' '.join(phrase) for phrase in phrases])\n",
    "    #phrases_s.append(dn_segmented)\n",
    "    print(dn[3].split(' '), ' '.join(arpa_phrase).replace('   ', ' | ').replace('|.|', ' . ').strip())\n",
    "    if i > 2: break\n",
    "    if i%50_000 == 0:\n",
    "        print('%d\\'000' % (i/1_000))\n",
    "phrases_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 15 by\n",
      ". 0 thi-nkdistrict.by\n",
      "- 4 nkdistrict by\n",
      "- 0 thi-nkdistrict by\n",
      "? 0 thi nkdistrict by\n",
      "_ 0 thi nkdistrict by\n",
      "thi nkdistrict by\n",
      "['think', 'district', 'by']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.': [14], '-': [3], '?': [], '_': []}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_arpa(phrase):\n",
    "    phrase_or = phrase\n",
    "    pos = {c:[] for c in '.-?_'}\n",
    "    for c in '.-?_':\n",
    "        p=-1\n",
    "        ctr=0\n",
    "        while True:\n",
    "            p = phrase.find(c, p+1)\n",
    "            print(c, p+1, phrase[p+1:])\n",
    "            if p > -1:\n",
    "                pos[c].append(p)\n",
    "            else:\n",
    "                break\n",
    "            ctr+=1\n",
    "            if ctr > 10: break\n",
    "        phrase = phrase.replace(c, ' ')\n",
    "        \n",
    "    print(phrase)\n",
    "    print(segment(phrase_or))\n",
    "    return(pos)\n",
    "    \n",
    "to_arpa('thi-nkdistrict.by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'think | district'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_arpa(phrase):\n",
    "    phrase_or = phrase\n",
    "    ph = ''\n",
    "    dots = phrase.split('.')\n",
    "    for i, dot in enumerate(dots):\n",
    "        dots[i] = ' | '.join(segment(dot))\n",
    "    return ' . '.join(dots)\n",
    "    \n",
    "to_arpa('think-district')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'th ? ink _ dis . trict - '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bo(cs, k, sub_phrase):\n",
    "    split0 = sub_phrase.split(cs[k])\n",
    "    for i, _ in enumerate(split0):\n",
    "        if k+1 == len(cs):\n",
    "            split0[i] = ' | '.join(segment(split0[i]))\n",
    "        else:\n",
    "            split0[i] = bo(cs, k+1, split0[i])\n",
    "    return f' {cs[k]} '.join(split0)\n",
    "\n",
    "def segment_punct(string):\n",
    "    cs = ''.join([c if c in string else '' for c in '.-_'])\n",
    "    if cs == '':\n",
    "        return segment(string)\n",
    "    return bo(cs, 0, string)\n",
    "\n",
    "phrases_s = []\n",
    "voc = set()\n",
    "for i, dn in enumerate(df.values):\n",
    "    phrases_s.append(segment_punct(dn[3]))\n",
    "    if i%50_000 == 0:\n",
    "        print(i/10_000)\n",
    "        print(dn[3])\n",
    "phrases_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = phrases_s.copy()\n",
    "for i,p in enumerate(phrases):\n",
    "        phrases[i] = p.split(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'T', 'OW0', 'K', 'AA1', 'Z', 'OW0']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases\n",
    "\n",
    "for p in phrases:\n",
    "    a=[g2p(pp) for pp in p]\n",
    "    break\n",
    "g2p('stocaz-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training/dataset_training_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "mortis | contrast | a | tim\n",
      "['M', 'AO1', 'R', 'T', 'IH0', 'S', '|', 'K', 'AA1', 'N', 'T', 'R', 'AE0', 'S', 'T', '|', 'AH0', '|', 'T', 'IH1', 'M']\n"
     ]
    }
   ],
   "source": [
    "def my_g2p(string):\n",
    "    return (['#' if c == ' ' else c for c in g2p(string)])\n",
    "\n",
    "def bo(cs, k, sub_phrase):\n",
    "    split0 = sub_phrase.split(cs[k])\n",
    "    for i, _ in enumerate(split0):\n",
    "        if k+1 == len(cs):\n",
    "            split0[i] = ' '.join(my_g2p(split0[i]))\n",
    "        else:\n",
    "            split0[i] = bo(cs, k+1, split0[i])\n",
    "    return f' {cs[k]} '.join(split0)\n",
    "\n",
    "def g2p_punct(string):\n",
    "    string = string.replace(' ', '')\n",
    "    cs = ''.join([c if c in string else '' for c in '.-_|'])\n",
    "    if cs == '':\n",
    "        return ' '.join(my_g2p(string))\n",
    "    return bo(cs, 0, string)\n",
    "\n",
    "arpanets = []\n",
    "for i, phrase in enumerate(df.segmented.values):\n",
    "    arpanets.append(g2p_punct(phrase))\n",
    "    if i%50_000 == 0:\n",
    "        print(i/10_000)\n",
    "        print(phrase)\n",
    "        print(arpanets[i].split(' '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
