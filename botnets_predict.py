from Model import Model
import pandas as pd
from Extractor import Extractor
import os, math



ROOT_DIR = '/media/princio/ssd512/stratosphere'
def root_join(path, *paths):
    return os.path.join(ROOT_DIR, path, paths)

TYPE='Normal'
FOLDER = {
    'Normal': 'normal',
    'Botnet': 'botnet'
}

df_10m = pd.read_csv(root_join('top_10m.dsfx.csv')).set_index('dsfx')
REQ_ROOT_DIR=root_join('req.dn/', FOLDER[TYPE])
RES_ROOT_DIR=root_join('res.dn/', FOLDER[TYPE])


def path(type_, filename):
    if type_ == 'res':
        return root_join('res.y/', filename[:-9] + '.res.y.csv')
    if type_ == 'req':
        return root_join('req.y/', filename[:-9] + '.req.y.csv')
    if type_ == 'unique':
        return root_join('unique.req.y/', filename[:-9] + '.unique.req.y.csv')

def to_uniques(df):
    counts = df.value_counts().rename('counts').to_frame().reset_index()
    df = df.drop_duplicates()
    counts.insert(0, 'num', df.index)
    return counts

def legit_top10m(df):
    if 'dsfx' not in df.columns:
        df['dsfx'] = df['DN'].apply(Extractor.domain_sfx)

    bibo = df.set_index('dsfx').join(df_10m)
    bibo['rank'] = bibo['rank'].fillna(-1).apply(lambda x: 0 if x<0 else 1)
    bibo = bibo.rename(columns={'rank': 'dsfx_legit'})
    bibo = bibo.reset_index()[['DN', 'dsfx', 'dsfx_legit', 'last', 'nosfx']]
    #bibo.to_csv(os.path.join(YREQ_ROOT_DIR, filename), index=False)
    return bibo


models = [ Model.load('nosfx'), Model.load('last')]

files = []
for walker in os.walk(REQ_ROOT_DIR):
    for filename in walker[2]:
        try:
            files.append(filename)
        except pd.errors.EmptyDataError:
            print(f'File <{filename}> is empty.')


errors = []
for num, filename in enumerate(files):
    df = pd.read_csv(os.path.join(REQ_ROOT_DIR, filename), names=['DN'])

    if df.shape[0] == 0:
        print('%5d/%d %s--EMPTY' % (num, len(files), filename))
        continue

    try:
        print('%5d/%d %50s%20d' % (num, len(files), filename, df.shape[0]))

        print('checking domains...')
        DNs = df['DN'].copy()
        DNs_correct = DNs.apply(Extractor.is_dn)
        DNs = DNs[DNs_correct == 1]

        print(f'found {df["DN"].shape[0] - DNs.shape[0]} wrong domains.')

        print('predict...')
        for model in models:
            df[model.name], _ = model.predict_u(DNs)

        print(df[model.name])

        print('legit top10m...')
        df = legit_top10m(df)
        df.to_csv(path('req', filename), index=False)

        print('uniquing...')
        df_uniques = to_uniques(df)
        df_uniques.to_csv(path('unique', filename), index=False)

        print('done')
    except Exception as e:
        print(f'error with {filename}: {e}', e)
        errors.append(f'error with {filename}: {e}')
        raise e

with open('/tmp/errors.txt', 'w') as fp:
    fp.writelines(errors)

print(errors)


#71/418 12_2013-10-17_capture-win10-from-2013-10-17-to-2013-10-18

# 04/02/2021:
# ['error with 12_2013-10-17_capture-win10-from-2013-10-17-to-2013-10-18.dns.list: Length of values (33309) does not match length of index (33308)', 'error with 149-1_2015-12-09_capture-win4.dns.list: Length of values (34667) does not match length of index (34662)', 'error with 149-2_2015-12-09_capture-win4.dns.list: Length of values (12422) does not match length of index (12372)', 'error with 226-1_2017-2-27_win5.dns.list: Length of values (621) does not match length of index (614)', 'error with 226-2_2017-02-27_win16.dns.list: Length of values (621) does not match length of index (614)', 'error with 54_botnet-capture-20110815-fast-flux-2.dns.list: Length of values (1681) does not match length of index (1680)', 'error with 83-2_2014-06-30_capture-win2.dns.list: Length of values (490) does not match length of index (489)']


# ycsv_list = get_filenames(UNIQUE_YREQ_ROOT_DIR)

# def funnn(row):
#     if row['dsfx_legit'] == 0: return row
#     if row['dsfx'].find('dyndns') == -1: return row
#     row['dsfx_legit'] = 0
#     return row

# for n, filename in enumerate(ycsv_list):
#     print(f'{n+1:5}/{len(ycsv_list)}: {filename}.')
#     df = pd.read_csv(os.path.join(UNIQUE_YREQ_ROOT_DIR, filename))
    
#     df = df.apply(funnn, axis=1)
    
#     df.to_csv(os.path.join(UNIQUE_YREQ_ROOT_DIR, filename), index=0)
    
#     clear_output(wait=True)


# ho effettuato due allenamenti (uno con la parte di dominio che include solo la prima label successiva al suffisso, ed uno che invece include tutto omettendo solo il suffisso) 