import os, json
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
from datetime import datetime
from PyInquirer import prompt
import csv
import hashlib
import re
import psycopg2
from Model import Model
from Report import Report
from Dataset import Dataset
from Repository import DatasetRepository, ModelRepository, MetricsRepository
from Metrics import Metrics

connection = psycopg2.connect(user = "postgres",
                                  password = "porcodio",
                                  host = "127.0.0.1",
                                  port = "5432",
                                  database = "malware")

cursor = connection.cursor()


def pretty(d, indent=0):
   for key, value in d.items():
      print('\t' * indent + str(key))
      if isinstance(value, dict):
         pretty(value, indent+1)
      else:
         print('\t' * (indent+1) + str(value))

if __name__ == "__main__":
    datasets = DatasetRepository.fetchall(connection)
    models = ModelRepository.fetchmany(connection)

    with open('output/averages.csv','w') as fp:
        models_average = {}
        for model in models.values():
            model_average = Metrics(list(datasets.values())[0].labels, list(datasets.values())[0].dga)
            for dataset in datasets.values():
                metrics = MetricsRepository.fetch(connection, model, dataset)
                if metrics is None:
                    conf_matrix, _ = model.test(dataset)
                    metrics = Metrics.create(model.classes, dataset.dga, conf_matrix)
                print("testing %s with %s" % (model.path, dataset.name))
                MetricsRepository.save(connection, model, dataset, metrics)
                fp.write("%s,%s\n" % (dataset.name, metrics.str_row(',')))
                model_average.sum(metrics)
            model_average.div(len(datasets.values()))
            model.average = model_average.tolist()
            model.save_average(connection)
            print(model_average)

            if model.training_confusion_matrix is not None:
                training_metrics = Metrics.create(model.classes, dataset.dga, model.training_confusion_matrix)
                print(training_metrics)
                print(model_average.str2(name=model.name()))
                print(training_metrics.str2(name=model.name()))
                fp.write(model.path)
                fp.write("%s,%s\n" % (model.path, model_average.str_row(',')))
                fp.write("%s,%s\n\n" % ("average", training_metrics.str_row(',')))
    

            
    connection.close()
